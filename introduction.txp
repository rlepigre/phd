\Include{Epigraph}
\Include{Macros}

\Caml(
  open Diagrams
  open ProofTree
  open PMLVerbatim
  open Lang
)

=> From Programming to Program Proving \label("intro")

\linesBefore(6)
\begin{epigraph}
  //[In 1949,] as soon as we started programming, we found to our surprise
  that it wasn't as easy to get programs right as we had thought. Debugging
  had to be discovered. I can remember the exact instant when I realized that
  a large part of my life from then on was going to be spent in finding
  mistakes in my own programs.//

	\blackline()

  \begin{raggedRight}
    Maurice Wilkes (1913-2010)
  \end{raggedRight}
  \linesAfter(6)
\end{epigraph}
(* *)
Since the apparition of the very first computers, every generation of
programmers has been faced with the issue of code reliability. Statically
typed languages such as Java, Haskell, OCaml, Rust or Scala have addressed
the problem by running syntactic checks at compile time to detect incorrect
programs. Their strongly typed discipline is especially useful when several
incompatible data objects have to be manipulated together. For example, a
program computing an integer addition on a boolean (or function) argument
is immediately rejected. In recent years, the benefit of static typing has
even begun to be recognised in the dynamically typed languages community.
Static type checkers are now available for languages like Javascript
\mcite(["Microsoft2012";"Facebook2014"]) or Python \cite("Lehtosalo2014").

In the last thirty years, significant progress has been made in the
application of type theory to computer languages. The Curry-Howard
correspondence, which links the type systems of functional programming
languages to mathematical logic, has been explored in two main directions.
On the one hand, proof assistants like Coq or Agda are based on very
expressive logics. To prove their consistency, the underlying programming
languages need to be restricted to contain only programs that can be
proved terminating. As a result, they forbid the most general forms of
recursion. On the other hand, functional programming languages like OCaml
or Haskell are well-suited for programming, as they impose no restriction
on recursion. However, they are based on inconsistent logics, which means
that they cannot be used for proving mathematical formulas.

The aim of this work is to provide a uniform environment in which
programs can be designed, specified and proved. The idea is to combine a
full-fledged, ML-like programming language with an enriched type-system
allowing the specification of computational behaviours. The system can
thus be used as ML for type-safe general programming, or as a proof
assistant for proving properties of ML programs. The uniformity of the
framework implies that programs can be incrementally refined to obtain
more guarantees. In particular, there is no syntactic distinction
between programs and proofs in the system. This means that programming
and proving features can be mixed when constructing proofs or programs.
For instance, proofs can be composed with programs for them to transport
properties (e.g. addition carrying its commutativity proof). In programs,
proof mechanisms can be used to eliminate unreachable code.

=> Writing functional programs

In this thesis, our first goal is to design a type system for a practical,
functional programming language. Out of the many possible technical choices,
we decided to consider a call-by-value language similar to OCaml or SML, as
they have proved to be highly practical and efficient. Our language provides
polymorphic variants \cite("Garrigue1998") and SML style records, which are
convenient for encoding data types. As an example, the type of lists can be
defined and used as follows.
### PML "examples/list.pml"
type rec list<a> = [Nil ; Cons of {hd : a ; tl : list}]

val rec exists : ∀a, (a ⇒ bool) ⇒ list<a> ⇒ bool = fun pred l →
  case l of
  | Nil[_]  → false
  | Cons[c] → if pred c.hd then true else exists pred c.tl

val rec rev_append : ∀a, list<a> ⇒ list<a> ⇒ list<a> = fun l1 l2 →
  case l1 of
  | Nil[_]  → l2
  | Cons[c] → rev_append c.tl Cons[{hd = c.hd; tl = l2}]
###
The ##exists## function takes as input a predicate and a list, and it
returns a boolean indicating whether there is an element satisfying the
predicate in the list. The ##rev_append## function takes as input two
lists ##l1## and ##l2##, and it appends ##l2## to the reversal of ##l1##.
Note that these two functions are polymorphic, they can hence be applied
to lists containing elements of an arbitrary type. In our syntax for
types, polymorphism is explicitly materialised using a universally
quantified type variable.

Polymorphism is an important feature as it allows for more generic
programs. In general, ML-like languages only allow a limited form of
polymorphism on let-bindings. In these systems, generalisation can only
happen on expressions of the form "##let x = t in u##". As a consequence,
the following function is rejected by the OCaml type checker.
### OCaml
let silly_ocaml : ('a → 'a) → unit → unit option =
  fun f u → f (Some (f u))
###
In our system, polymorphism is not limited: universal quantification is
allowed anywhere in types. Our types thus contain the full power of System
F \mcite(["Girard1972" ; "Reynolds1974"]). In particular, the equivalent of
##silly_ocaml## is accepted by our type-checker.
### PML "examples/silly.pml"
val silly : (∀a, a ⇒ a) ⇒ unit ⇒ option<unit> =
  fun f u → f Some[f u]
###
In fact, System F polymorphism is not the only form of quantification that
is supported in our system. It also provide existential types, which are an
essential first step toward the encoding of a module system supporting a
notion of abstract interface. Moreover, our system is based on higher-order
logic, which means that types are not the only objects that can be quantified
over in types. In particular, we will see that quantifiers can range over
terms in the next section.

The programming languages of the ML family generally include operations
generating side-effects, for example references (i.e. mutable variables).
Our system is no exception as it provides control operators. As first
discovered by Timothy G. Griffin \cite("Griffin1990"), control operators
like Lisp's //call/cc// can be used to give a computational interpretation
to classical logic. On the programming side, they can be used to encode an
exception mechanism. For example, the following function can be used to
compute the conjunction of a list of booleans rather efficiently.
### PML "examples/list.pml"
val rec conjunction : list<bool> ⇒ bool = fun l → save k →
  case l of
  | Nil[_]  → true
  | Cons[c] → if c.hd then conjunction c.tl else restore k false
###
Here, the continuation is saved in a variable ##k## and is restored with
the value ##false## if the boolean ##false## is encountered in the list.

Our control operators can also be used to define programs which types
correspond to logical formulas that are only valid in classical logic.
For instance, the type of the following programs corresponds to Pierce's
law, the law of the excluded middle and the principle of double negation
elimination.
### PML "examples/classical.pml"
val pierce : ∀a b, ((a ⇒ b) ⇒ a) ⇒ a =
  fun x → save k → x (fun y → restore k y)

// Disjoint sum of two types (logical disjunction)
type either<a,b> = [InL of a ; InR of b]

// Usual definition of the logical negation
type neg<a> = a ⇒ ∀x, x

val excl_mid : ∀a, {} ⇒ either<a, neg<a>> =
  fun _ → save k → InR[fun x → [k] InL[x]]

val dneg_elim : ∀a, neg<neg<a>> ⇒ a = pierce
###
Note that the definition of ##excl_mid## contains a dummy function
constructor. Its presence is required for a reason related to value
restriction (see \section("valrest")). It would not be necessary if
##excl_mid## was not polymorphic in ##a##. Moreover, note that
##dneg_elim## can be defined to be exactly ##pierce## thanks to
subtyping (see \chapter("subtyping")).

From a computational point of view, manipulating continuations using
control operators can be understood as cheating. For example
##excl_mid## (or rather, ##excl_mid {}##) saves the continuation and
immediately returns a (possibly false) proof of ##neg<a>##. Now, if
this proof is ever applied to a proof of ##a## (which would result
in absurdity), the program backtracks and returns the given proof of
##a##.

=<

=> Proofs of ML programs

The system presented in this thesis is not only a programming language,
but also a proof assistant focusing on program proving. Its proof mechanism
relies on equality types of the form ##t ≡ u##, where ##t## and ##u## are
arbitrary (possibly untyped) terms of the language itself. Such an equality
type is inhabited by ##{}## (i.e. the record with no fields) if the denoted
equivalence is true, and it is empty otherwise. Equivalences are managed
using a partial decision procedure that is guided by the construction of
programs. An equational context is maintained by the type checker to keep
track of the equivalences that are assumed to be true during the
construction of typing proofs. This context is extended whenever a new
equation is learned (e.g. when a lemma is applied), and equations are
proved by looking for contradictions (e.g. when two different variants
are suppose equivalent).

To illustrate the proof mechanism, we will consider simple examples of
proofs on unary natural number (a.k.a. Peano numbers). Their type is
given bellow, together with the corresponding addition function defined
using recursion on its first argument.
### PML "examples/nat.pml"
type rec nat = [Zero ; Succ of nat]

val rec add : nat ⇒ nat ⇒ nat = fun n m →
  case n of
  | Zero[_] → m
  | Succ[k] → Succ[add k m]
###
As a first example, we will show that ##add Zero[{}] n ≡ n## for all ##n##.
To express this property, we can use the type ##∀n:ι, add Zero[{}] n ≡ n##,
where ##ι## can though of as the set of all the program values. This
statement can then be proved as follows.
### PML "examples/nat.pml"
val add_z_n : ∀n:ι, add Zero[{}] n ≡ n = {}
###
Here, the proof is immediate as have ##add Zero[{}] n ≡ n## by definition
of the ##add## function. Note that this equivalence holds for all ##n##,
whether it corresponds to an element of ##nat## or not. For instance, it
can be used to show ##add Zero[{}] true ≡ true##.

Let us now show that for every ##n## we have ##add n Zero[{}] ≡ n##. Although
this property looks similar to ##add_z_n##, the following proof is invalid.
### PML "examples/nat.pml"
// val add_n_z : ∀n:ι, add n Zero[{}] ≡ n = {}
###
Indeed, the equivalence ##add n Zero[{}] ≡ n## does not hold when ##n##
is not a unary natural number. In this case, the computation of
##add n Zero[{}]## will produces a runtime error. As a
consequence, we need to rely on a form of quantification that only ranges
over unary natural numbers. This can be achieved with the type
##∀n∈nat, add n Zero[{}] ≡ n##, which corresponds to a (dependent) function
taking as input a natural number ##n## and returning a proof of
##add n Zero[{}] ≡ n##. This property can then be proved using induction and
case analysis as follows.
### PML "examples/nat.pml"
val rec add_n_z : ∀n∈nat, add n Zero[{}] ≡ n = fun n →
  case n of
  | Zero[_] → {}
  | Succ[k] → let ih = add_n_z k in {}
###
In the case where ##n## is ##Zero[{}]## we need to prove
##add Zero[{}] Zero[{}]] ≡ Zero[{}]##, which is immediate by definition of
the ##add## function. In the case where ##n## is ##Succ[k]## we need to show
##add Succ[k] Zero[{}] ≡ Succ[k]##. By definition of ##add##, this can be
reduced to ##Succ[add k Zero[{}]] ≡ Succ[k]##. We can then conclude using
the induction hypothesis (i.e. ##add_n_z k##) with which we learn
##add k Zero[{}] ≡ k##.

It is important to note that, in our system, a program that is considered
as a proof needs to go though a termination checker. Indeed, a looping
program could be used to prove anything otherwise. For example, the following
proof is rejected.
### PML "examples/nat.pml"
// val rec add_n_z_loop : ∀n∈nat, add n Zero[{}] ≡ n = fun n →
//   let ih = add_n_z_loop n in {}
###
It is however easy to see that ##add_z_n## and ##add_n_z## are terminating,
and hence valid. In the following, we will always implicitly consider that
the programs considered as proofs have been proved terminating.

There are two main ways of learning new equations in the system. On the one
hand, when a term ##t## is matched in a case analysis, a branch can only be
reached when the corresponding pattern ##C[x]## matches. In this case we
can extend the equational context with the equivalence ##t ≡ C[x]##. On the
other hand, it is possible to invoke a lemma by calling the corresponding
function. In particular, this must be done to use the induction hypothesis
in proofs by induction like in ##add_z_n## or the following lemma.
### PML "examples/nat.pml"
val rec add_n_s : ∀n m∈nat, add n Succ[m] ≡ Succ[add n m] = fun n m →
  case n of
  | Zero[_] → {}
  | Succ[k] → let ind_hyp = add_n_s k m in {}
###
In this case, the equation corresponding to the conclusion of the used lemma
is directly added to the context. Of course, more complex results can be
obtained by combining more lemmas. For example, the following proves the
commutativity of addition using a proof by induction with ##add_n_z## and
##add_n_s##.
### PML "examples/nat.pml"
val rec add_comm : ∀n m∈nat, add n m ≡ add m n = fun n m →
  case n of
  | Zero[_] → let lem = add_n_z m in {}
  | Succ[k] → let ih  = add_comm k m in
               let lem = add_n_s m k in {}
###
Many more examples of proofs (and programs) are provided in \chapter("implem")
and together with the implementation of the system. Each of them (including
those in the current chapter) have been automatically checked upon the
generation of this document. They are thus correct with respect to the
implementation.

=<

=> A brief history of value restriction

\label("valrest")
(* TODO TODO TODO from here TODO TODO TODO *)
A soundness issue related to side-effects and call-by-value evaluation
arose in the seventies with the advent of ML. The problem stems from a
bad interaction between side-effects and Hindley-Milner polymorphism. It was
first formulated in terms of references \citen("Wright1995")("Section 2").
To extend ||ml|| with reference, the naive approach consist in defining
the following abstract type and polymorphic procedures (in the OCaml syntax).
### OCaml
  type 'a ref

  val ref  : 'a -> 'a ref
  val (:=) : 'a ref -> 'a -> unit
  val (!)  : 'a ref -> 'a
###
However, this leads to trouble when working with references of a polymorphic
type. The problem is demonstrated by the following example, which is accepted
by the naive extension of the type system.
### OCaml
  let l = ref [] in
  l := [true]; (List.hd !l) + 1
###
On the first line, variable ##l## is given the polymorphic type
##'a list ref##, which can be unified both with ##bool list ref## and
and ##int list ref## on the second line. This is an obvious violation of
type safety, which is the purpose of a type system.

To solve the problem, many alternative type systems were designed (e.g.
\mcite(["Tofte1990"; "Damas1982"; "Leroy1991"; "Leroy1993"])). However, they
all introduced a complexity that contrasted with the elegance and simplicity
of ||ml|| systems (see \citen("Wright1995")("Section 2") and
\citen("Garrigue2004")("Section 2") for a detailed account).
(* *)
A simple and elegant solution was finally found by Andrew Wright in the
nineties. He suggested restricting generalization (i.e. introduction of
polymorphism) to syntactic values \mcite(["Wright1994";"Wright1995"]).

In ||ml||, generalization usually happens in expressions of the form
##let x = u in t##, called let-bindings. The type-checking of such an
expression proceeds by infering the type of the term ##u##, which may
contain unification variables. The type of ##u## is then generalized by
introducing universal quantification over the unification variables.
Finally, the term ##t## is type-checked under the assumption that ##x##
has the most general type of ##u##. With value restriction, the
generalization of the type of ##u## only happens if ##u## is a
syntactic value. Consequently, the example above is rejected since
##ref []## is not a value, and hence its infered type ##'_a list ref##
is only weakly polymorphic. Thus, it cannot be unified with both
##bool list ref## and ##nat list ref##.

(* FIXME integrate this shit

As mentioned in the previous section, the languages of the ||ml|| family
need a restriction on polymorphism to remain type-safe. With the value
restriction, the typing rule for ||ml|| polymorphism becomes the following.
$$ \ternaryR{Γ ⊢ v : B}{Γ, x : ∀X₁,...,X_n B ⊢ t : A}{v value}
    {Γ ⊢ let x = v in t : A} $$
When the value that is bound by a let-binding is not a value its type is
not generalized and the following rule applies.
$$ \binaryR{Γ ⊢ u : B}{Γ, x : B ⊢ t : A}{Γ ⊢ let x = u in t : A} $$

In slightly more expressive type systems like System F, the value
restriction appears in the typing rule for the introduction of the
universal quantifier. The ususal typing rule
$$ \binaryRN{∀_i}{Σ,X;Γ ⊢ t:A}{X ∉ FV(Γ)}{Σ; Γ ⊢ t : ∀X A} $$
cannot be proved safe (in a call-by-value system with side-effects) if
$t$ is not a syntactic value. This will also be the case in our system
when we bring together polymorphism and the \lmcalc.

*)

In this thesis, we consider control structures, which have been shown to
give a computational interpretation to classical logic by Timothy Griffin
\cite("Griffin1990"). One way of extending ||ml|| with control structures
is to introduce the following two primitive polymorphic procedures.
### OCaml
  val callcc : ('a cont -> 'a) -> 'a
  val throw  : 'a cont -> 'a -> 'b
###
The function ##callcc## corresponds to the control operator //call/cc//,
which was first introduced in the //Scheme// programming language. When
called, this function saves the current continuation (i.e. the current
state of the programs environment) and feeds it to the function it is
given as an argument. The continuation can be restored in the body of
this function using the ##throw## function.

In 1991, Robert Harper and Mark Lillibridge found a complex program
breaking the type safety of ||ml|| with //call/cc// \cite("Harper1991").
### OCaml
  let c = callcc
    (fun k -> ((fun x -> x), (fun f -> throw k (f, (fun _ -> ())))))
  in print_string ((fst c) "Hello world!"); (snd c) (fun x -> x+2)
###
Intuitively, this program saves the continuation and then builds a couple
##c## containing two functions. The first one is simply the identity
function. The second one takes a function ##f## as argument, uses
##throw## to restore the previously saved continuation and replaces the
two functions by ##f## and a constant function returning the only value
of type ##unit##. Consequently, the first element of the couple ##c## can
be used as the identity function as long as the second element of ##c##
has not been used. If the second element of ##c## is called with a
function ##g##, then ##g## becomes the first element of ##c## and the
computation restarts. This leads to a crash since the function
##fun x -> x+2## is applied to a value of type ##string##.

The infered type for ##c## is ##('_a -> '_a) * (('_a -> '_a) -> unit)##
prior to generalization. This means that without value restriction, the
last line of the example is type-checked in a context where ##c## has
the polymorphic type ##('a -> 'a) * (('a -> 'a) -> unit)##. As a
consequence, the type ##'a -> 'a## can be unified with
##string -> string## and ##int -> int##. As with references, value
restriction solves the inconsistency and yields a sound type system. 
Indeed, ##'_a -> '_a## cannot be unified with ##string -> string## and
with ##int -> int## at the same time.

It is relatively easy to translate the counter example of Robert Harper
and Mark Lillibridge into our language. Indeed, a term of the form
##callcc (fun k -> t)## will be translated into $\t("μα t")$ where $t$
is the translation of ##t## in which every subterm of the form
##throw k u## has replaced by $\t("[α]u")$.
In the context of the \lmcalc, the soundness issue arises when evaluating
$\t("t μα u")$ when $\t("μα u")$ has a polymorphic type. Such a situation
cannot happen with value restriction since $\t("μα u")$ is not a value.

=<

=> Dependent functions and relaxed restriction

Several difficulties arise when combining call-by-value evaluation,
side-effects, dependent products and equality types. Most notably, dependent
products need to be restricted to preserve soundness. Indeed, if ##f## has
type ##(a:A) ⇒ B(a)## and ##t## has type ##A## then we cannot always derive
that ##f t## has type ##B(t)##. As a consequence, we need to restrict the
application of dependent functions so that ##f t## is guaranteed to be of
type ##B(t)##.
(* *)
The simplest possible approach consist in only allowing syntactic values as
argument of dependent functions. This //value restriction// has been
introduced by Andrew Wright \cite("Wright1994") to solve a similar soundness
problem related to polymorphism.

With value restriction, the expressivity of our dependent products
is considerably weakened. In particular, it forbids the application of
##add_n_zero## to ##add Zero Zero## to obtain a proof of
##add (add Zero Zero) Zero ≡ add Zero Zero##. Indeed, the term
##add Zero Zero## is not a value. As a consequence, value restriction
breaks the modularity of our proof system, as it relies heavily on dependent
products.

Regular programs are also affected by value restriction in a similar way.
For example, it is possible to define a type for vectors (i.e. lists of
fixed length) together with their concatenation function in the following
way.
### PML
val rec len : ∀A List(A) ⇒ Nat = fun l →
  match l with
  | Nil    → Zero
  | Cons c → Succ (len c.tl)

type Vect(A,n) = ∃l (l:List(A) | len l ≡ n)

val rec concat : ∀A (n:Nat) ⇒ (m:Nat) ⇒ Vect(A,n) ⇒ Vect(A,m) ⇒
                      Vect(A,add n m) = fun n m ln lm →
  match n with
  | Zero    → lm
  | Succ nn → (match ln with
               | Nil    → 8<
               | Cons c → let ls = concat nn m c.tl lm in
                            Cons {hd = c.hd; tl = ls})
###
Here, the symbol ##8<## is used to fill a branch of the program that is
not accessible. Indeed, if ##n## is not ##Zero## then the list ##ln##
cannot be equal to ##Nil## by definition of ##Vect(A,n)## (its length
must be equal to ##n##). Let us now suppose that we want to produce,
using ##concat##, a function ##concat3## computing the concatenation of
three vectors. We would expect to be able to define it in the following
way.
### PML
val concat3 : ∀A (n:Nat) ⇒ (m:Nat) ⇒ (k:Nat) ⇒
                   List(n) ⇒ List(m) ⇒ List(k) ⇒
                     List(add (add n m) k) = fun n m k ln lm lk →
  concat (add n m) k (concat n m ln lm) lk
###
However, this definition is not valid with value restriction since
##concat## is applied to the non-value argument ##add n m##. This again
demonstrates that value restruction breaks the modularity of programs
that are typed using dependent products. As a consequence, we need to
relax value restriction so that some well-behaved terms can be used as
arguments for dependent functions.

The equality types provide a solution to our expressivity
problem. The equivalence relation they denote identifies terms (including
values) with the same observable computational behaviour. Hence, equal
terms have the same behaviour on every possible input. Consequently, we
can use a term ##t## as an argument of a dependent function, if an only
if there is a value ##v## such that we can derive ##t ≡ v##. The same idea
can be applied whenever value restriction was previously required, and the
obtained system is conservative over the one with the syntactic
restriction. Indeed, finding a value equivalent to a term that is already
a value can always be done using reflexivity. Although our new idea seems
simple, proving the soundness of the obtained system is surprisingly
subtle \cite("Lepigre2016").

=<

=> Main results and contributions

The main contribution of this paper is a new approach to value restriction.
The syntactic restriction on terms is replaced by a semantical restriction
expressed in terms of an observational equivalence relation denoted
$({≡})$. Although this approach seems simple, building a model to prove
soundness semantically is surprisingly subtle. Subject reduction is not
required here, as our model construction implies type safety. Furthermore
our type system is consistent as a logic.

In this paper, we restrict ourselves to a second order type system but it
can easily be extended to higher-order. Types are built from two basic sorts
of objects: propositions (the types themselves) and individuals (untyped
terms of the language). Terms appear in a restriction operator
$A \restriction t ≡ u$ and a membership predicate $t ∈ A$. The former is used
to define the equality types (by taking $A = \top$) and the latter is used
to encode dependent product.
$$ Π_{a : A} B := ∀ a (a ∈ A ⇒ B)$$
Overall, the higher-order version of our system is similar to a Curry-style
HOL with ML programs as individuals. It does not allow the definition of a
type which structure depends on a term (e.g. functions with a variable number
of arguments). Our system can thus be placed between HOL (a.k.a. $F_\omega$)
and the pure calculus of constructions (a.k.a. $CoC$) in (a Curry-style and
classical version of) Barendregt's $λ$-cube.

Throughout this paper we build a realizability model à la Krivine
\cite("Krivine2009") based on a call-by-value abstract machine. As a
consequence, formulas are interpreted using three layers (values, stacks
and terms) related via orthogonality. The crucial property for the soundness
of semantical value restriction is that
$$ φ^{\bot\bot} ∩ Λ_v = φ $$
for every set of values $φ$ (closed under $({≡})$). $Λ_v$ denotes the set of
all values and $φ^\bot$ (resp. $φ^{\bot\bot}$) the set of all stacks (resp.
terms) that are compatible with every value in $φ$ (resp. stacks in
$φ^\bot$). To obtain a model satisfying this property, we need to extend our
programming language with a term $δ_{v,w}$ which reduction depends on the
observational equivalence of two values $v$ and $w$.

(*
A model is built using classical
realizability techniques in which the interpretation of a type $A$ is spread
among two sets: a set of values $⟦A⟧$ and a set of terms $⟦A⟧^{\bot\bot}$.
The former contains all values that should have type $A$. For example,
$⟦nat⟧$ should contain the values of the form ##S[S[...Z[]...]]##. The
set $⟦A⟧^{\bot\bot}$ is the completion of $⟦A⟧$ with all the terms behaving
like values of $⟦A⟧$ (in the observational sense).
(* *)
To show that the relaxation of the value restriction is sound, we need the
values of $⟦A⟧^{\bot\bot}$ to also be in $⟦A⟧$. In other words, the
completion operation should not introduce new values. To obtain this
property, we need to extend the language with a new, non-computable
instruction internalizing equivalence. This new instruction is only used to
build the model, and will not be available to the user (nor will it appear
in an implementation).
(* TODO *)
*)

Polymorphism and subtyping are essential for programming in a generic way.
They lead to programs that are shorter, easier to understand and hence more
reliable. Although polymorphism is widespread among practical programming
languages, only limited forms of subtyping are used in practice (e.g.
OCaml's polymorphic variants or modules). Overall, subtyping is useful for
both product types (i.e. records or modules) and sum types (i.e. polymorphic
variants). It provides canonical injections between a type and its subtypes.
For example, unary natural numbers may be defined as a subtype of unary
integers.
(* TODO *)

=<

=> Related works and similar systems

To our knowledge, combining call-by-value evaluation, side-effects and
dependent products has never been achieved before. At least not for a
dependent product fully compatible with effects and call-by-value. For
example, the Aura language \cite("Jia2008") forbids dependency on terms that
are not values in dependent applications. Similarly, the $F^{★}$ language
\cite("Swamy2011") relies on (partial) let-normal forms to enforce values
in argument position. Daniel Licata and Robert Harper have defined a notion
of positively dependent types \cite("Licata2009") which only allow dependency
over strictly positive types.
(* *)
Finally, in language like ATS \cite("Xi2004") and DML \cite("Xi1999")
dependent types are limited to a specific index language.

The system that seems the most similar to ours is NuPrl
\cite("Constable1986"), although it is inconsistent with classical reasoning.
NuPrl accommodates an observational equivalence $({\sim})$ (Howe's squiggle
relation \cite("Howe1989")) similar to our $({≡})$ relation. It is partially
reflected in the syntax of the system. Being based on a Kleene style
realizability model, NuPrl can also be used to reason about untyped terms.

The central part of this paper consists in a classical realizability model
construction in the style of Jean-Louis Krivine \cite("Krivine2009"). We
rely on a call-by-value presentation which yields a model in three layers
(values, terms and stacks). Such a technique has already been used to account
for classical ML-like polymorphism in call-by-value in the work of Guillaume
Munch-Maccagnoni \cite("Munch2009"). It is here extended to include dependent
products. Note that our main theorem seems unrelated to lemma 9 in
Munch-Maccagnoni's work \cite("Munch2009").

The most actively developed proof assistants following the Curry-Howard
correspondence are Coq and Agda \mcite(["CoqTeam2004";"Norell2008"]). The
former is based on Coquand and Huet's calculus of constructions and the
latter on Martin-Löf's dependent type theory \mcite(["Coquand1988";
"Martin-Löf1982"]). These two constructive theories
provide dependent types, which allow the definition of very expressive
specifications. Coq and Agda do not directly give a computational
interpretation to classical logic. Classical reasoning can only be done
through the definition of axioms such as the law of the excluded middle.
Moreover, these two languages are logically consistent, and hence their
type-checkers only allow terminating programs. As termination checking is
a difficult (and undecidable) problem, many terminating programs are
rejected. Although this is not a problem for formalizing mathematics, this
makes programming tedious.

The TRELLYS project \cite("Casinghino2014") aims at providing a language in
which a consistent core interacts with type-safe dependently-typed
programming with general recursion. Although the language defined in
\cite("Casinghino2014") is call-by-value and allows effect, it suffers from
value restriction like Aura \cite("Jia2008"). The value restriction does not
appear explicitly but is encoded into a well-formedness judgement appearing
as the premise of the typing rule for application. Apart from value
restriction, the main difference between the language of the TRELLYS project
and ours resides in the calculus itself. Their calculus is Church-style (or
explicitly typed) while ours is Curry-style (or implicitly typed). In
particular, their terms and types are defined simultaneously, while our type
system is constructed on top of an untyped calculus.

Another similar system can be found in the work of Alexandre Miquel
\cite("Miquel2001"), where propositions can be classical and Curry-style.
However the rest of the language remains Church style and does not embed a
full ML-like language.

The PVS system \cite("Owre1996") is similar to ours as it is based on
classical higher-order logic. However this tool does not seem to be a
programming language, but rather a specification language coupled with proof
checking and model checking utilities. It is nonetheless worth mentioning
that the undecidability of PVS's type system is handled by generating proof
obligations. Our system will take a different approach and use a
non-backtracking type-checking and type-inference algorithm.

=<

=> Thesis overview

The starting point of this thesis is an untyped call-by-value language
defined in \chapter("calculus"). The terms of the language are evaluated
using an abstract environment machine, which allows us to account for both
computational effects and a form of contextual equivalence. A higher-order
type system for our language is then defined in \chapter("typeSystem"). Its
main feature is an equality type over terms that is interpreted using the
untyped notion of equivalence of \chapter("calculus"). This enables the
specification of program properties that can then be proved using equational
reasoning. The soundness and the type safety of our system are then proved
using standard classical realizability techinques. In particular, the
interpretation of types is spread among three sets related by orthogonality:
a set of values, a set of envaluation contexts and a set of terms.

The type system defined in \chapter("typeSystem") provides a weak form of
dependent function type, which can be used to quantify over terms of a
given type. However, value restriction is required on the arguments of
a dependent function, which make it practically useless.
\chapter("semValRest") provides a solution to this problem by proposing
a relaxed notion of value restriction expressed using observational
equivalence. The soundness of this new approach is proved by constructing
a somewhat surprising realizability model.

From \chapter("subtyping") onwards, a more practical approach is taken. The
type system is extended with inductive and coinductive types, which are
handled using a subtyping relation. A fixpoint operator is also provided to
allow for reccursive programs, which are shown terminating using the size
change principle. In \chapter("implem"), the implementation of the system is
presented. The source code of the prototype is distributed with this
document, and the last version is available online. (* FIXME http *)

(*
Finally, \chapter("conclusion") presents several ideas and problems that
could not be tackled in this thesis for lack of time. The limitations of
the prototype implementation are assesed, and a plan for future developpement
is suggested.
*)

=<

=<


(*
As a consequence, we may say that we follow the
//program as proof principle//, rather than the usual //proof as program
principle//. In particular, proofs can be composed as (and with) programs
to form proof tactics.

The special term ##8<## (to be pronounced "scissors") can be introduced
whenever the goal is derivable from the context, using equational reasoning.
 *)


