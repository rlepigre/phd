\Include{Epigraph}
\Include{Macros}

\Caml(
  open Diagrams
  open ProofTree
  open PMLVerbatim
  open Lang
)

=> From Programming to Program Proving \label("intro")

\linesBefore(6)
\begin{epigraph}
  //[In 1949,] as soon as we started programming, we found to our surprise
  that it wasn't as easy to get programs right as we had thought. Debugging
  had to be discovered. I can remember the exact instant when I realised that
  a large part of my life from then on was going to be spent in finding
  mistakes in my own programs.//

	\blackline()

  \begin{raggedRight}
    Maurice Wilkes (1913-2010)
  \end{raggedRight}
  \linesAfter(6)
\end{epigraph}
(* *)
Since the apparition of the very first computers, every generation of
programmers has been faced with the issue of code reliability. Statically
typed languages such as Java, Haskell, OCaml, Rust or Scala have addressed
the problem by running syntactic checks at compile time to detect incorrect
programs. Their strongly typed discipline is especially useful when several
incompatible data objects have to be manipulated together. For example, a
program computing an integer addition on a boolean (or function) argument
is immediately rejected. In recent years, the benefit of static typing has
even begun to be recognised in the dynamically typed languages community.
Static type checkers are now available for languages like Javascript
\mcite(["Microsoft2012";"Facebook2014"]) or Python \cite("Lehtosalo2014").

In the last thirty years, significant progress has been made in the
application of type theory to computer languages. The Curry-Howard
correspondence, which links the type systems of functional programming
languages to mathematical logic, has been explored in two main directions.
On the one hand, proof assistants like Coq or Agda are based on very
expressive logics. To prove their consistency, the underlying programming
languages need to be restricted to contain only programs that can be
proved terminating. As a result, they forbid the most general forms of
recursion. On the other hand, functional programming languages like OCaml
or Haskell are well-suited for programming, as they impose no restriction
on recursion. However, they are based on inconsistent logics, which means
that they cannot be used for proving mathematical formulas.

The aim of this work is to provide a uniform environment in which
programs can be designed, specified and proved. The idea is to combine a
full-fledged, ML-like programming language with an enriched type system
allowing the specification of computational behaviours. This language can
thus be used as ML for type-safe general programming, and as a proof
assistant for proving properties of ML programs. The uniformity of the
framework implies that programs can be incrementally refined to obtain
more guarantees. In particular, there is no syntactic distinction
between programs and proofs in the system. This means that programming
and proving features can be mixed when constructing proofs or programs.
For instance, proofs can be composed with programs for them to transport
properties (e.g., addition carrying its commutativity proof). In programs,
proof mechanisms can be used to eliminate dead code (i.e., portions of a
program that cannot be reached during its execution).

=> Writing functional programs

\label("sectprog")
In this thesis, our first goal is to design a type system for a practical,
functional programming language. Out of the many possible technical choices,
we decided to consider a call-by-value language similar to OCaml or SML, as
they have proved to be highly practical and efficient. Our language provides
polymorphic variants \cite("Garrigue1998") and SML style records, which are
convenient for encoding data types. As an example, the type of lists can be
defined and used as follows.
### PML "examples/demo.pml"
type rec list<a> = [Nil ; Cons of {hd : a ; tl : list}]

val rec exists : ∀a, (a ⇒ bool) ⇒ list<a> ⇒ bool = fun pred l →
  case l {
    | Nil[_]  → false
    | Cons[c] → if pred c.hd { true } else { exists pred c.tl }
  }

val rec rev_append : ∀a, list<a> ⇒ list<a> ⇒ list<a> = fun l1 l2 →
  case l1 {
    | Nil[_]  → l2
    | Cons[c] → rev_append c.tl Cons[{hd = c.hd; tl = l2}]
  }
###
The ##exists## function takes as input a predicate and a list, and it
returns a boolean indicating whether there is an element satisfying the
predicate in the list. The ##rev_append## function takes as input two
lists ##l1## and ##l2##, and it appends ##l2## to the reversal of ##l1##.
Note that these two functions are polymorphic, they can hence be applied
to lists containing elements of an arbitrary type. In our syntax for
types, polymorphism is explicitly materialised using a universally
quantified type variable.

Polymorphism is an important feature as it allows for more generic
programs. In general, ML-like languages only allow a limited form of
polymorphism on let-bindings. In these systems, generalisation can only
happen on expressions of the form "##let x = t in u##". As a consequence,
the following function is rejected by the OCaml type checker.
### OCaml
let silly_ocaml : ('a → 'a) → unit → unit option =
  fun f u → f (Some (f u))
###
In our system, polymorphism is not limited: universal quantification is
allowed anywhere in types. Our types thus contain the full power of System
F \mcite(["Girard1972" ; "Reynolds1974"]). In particular, the equivalent of
##silly_ocaml## is accepted by our type-checker.
### PML "examples/demo.pml"
val silly : (∀a, a ⇒ a) ⇒ unit ⇒ option<unit> =
  fun f u → f Some[f u]
###
In fact, System F polymorphism is not the only form of quantification that
is supported in our system. It also provides existential types, which are an
essential first step towards the encoding of a module system supporting a
notion of abstract interface. Moreover, our system is based on higher-order
logic, which means that types are not the only objects that can be quantified
over in types. In particular, we will see that quantifiers can range over
terms in the next section.

The programming languages of the ML family generally include effectful
operations, for example references (i.e., mutable variables).
Our system is no exception as it provides control operators. As first
discovered by Timothy G. Griffin \cite("Griffin1990"), control operators
like Lisp's //call/cc// can be used to give a computational interpretation
to classical logic. On the programming side, they can be used to encode an
exception mechanism. For example, the following function can be used to
compute the conjunction of a list of booleans rather efficiently.
### PML "examples/demo.pml"
val rec conjunction : list<bool> ⇒ bool = fun l → save k →
  case l {
    | Nil[_]  → true
    | Cons[c] → if c.hd { conjunction c.tl } else { restore k false }
  }
###
Here, the continuation is saved in a variable ##k## and is restored with
the value ##false## if the boolean ##false## is encountered in the list.

Our control operators can also be used to define programs whose types
correspond to logical formulas that are only valid in classical logic.
For instance, the type of the following programs corresponds to Peirce's
law, the principle of double negation elimination and the law of the
excluded middle.
### PML "examples/demo.pml"
val peirce : ∀a b, ((a ⇒ b) ⇒ a) ⇒ a =
  fun x → save k → x (fun y → restore k y)

// Usual definition of logical negation
type neg<a> = a ⇒ ∀x, x

val dneg_elim : ∀a, neg<neg<a>> ⇒ a = peirce

// Disjoint sum of two types (logical disjunction)
type either<a,b> = [InL of a ; InR of b]

val excl_mid : ∀a, {} ⇒ either<a, neg<a>> =
  fun _ → save k → InR[fun x → restore k InL[x]]
###
Note that the definition of ##excl_mid## contains a dummy function
constructor. Its presence is required for a reason related to value
restriction (see \section("valrest")). It would not be necessary if
##excl_mid## was not polymorphic in ##a##. Moreover, note that
##dneg_elim## can be defined to be exactly ##peirce## thanks to
subtyping (see \chapter("subtyping")).

From a computational point of view, manipulating continuations using
control operators can be understood as cheating. For example
##excl_mid## (or rather, ##excl_mid {}##) saves the continuation and
immediately returns a (possibly false) proof of ##neg<a>##. Now, if
this proof is ever applied to a proof of ##a## (which would result
in absurdity), the program backtracks and returns the given proof of
##a##. This interpretation in terms of cheating has been well-known
for a long time (see, for example, \id(citen "Wadler2003" "Section 4")).

=<

=> Proofs of ML programs

The system presented in this thesis is not only a programming language,
but also a proof assistant focusing on program proving. Its proof mechanism
relies on equality types of the form ##t ≡ u##, where ##t## and ##u## are
arbitrary (possibly untyped) terms of the language itself. Such an equality
type is inhabited by ##{}## (i.e., the record with no fields) if the denoted
equivalence is true, and it is empty otherwise. Equivalences are managed
using a partial decision procedure that is driven by the construction of
programs. An equational context is maintained by the type checker to keep
track of the equivalences that are assumed to be true during the
construction of proofs. This context is extended whenever a new
equation is learned (e.g., when a lemma is applied), and equations are
proved by looking for contradictions (e.g., when two different variants
are supposed equivalent).

To illustrate the proof mechanism, we will consider simple examples of
proofs on unary natural number (a.k.a. Peano numbers). Their type is
given below, together with the corresponding addition function defined
using recursion on its first argument.
### PML "examples/demo.pml"
type rec nat = [Zero ; Succ of nat]

val rec add : nat ⇒ nat ⇒ nat = fun n m →
  case n { Zero[_] → m | Succ[k] → Succ[add k m] }
###
As a first example, we will show that ##add Zero n ≡ n## for all ##n##.
To express this property we can use the type ##∀n:ι, add Zero n ≡ n##,
where ##ι## can be thought of as the set of all the usual program
values. This statement can then be proved as follows.
### PML "examples/demo.pml"
val add_z_n : ∀n:ι, add Zero n ≡ n = {}
###
Here, the proof is immediate (i.e., ##{}##) as have ##add Zero n ≡ n## by
definition of the ##add## function. Note that this equivalence holds for
all ##n##, whether it corresponds to an element of ##nat## or not. For
instance, it can be used to show ##add Zero true ≡ true##.

Let us now show that for every ##n## we have ##add n Zero ≡ n##. Although
this property looks similar to ##add_z_n##, the following proof is invalid.
### PML "examples/demo.pml"
// val add_n_z : ∀n:ι, add n Zero ≡ n = {}
###
Indeed, the equivalence ##add n Zero ≡ n## does not hold when ##n##
is not a unary natural number. In this case, the computation of
##add n Zero## will produces a runtime error. As a
consequence, we need to rely on a form of quantification that only ranges
over unary natural numbers. This can be achieved with the type
##∀n∈nat, add n Zero ≡ n##, which corresponds to a (dependent) function
taking as input a natural number ##n## and returning a proof of
##add n Zero ≡ n##. This property can then be proved using induction and
case analysis as follows.
### PML "examples/demo.pml"
val rec add_n_z : ∀n∈nat, add n Zero ≡ n = fun n →
  case n {
    | Zero[_] → {}
    | Succ[k] → let ih = add_n_z k in {}
  }
###
If ##n## is ##Zero##, then we need to show ##add Zero Zero ≡ Zero##, which
is immediate by definition of ##add##. In the case where ##n## is ##Succ[k]##
we need to show ##add Succ[k] Zero ≡ Succ[k]##. By definition of ##add##,
this can be reduced to ##Succ[add k Zero] ≡ Succ[k]##. We can then use the
induction hypothesis (i.e., ##add_n_z k##) to learn ##add k Zero ≡ k##, with
which we can conclude the proof.

It is important to note that, in our system, a program that is considered
as a proof needs to go through a termination checker. Indeed, a looping
program could be used to prove anything otherwise. For example, the following
proof is rejected.
### PML "examples/demo.pml"
// val rec add_n_z_loop : ∀n∈nat, add n Zero ≡ n = fun n →
//   let ih = add_n_z_loop n in {}
###
It is however easy to see that ##add_z_n## and ##add_n_z## are terminating,
and hence valid. In the following, we will always assume that the programs
used as proofs have been shown terminating.

There are two main ways of learning new equations in the system. On the one
hand, when a term ##t## is matched in a case analysis, a branch can only be
reached when the corresponding pattern ##C[x]## matches. In this case we
can extend the equational context with the equivalence ##t ≡ C[x]##. On the
other hand, it is possible to invoke a lemma by calling the corresponding
function. In particular, this must be done to use the induction hypothesis
in proofs by induction like in ##add_z_n## or the following lemma.
### PML "examples/demo.pml"
val rec add_n_s : ∀n m∈nat, add n Succ[m] ≡ Succ[add n m] = fun n m →
  case n {
    | Zero[_] → {}
    | Succ[k] → let ind_hyp = add_n_s k m in {}
  }
###
In this case, the equation corresponding to the conclusion of the used lemma
is directly added to the context. Of course, more complex results can be
obtained by combining more lemmas. For example, the following proves the
commutativity of addition using a proof by induction with ##add_n_z## and
##add_n_s##.
### PML "examples/demo.pml"
val rec add_comm : ∀n m∈nat, add n m ≡ add m n = fun n m →
  case n {
    | Zero[_] → let lem = add_n_z m in {}
    | Succ[k] → let ih  = add_comm k m in
                 let lem = add_n_s m k in {}
  }
###
Many more examples of proofs and programs are provided in \chapter("implem")
(and even more with the implementation of the system). Each of them (including
those in the current chapter) have been automatically checked upon the
generation of this document. They are thus correct with respect to the
implementation.

=<

=> A brief history of value restriction

\label("valrest")
A soundness issue related to side-effects and call-by-value evaluation
arose in the seventies with the advent of ML. The problem stems from a
bad interaction between side-effects and Hindley-Milner polymorphism. It was
first formulated in terms of references, as explained in
\id(citen "Wright1995" "Section 2").
To extend an ML-style language with references, the naive approach consist
in defining an abstract type ##'a ref## and polymorphic procedures with the
following signature (given in OCaml syntax).
### OCaml
  type 'a ref

  val ref  : 'a → 'a ref
  val (:=) : 'a ref → 'a → unit
  val (!)  : 'a ref → 'a
###
Here, the function ##ref## takes as input a value of some type and creates
a new reference cell containing an element of the corresponding type. The
value of a reference can then be updated using the infix operator ##(:=)##
(to be pronounced "set"), and its value can be obtained using the prefix
operator ##(!)## (to be pronounced "get").

These immediate additions quickly lead to trouble when working with
polymorphic references. The problem can be demonstrated by the following
example, which is accepted by the naive extension of the type system.
### OCaml
  let l = ref [] in
  l := [true]; (List.hd !l) + 1
###
On the first line, variable ##l## is given the polymorphic type
##'a list ref##, which can be unified both with ##bool list ref## and
##int list ref## on the second line. This is an obvious violation of
type safety, which is the very purpose of a type system.

To solve the problem, many alternative type systems were designed (e.g.
\mcite(["Tofte1990"; "Damas1982"; "Leroy1991"; "Leroy1993"])). However, they
all introduced a complexity that contrasted with the elegance and simplicity
of ML systems (for a detailed account, see \citen("Wright1995")("Section 2")
and \citen("Garrigue2004")("Section 2")). A simple and elegant solution was
finally found by Andrew Wright in the nineties. He suggested restricting
generalisation (i.e., introduction of polymorphism) to syntactic values
\mcite(["Wright1994";"Wright1995"]).

In ML, generalisation usually happens in expressions of the form
"##let x = u in t##", called let-bindings. The type-checking of such an
expression proceeds by inferring the type of the term ##u##, which may
contain unification variables. The type of ##u## is then generalized by
universally quantifying over these unification variables.
Finally, the term ##t## is type-checked under the assumption that ##x##
has the most general type of ##u##. With value restriction, the
generalisation of the type of ##u## only happens if ##u## is a
syntactic value. Consequently, the example above is rejected since
##ref []## is not a value, and hence its inferred type ##'_a list ref##
is only weakly polymorphic (i.e., it can only be unified with exactly one,
yet unknown type). Thus, it cannot be unified with both ##bool list ref##
and ##nat list ref##.

As mentioned in \section("sectprog"), the system presented in this thesis
does not include references, but control structures. One way of extending
ML with control structures is again to introduce an abstract type equipped
with polymorphic operations.
### OCaml
  type 'a cont

  val callcc : ('a cont → 'a) → 'a
  val throw  : 'a cont → 'a → 'b
###
The function ##callcc## corresponds to the control operator //call/cc//,
which was first introduced in the //Scheme// programming language. When
called, this function saves the current continuation (i.e., the current
state of the program's environment) and feeds it to the function it is
given as an argument. The continuation can be restored in the body of
this function using the ##throw## function.

As for references, the addition of control structures breaks the type safety
of ML in the presence of polymorphism. A complex counterexample was first
discovered by Robert Harper and Mark Lillibridge \cite("Harper1991").
### OCaml
  let c = callcc (fun k →
    ((fun x → x), (fun f → throw k (f, (fun _ → ())))))
  in print_string ((fst c) "Hello world!"); (snd c) (fun x → x+2)
###
Intuitively, the program first saves the continuation and builds a pair
##c## containing two functions. The first one is simply the identity
function. The second one takes a function ##f## as argument and calls
##throw## to restore the previously saved continuation. It then replaces
##c## with a pair containing ##f## and a constant function. Consequently,
the first element of the pair ##c## can be used as the (polymorphic)
identity function as long as the second element of ##c## has not been used.
However, when the second element of ##c## is called with a function ##g##,
then ##g## becomes the first element of ##c## and the computation restarts.
This is problematic since the function ##fun x → x+2## is then applied to
a value of type ##string##, which is thus fed to an integer addition.

During type checking of the let-binding, the type that is inferred for
the pair ##c## (prior to generalisation) is
##('_a → '_a) * (('_a → '_a) → unit)##.
As a consequence, if there is no value restriction, the last line of the
example is type-checked in a context where ##c## has the polymorphic type
##('a → 'a) * (('a → 'a) → unit)##. In particular, the type ##'a → 'a##
can be unified with both ##string → string## and ##int → int##.
As with references, the value restriction forbids such unifications.

Note that it is relatively easy to translate the counter example into our
language. Indeed, terms of the form ##callcc (fun k → t)## are translated
to ##save k → t## and terms of the form ##throw k u## to ##restore k u##.
Moreover, as our system contains system F, value restriction needs to be
stated differently. It appears on the typing rule for the introduction
of the universal quantifier. In the system, value restriction corresponds
to only applying this rule to terms that are values.

=<

=> Dependent functions and relaxed restriction

One of the main features of our system is a dependent function type. It is
essential for building proofs as it provides a form of typed quantification.
However, combining call-by-value evaluation, side-effects and dependent
functions is not straightforward. Indeed, if ##t## is a dependent function
of type ##∀x∈a, b<x>## and if ##u## has type ##a##, then is it not always
the case that ##t u## evaluates to a value of type ##b<u>##. As a consequence,
we need to restrict the application of dependent functions to make sure that
it is type safe. The simplest possible approach consists in only allowing
syntactic values as arguments of dependent functions, which is another
instance of the value restriction. It is however not satisfactory as it
considerably weakens the expressiveness of dependent functions. For example,
##add_n_z## cannot be used to prove
##add (add Zero Zero) Zero ≡ add Zero Zero##.
Indeed, the term ##add Zero Zero## is not a value, which means that it cannot
be used as argument of a dependent function. This problem arises very
often as proofs rely heavily on dependent functions. As a consequence,
the value restriction breaks the modularity of our proof system.

Surprisingly, our equality types provide a solution to the problem. Indeed,
they allow us to identify terms having the same observable computational
behaviour. We can then relax the restriction to terms that are equivalent
to some value. In other words, we consider that a term ##u## is a value if
we can find a value ##v## such that ##u ≡ v##. This idea can be applied
whenever value restriction was previously required. Moreover, the obtained
system is conservative over the one with the syntactic restriction. Indeed,
finding a value that is equivalent to a term that is already a value can
always be achieved using reflexivity. Although this new idea seems simple,
establishing the soundness of the obtained system is relatively subtle
\cite("Lepigre2016").

In practice, using a term as a value is not always immediate. For example,
the system is not able to directly prove that ##add n m## is a value,
provided that ##n## and ##m## are two natural numbers. It is however
possible to establish this fact internally as follows.
### PML "examples/demo.pml"
val rec add_total : ∀n m∈nat, ∃v:ι, add n m ≡ v = fun n m →
  case n {
    | Zero[_] → {}
    | Succ[k] → let ih = add_total k m in {}
  }
###
Here, ##add_total## proves that for any values ##n## and ##m## in the type
##nat##, there is some value ##v## such that ##add n m ≡ v##. Note that we
did not specifically require ##v## to be a natural number as this is
usually not necessary in practice. Thanks to ##add_total##, we can give a
proof of the associativity of our addition function.
### PML "examples/demo.pml"
val rec add_asso : ∀n m p∈nat, add n (add m p) ≡ add (add n m) p =
  fun n m p →
    let tot_m_q = add_total m p in
    case n {
      | Zero[_] → {}
      | Succ[k] → let tot_p_m = add_total k m in
                   let ih = add_asso k m p in {}
    }
###
Note that the proof requires two calls to ##add_total##. The first one is
used in both the base case and the induction case. It is required so that
the system can unfold the definition of ##add n (add m p)## according to
the head constructor of ##n##. As we are in call-by-value, we can only
reduce the definition of a function when it is applied to values. It is
the case here as ##n## is a variable and ##add m p## is equivalent to
some value, as witnessed by ##tot_m_q##. The second call to ##add_total##
is required for a similar reason in the successor case.

=<
=> Handling undecidability

Typing and subtyping are most likely to be undecidable in our system.
Indeed, it contains Mitchell's variant of System F \cite("Mitchell1991")
for which typing and subtyping are both known to be undecidable
\mcite(["Tiuryn1996" ; "Tiuryn2002" ; "Wells1994" ; "Wells1999" ]).
Moreover, as argued in \cite("Lepigre2017"), we believe that there are
no practical, complete semi-algorithms for extensions of System F like
ours. Instead, we propose an incomplete semi-algorithm that may fail or
even diverge on a typable program. In practice we almost never meet non
termination, but even in such an eventuality, the user can always
interrupt the program to obtain a relevant error message. This design
choice is a very important distinguishing feature of the system. To
our knowledge, such ideas have only be used (and implemented) in some
unpublished work of Christophe Raffalli \mcite(["Raffalli1998" ;
"Raffalli1999"]) and in \cite("Lepigre2017").

One of the most important idea that make the system practical and
possible to implement is to only work with syntax-directed typing
and subtyping rules. This means that only one of our typing rules
can be applied for each different term constructors. Similarly,
we have only two subtyping rules per type constructors: one where
it appears on the left of the inclusion, and one where it appears
on the right. As type-checking can only diverge in subtyping, an
error message can be built using the last applied typing rule.
Moreover, all the undecidability of the system is concentrated into
the management of unification variables, termination checking and
equivalence derivation.

As a proof of concept, we implemented our system in a prototype
called PML2. The last version of its source code is available
online (\website("http://lepigre.fr/these/")). Its implementation
mostly follows the typing and subtyping rules of the system given
in \chapter("subtyping"). Overall, our system provides a similar
user experience to statically typed functional language like OCaml
or Haskell. In such languages, type annotations are also required
for advanced features like polymorphic recursion.

=<
=> Related work and similar systems

To our knowledge, combining call-by-value evaluation, side-effects and
dependent products has never been achieved before. At least not for a
dependent product fully compatible with effects and call-by-value. For
example, the Aura language \cite("Jia2008") forbids dependency on terms that
are not values in dependent applications. Similarly, the $F^{★}$ language
\cite("Swamy2011") relies on (partial) let-normal forms to enforce values
in argument position. Daniel Licata and Robert Harper have defined a notion
of positively dependent types \cite("Licata2009") which only allow dependency
over strictly positive types. Finally, in languages like ATS \cite("Xi2004")
and DML \cite("Xi1999") dependent types are limited to a specific index
language.

The system that seems the most similar to ours is NuPrl
\cite("Constable1986"), although it is inconsistent with classical reasoning
and not effectful. NuPrl accommodates an observational equivalence relation
similar to ours (Howe's //squiggle// relation \cite("Howe1989")). It is
partially reflected in the syntax of the system. Being based on a Kleene
style realizability model, NuPrl can also be used to reason about untyped
terms.

The central part of this paper consists in the construction of a classical
realizability model in the style of Jean-Louis Krivine \cite("Krivine2009").
We rely on a call-by-value presentation which yields a model in three layers
(values, terms and stacks). Such a technique has already been used to account
for classical ML-like polymorphism in call-by-value in the work of Guillaume
Munch-Maccagnoni \cite("Munch2009"). It is here extended to include dependent
products. Note that our main result (\theorem("main")) is unrelated to
Lemma 9 in Munch-Maccagnoni's work \cite("Munch2009").

The most actively developed proof assistants following the Curry-Howard
correspondence are Coq and Agda \mcite(["CoqTeam2004";"Norell2008"]). The
former is based on Coquand and Huet's calculus of constructions and the
latter on Martin-Löf's dependent type theory \mcite(["Coquand1988";
"Martin-Löf1982"]). These two constructive theories
provide dependent types, which allow the definition of very expressive
specifications. Coq and Agda do not directly give a computational
interpretation to classical logic. Classical reasoning can only be done
through a negative translation or the definition of axioms such as the
law of the excluded middle. In particular, these two languages are not
effectful. However, they are logically
consistent, which means that they only accept terminating programs. As
termination checking is a difficult (and undecidable) problem, many
terminating programs are rejected. Although this is not a problem for
formalizing mathematics, this makes programming tedious. In our system,
only proofs need to be shown terminating. Moreover, it is possible to
reason about non-terminating and even untyped programs.

The TRELLYS project \cite("Casinghino2014") aims at providing a language in
which a consistent core interacts with type-safe dependently-typed
programming with general recursion. Although the language defined in
\cite("Casinghino2014") is call-by-value and effectful, it suffers from
value restriction like Aura \cite("Jia2008"). The value restriction does not
appear explicitly but is encoded into a well-formedness judgement appearing
as the premise of the typing rule for application. Apart from value
restriction, the main difference between the language of the TRELLYS project
and ours resides in the calculus itself. Their calculus is Church-style (or
explicitly typed) while ours is Curry-style (or implicitly typed). In
particular, their terms and types are defined simultaneously, while our type
system is constructed on top of an untyped calculus.

Another similar system can be found in the work of Alexandre Miquel
\cite("Miquel2001"), where propositions can be classical and Curry-style.
However the rest of the language remains Church style and does not embed a
full ML-like language.

The PVS system \cite("Owre1996") is similar to ours as it is based on
classical higher-order logic. However this tool does not seem to be a
programming language, but rather a specification language coupled with proof
checking and model checking utilities. It is nonetheless worth mentioning
that the undecidability of PVS's type system is handled by generating proof
obligations. The Why3 language \cite("Filliâtre2013") also relies on
generated proof obligations but it embeds a programming language (called
WhyML) corresponding to a very restricted subset of ML. Our system takes
a completely different approach and relies
on a non-backtracking type-checking algorithm. Although our system is
likely to be undecidable, we argue as in \cite("Lepigre2017") that this
seems not to be a problem in practice and allows for a simpler
implementation of the type system.

Several systems have been proposed for proving ML programs. ProPre
\cite("Manoury1992") relies on a notion of //algorithms//,
corresponding to equational specifications of programs. It is used in
conjunction with a type system based on intuitionistic logic. Although
it is possible to use classical logic to prove that a program meets its
specification, the underlying programming language is not effectful.
Similarly, the PAF! system implements a logic supporting proofs of
programs, but it is restricted to a purely functional subset of ML.
Another approach for reasoning about purely functional ML programs is
given in \cite("Régis-Gianas2007"), where Hoare logic is used to specify
program properties.

Finally, another approach to proving ML programs (including effectful
ones) consists in compiling a program down to a higher-order formula
\mcite(["Chargueraud2010" ; "Chargueraud2011"]). Such formulas can
then be manipulated in an external prover like Coq \cite("CoqTeam2004").
The user is thus required to master at least two languages, contrary
to our system in which programming and proving both take place in the
same language.

=<

=> Thesis overview

The starting point of this thesis is an untyped, call-by-value language. It
is defined in \chapter("calculus"), following a gentle introduction to the
$λ$-calculus and its evaluation in abstract machines. The formal definition
of our language is itself based on an abstract environment machine, which
allows us to account for computational effects easily. Another benefit of
this presentation is that it provides a natural definition of contextual
equivalence. It is given in \chapter("obsEquiv"), where a broader class of
relations is studied.

A higher-order type system for our language, together with its semantics,
is then defined in \chapter("typeSystem"). Its most singular feature is an
equality type over terms, that is interpreted using the untyped notion of
equivalence described in \chapter("calculus"). This enables the specification
of program properties that can then be proved using equational reasoning.
The adequacy of our type system with respect to its semantics is then proved
using classical realizability techniques. As our language is call-by-value,
the interpretation of types is spread among three sets related by
orthogonality: a set of values, a set of evaluation contexts and a set of
terms.

The type system defined in \chapter("typeSystem") provides a weak form of
dependent function type, which can be used to preform typed quantification.
However, value restriction is required on the arguments of dependent
functions, which makes them practically useless. \chapter("semValRest")
provides a solution to this problem by proposing a relaxed restriction
expressed using observational equivalence. The soundness of this new
approach is established by constructing a somewhat surprising realizability
model. It relies on a new instruction, that internalises our notion of
program equivalence into the reduction relation of our abstract machine.
For the definition of reduction and equivalence not to be circular, we need
to rely on a stratified construction of these relations.

In \chapter("subtyping"), a more practical approach is taken. The system
is extended with a notion of subtyping, which yields a system that can be
directly implemented with syntax-directed rules. While remaining compatible
with the realizability model of \chapter("semValRest"), our notion
of subtyping is able to handle all the connectives that do not have
algorithmic contents. This means that quantifiers and equality types are
only managed by subtyping. At the end of \chapter("subtyping"), we sketch
the extension of the system with inductive and coinductive types, and with
general recursion. To this aim, we rely on a recently submitted paper
\cite("Lepigre2017").

Finally, \chapter("implem") is dedicated to examples of programs and proofs
and to discussions on the implementation of the system. The source code of
the prototype is distributed with this document. The latest version of the
prototype, this document and other attached files are available online
(\website("http://lepigre.fr/these/")).

=<

=<
