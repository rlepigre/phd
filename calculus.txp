\Caml(open Diagrams open ProofTree)
\Include{Macros}

=> Untyped calculus and abstract machine \label("calculus")

In this chapter, we introduce the programming language that will be
considered throughout this thesis. Its operational semantics is expressed
in terms of an abstract machine, which will allow us to account for
computational effects.

=> The pure $λ$-calculus

In this thesis, we consider a programming language of the ML family,
similar to OCaml or SML. Like every functional language, its
syntax is based on the $λ$-calculus. Introduced by Alonzo Church in
the Thirties, the $λ$-calculus \cite("Church1941") is a formalism
for representing computable functions, and in particular reccursive
functions. As shown by Alan Turing in 1937, the $λ$-calculus is a
//universal model of computation// \cite("Turing1937").

(* The language of λ-terms. *)
\begin{def}\label("deflambda")
The terms of the $λ$-calculus (or $λ$-terms) are built from a countable
alphabet of variables (or $λ$-variables) denoted $\cal{V}_λ=\{x,y,z...\}$.
The set of all the $λ$-terms is denoted $Λ$ and is defined as the language
recognised by the following ||bnf|| grammar.
$$t,u ::= {x} \| {λx t} \| {t u} \hspace(4.0) x ∈ \cal{V}_λ$$
A term of the form $λx t$ is called an abstraction (or $λ$-abstractions)
and a term of the form $t u$ is called an application.
\end{def}
Intuitively, a $λ$-abstraction $λx t$ forms a fonction by binding the
variable $x$ in the term $t$. This would be denoted $x ↦ t$ in common
mathematics. Similarly, a term of the form $t u$ denotes the application
of (the function) $t$ to (the argument) $u$. This would be denoted $t(u)$
in common mathematics. 
\begin{rem}
As $λ$-terms have a tree-like structure, parenthesis are sometimes required
for disambiguation. For example, the term $λx t u$ can be read both as
$(λx t) u$ and as $λx (t u)$. To lighten the notations we will consider
application to be left-associative and abstraction to bind stronger that
application. As a consequence, we will always read the term $λx t x u$ as
$λx ((t x) u)$.
\end{rem}
\begin{rem}
The syntax of the $λ$-calculus only allows for one-place functions. To form
a function of two arguments (or more) one must rely on Curryfication.
Indeed, a function of two arguments can be seen as a function of one
argument returning a function. Following this scheme, the multiple arguments
of the function are given in turn, and not simultaneously. As an example,
the function $(x,y) ↦ x$ can be encoded as $λx λy x$.
\end{rem}

(* Free variables and bound variables. *)
Although this is not reflected explicitely in the syntax of $λ$-terms,
a $λ$-variable may play two very different roles. It can be used either as
a constant, like $y$ in the constant function $λx y$, or as a reference to
a binder, like $x$ in the identity function $λx x$. Variable binding and
the associated notions of free and bound variable are hence essential.
\begin{def}\label("freelvars")
Given a term $t$, we denote by $FV_λ(t)$ the set of its free $λ$-variables
and $BV_λ(t)$ the set of its bound $λ$-variables. These sets are defined
inductively on the structure of the term $t$.
\begin{center}
\linesBefore(4) (* FIXME hack *)
\diagram(
let contents = two_cols
 [ [<$ FV_λ(x) $>   ; <$ = $>; <$ \{x\} $>                   ]
 ; [<$ FV_λ(λx t) $>; <$ = $>; <$ FV_λ(t) \setminus \{x\} $> ]
 ; [<$ FV_λ(t u) $> ; <$ = $>; <$ FV_λ(t) ∪ FV_λ(u) $>       ]
 ; [<$ BV_λ(x) $>   ; <$ = $>; <$ ∅ $>                       ]
 ; [<$ BV_λ(λx t) $>; <$ = $>; <$ BV_λ(t) ∪ \{x\} $>         ]
 ; [<$ BV_λ(t u) $> ; <$ = $>; <$ BV_λ(t) ∪ BV_λ(u) $>       ] ]
let _ = array [`East; `Main; `West; `East; `Main; `West] contents
          ~horizontal_padding:(fun n -> if n = 3 then 10.0 else 1.0)
)
\end{center}
\end{def}
\begin{rem}
Nothing prevents a $λ$-variable to have both free and bound occurrences in
a term. For example, in $t = λx y λy x y$ the first occurrence of $y$ is
free while its second occurrence is bound. We have $y ∈ FV_λ(t) = \{y\}$
and $y ∈ BV_λ(t) = \{x, y\}$.
\end{rem}

(* Substitution, α-equivalence and β-reduction. *)
When a $λ$-abstraction (i.e. a function) is applied to an argument, we
obtain a term of the form $(λx t) u$, called a $β$-redex. The reduction
of such $β$-redexes plays an essential role in computation. Intuitively,
the reduction of the $β$-redex $(λx t) u$ will be performed by replacing
every occurence of the bound variable $x$ by $u$ in the term $t$. This
operation, called substitution, is formally defined as follows.
\begin{def}
Let $t ∈ Λ$ and $u ∈ Λ$ be two $λ$-terms, and $x ∈ \cal{V}_λ$ be a
$λ$-variable. We denote $t[x ← u]$ the term $t$ in which every free
occurrence of $x$ has been replaced by $u$. This operation is defined
inductively on the structure of $t$.
\begin{center}
\linesBefore(5) (* FIXME hack *)
\diagram(
let contents = two_cols
 [ [<$ x[x ← u] $>      ; <$ = $>; <$ u $>                   ]
 ; [<$ y[x ← u] $>      ; <$ = $>; <$ y $>                   ]
 ; [<$ (λx t)[x ← u] $> ; <$ = $>; <$ λx t $>                ]
 ; [<$ (λy t)[x ← u] $> ; <$ = $>; <$ λy t[x ← u] $>         ]
 ; [<$ (t₁ t₂)[x ← u] $>; <$ = $>; <$ t₁[x ← u] t₂[x ← u] $> ]
 ]
let _ =
  array [`East; `Main; `West; `East; `Main; `West] contents
    ~horizontal_padding:(fun n -> if n = 3 then 10.0 else 1.0)
)
\end{center}
\end{def}
Substitution is a subtle notion, and care should be taken to avoir capture
of variables. For example, let us consider the function $λx λy x$ which
takes an argument $x$ and returns a constant function with value $x$. If we
apply this function to $y$, the expected result is a constant function with
value $y$. However, if we blindly substitute $x$ with $y$ in $λy x$ we obtain
the identity function $λy y$. Indeed, the free variable $y$ has been captured
and now references a binder that had (coincidentally) the same name.

To solve this problem, we need to make sure that whenever a substitution
$t[x ← u]$ is performed, no free variable of $u$ is bound in $t$ (i.e.
$FV_λ(u) ∩ BV_λ(t) = ∅$). Although we cannot rename the free variables of
$u$, it is possible to rename the bound variables of $t$. Indeed, changing
the name of a bound variable has no effect on computational behaviour of a
term. Two terms that are equivalent up to the names of their bound variables
are said to be $α$-equivalent.
\begin{def}
The $α$-equivalence relation $({≡}_α) ⊆ Λ×Λ$ is defined, like in
\cite("Krivine1990"), as the smallest relation such that:
\begin{itemize}
\item if $x ∈ \cal{V}_λ$ then $x ≡_α x$,
\item if $t₁ ≡_α t₂$ and $u₁ ≡_α u₂$ then ${t₁ u₁} ≡_α {t₂ u₂}$,
\item if ${t₁[x₁ ← y]} \nequiv_α {t₂[x₂ ← y]}$ for only finitely many
      $y ∈ \cal{V}_λ$ then ${λx₁ t₁} ≡_α {λx₂ t₂}$.
\end{itemize}
\end{def}
\begin{lem}\label("alphalem")
Given a term $t ∈ Λ$ and a finite set of variables $V ⊆ \cal{V}_λ$, it is
always possible to find a term $t' \in Λ$ such that $t' ≡_α t$ and
$BV_λ(t') ∩ V = ∅$.
\begin{proof}
A full proof is available in \cite("Krivine1990"), Lemma 1.11.
\end{proof}
\end{lem}
\begin{def}
Let $t ∈ Λ$ and $u ∈ Λ$ be two $λ$-terms, and $x ∈ \cal{V}_λ$ be a
$λ$-variable. We denote $t[x := u]$ the capture-avoiding substitution of
$x$ by $u$ in $t$. It is defined as $t'[x ← u]$ where $t' ∈ Λ$ is
a term such that $t' ≡_α t$ and $BV_λ(t') ∩ FV(u) = ∅$. Such a term exists
according to \lemRef("alphalem").
\end{def}

=<

=> Evaluation contexts and reduction

In order to define the most general notion of reduction over $λ$-terms, we
need to be able to refer to any $β$-redex in a term. To this aim, we
introduce the notion of evaluation context. Intuitively, a context will
consist in a term with a hole (i.e. a place-holder for a subterm).
A context will thus allow us to focus on any particular subterm of a term.
\begin{def}
The set of evaluation contexts $[Λ]$ is defined as the language recognised
by the following ||bnf|| grammar.
$$ E ::= [{\wc}] \| {λx E} \| {E t} \| {t E} \hspace(4.0) x∈\cal{V}_λ, t∈Λ$$
\end{def}
\begin{def}
Given a term $u ∈ Λ$ and an evaluation context $E ∈ [Λ]$, we $E[u]$ the term
formed by puting $u$ into the hole of the evaluation context $E$. It is
defined by induction on the structure of $E$ as follows.
\begin{center}
\diagram(
let contents = two_cols
 [ [<$ [{\wc}][u] $>; <$ = $>; <$ u       $> ]
 ; [<$ (λx E)[u]  $>; <$ = $>; <$ λx E[u] $> ]
 ; [<$ (E t)[u]   $>; <$ = $>; <$ E[u] t  $> ]
 ; [<$ (t E)[u]   $>; <$ = $>; <$ t E[u]  $> ]
 ]
let _ =
  array [`East; `Main; `West; `East; `Main; `West] contents
    ~horizontal_padding:(fun n -> if n = 3 then 10.0 else 1.0)
)
\end{center}
\end{def}
\begin{def}\label("generalbeta")
The general $β$-reduction relation is defined as the smallest relation
$({→}_β)$ such that for every evaluation context $E$, for every terms
$t ∈ Λ$ and $u ∈ Λ$, and for every variable $x ∈ \cal{V}_λ$ we have the
following.
$$ {E[(λx t) u]} →_β {E[t[x := u]]}$$
We say that the term $t ∈ Λ$ is in $β$-normal-form if there is no $u ∈ Λ$
such that $t →_β u$. We denote $({→}_β^{∗})$ the reflexive, transitive
closure of $({→}_β)$.
\end{def}

The general $β$-reduction relation $({→}_β)$ is non-deterministic. Indeed,
given a term $t$, there might be two (different) terms $u₁$ and $u₂$ such
that $t →_β u₁$ and $t →_β u₂$. However, $({→}_β)$ has the Church-Rosser
property \cite("Church1936").
\begin{thm}
Let $t ∈ Λ$ be a term. If there are $u₁ ∈ Λ$ and $u₂ ∈ Λ$ such that
$t →_β u₁$ and $t →_β u₂$, then there must be $u ∈ Λ$ such that
${u₁ →_β u}^{∗}$ and ${u₂ →_β u}^{∗}$.
\begin{proof}
A full proof is available in \cite("Church1936").
\end{proof}
\end{thm}
Intuitivly, the Church-Rosser property enforces a weak form of determinism.
Indeed, it implies that a program can only compute one particular result,
even if it can be attained in several different ways.

=<

=> Evaluation strategies

In this thesis, we consider a programming language that is effectful, hence
it does not have the Church-Rosser property. As a consequence, we need to
choose an evaluation order for the language to remain deterministic enough
(in our case completely deterministic). The two evaluation orders that are
the most widely used in practice are call-by-name and call-by-value. They
both reduce outermost $β$-redexes first, which means that they can be
formalised without the use of evaluation contexts. In call-by-name, the terms
that are in function position are reduced first. The computation of the terms
that are in argument position is postponed to the time of their effective
use. In call-by-value, the arguments are evaluated first, before performing
the application.
\begin{def}
The call-by-name reduction relation $({→}_N) ⊆ Λ×Λ$ is the smallest relation
satisfying the following two rules.
$$
  \axiomR{{(λx t) u} →_N {t[x := u]}}
  \hspace(6.0)
  \unaryR{t →_N t'}{{t u} →_N {t' u}}
$$
\end{def}
\begin{rem}
In this thesis, we will often define relations using deduction rule
systems. A deduction rule is formed using premisses $\{P_i\}_{1≤i≤n}$ and
a conclusion $C$ separated by an horizontal bar.
$$ \ternaryR{P₁}{...}{P_n}{C} $$
The meaning of such a rules is that the conclusion $C$ can be deduced when
all the premisses $P_i$ are true. Note that there might be no premisse, and
in this case the conclusion can be deduced immediatly.
\end{rem}
In call-by-value, both the function and its argument need to be fully
evaluated before the application can be performed. Consequently, two
different strategies can be defined: left-to-right and right-to-left
call-by-value evaluation. The evaluation order that is the most widely
used in practice is left-to-right call-by-value. However, some practical
languages like OCaml use right-to-left evaluation.
\begin{def}
A term $t$ is said to be a value if it is either a $λ$-variable or a
$λ$-abstraction. We will use the letters $v$ and $w$ to denote values.
\end{def}
\begin{def}
The left-to-right call-by-value reduction relation $({→}_S) ⊆ Λ×Λ$ is
the smallest relation satisfying the following rules.
$$
  \unaryR{v value}{{(λx t) v} →_S {t[x := v]}}
  \hspace(4.0)
  \unaryR{t →_S t'}{{t u} →_S {t' u}}
  \hspace(4.0)
  \unaryR{u →_S u'}{{(λx t) u} →_S {(λx t) u'}}
$$
\end{def}
\begin{def}
The right-to-left call-by-value reduction relation $({→}_V) ⊆ Λ×Λ$ is the
smallest relation satisfying the following rules.
$$
  \unaryR{v value}{{(λx t) v} →_V {t[x := v]}}
  \hspace(4.0)
  \unaryR{u →_V u'}{{t u} →_V {t u'}}
  \hspace(4.0)
  \binaryR{t →_V t'}{v value}{{t v} →_V {t' v}}
$$
\end{def}

In this thesis, $λ$-terms and programs in general will be evaluated in an
abstract machine called a Krivine machine \cite("Krivine2007"). This
machine will emulate the right-to-left evaluation relation $({→}_V)$.

=<

=> Call-by-value Krivine machine

In the previous section, we explicited the syntax of the $λ$-calculus and
the evaluation of $λ$-terms. We will now reformulate these definition in
terms of a call-by-value Krivine abstract machine \cite("Krivine2007").
Our presentation will differ from the original machine, which is
call-by-name. Although call-by-value Krivine machines have rarely been
published, they are well-known among classical realizability experts.

\begin{def}
Values, terms, stacks and processes are generated by the following four
||bnf|| grammars. The names of the corresponding sets are displayed on
the right.
\begin{center}
\linesBefore(6)
\diagram(
let _ = array [`East ; `East ; `West; `West]
  ~horizontal_padding:(function 3 -> 20.0 | _ -> 1.0)
  [ [ <$v,w$>; <$::=$>; <$ x \| {λx t} $>          ; <$(Λ_{val})$> ]
  ; [ <$t,u$>; <$::=$>; <$ v \| {t u} $>           ; <$(Λ)$>       ]
  ; [ <$π,ρ$>; <$::=$>; <$ ε \| {v ⋅ π} \| [t] π $>; <$(Π)$>       ]
  ; [ <$p,q$>; <$::=$>; <$ t ∗ π $>                ; <$(Λ × Π)$>   ] ]
)
\end{center}
\end{def}
The syntactic distinction between terms and values is specific to the
call-by-value presentation, they would be collapsed in call-by-name.
Intuitively, a stack can be thought of as an evaluation context represented
as a list of terms and values. The values are to be considered as arguments
to be fed to the term in the context, and the terms are to be considered as
functions to which the term in the context will be applied. The symbol $ε$
is used to denote an empty stack. A process $t ∗ π$ is to be considered as
the state of our abstract machine. Its reduction will consist in the
interaction between the term $t$ and its evaluation context encoded into
a stack $π$.

As we are considering a call-by-value calculus, only values are (and should
be) substituted to $λ$-variables during evaluation. From now on, we will
hence work with the following definition of substitution. In particular,
a substitution of the form $t[x := u]$ will be forbidden if $u$ is not a
syntactic value.
\begin{def}
Let $t ∈ Λ$ be a term, $x ∈ \cal{V}_λ$ be a $λ$-variable and $v \in Λ_{val}$
be a value. We denote $t[x := v]$ the capture-avoiding substitution of $x$
by $v$ in $t$.
\end{def}
\begin{def}
The reduction relation $({\succ}) ⊆ (Λ×Π) × (Λ×Π)$ is defined as the smallest
relation satisfying the following rules. We will denote $({\succ}^{∗})$ its
reflexive and transitive closure.
\begin{center}
\linesBefore(4)
\diagram(
let _ = array [`East ; `Main ; `West]
  ~horizontal_padding:(function _ -> 5.0)
  [ [ <${t u} ∗ π$>     ; <$\succ$>; <$u ∗ {[t]π}$>       ]
  ; [ <$v ∗ {[t] π}$>   ; <$\succ$>; <$t ∗ {v·π}$>        ]
  ; [ <$λx t ∗ {v · π}$>; <$\succ$>; <${t[x := v]} ∗ π$>  ] ]
)
\end{center}
\end{def}
Three reduction rules are used to handle call-by-value evaluation. When an
application is encountered, the function is stored in a stack-frame in order
to evaluate its argument first. Once the argument has been completely
computed, a value faces the stack-frame containing the function. At this
point the function can be evaluated and the value is stored in the stack
ready to be consumed by the function as soon as it evaluates to a
$λ$-abstraction. A capture-avoiding substitution can then be performed to
effectively apply the argument to the function. For example, the process
${(λx x y) λz z} ∗ ε$ reduces in the following way.
$$
  {{(λx x y) λz z} ∗ ε}
  \succ
  {{λz z} ∗ {[λx x y]ε}}
  \succ
  {{λx x y} ∗ {{λz z} · ε}}
  \succ
  {{(λz z) y} ∗ ε}
  \succ
  {y ∗ ε}
$$
\begin{rem}
The choice of right-to-left call-by-value allows for a simpler abstract
machine. Indeed, an additional stack constructor and reduction rule would
be required to implement the more usual left-to-right evaluation.
\end{rem}

=<

=> Computational effects and $λμ$-calculus

We are now going to extend the calculus and our abstract machine with
operations allowing the manipulation of the stack. More precisely, we
will provide a way to save the stack (i.e. the evaluation context or the
continuation), so that it can be restored at a later stage. A natural
way to extend our language is to use the syntax of Michel Parigot's
$λμ$-calculus \cite("Parigot1992"). We hence introduce a new binder $μα t$
capturing the current stack in the $μ$-variable $α$. It can then be
restored in $t$ using the syntax $[α]u$.
\begin{def}
Let $\cal{V}_μ = \{α, β, γ...\}$ be a countable set of $μ$-variables (or
stack variables) disjoint from $\cal{V}_λ$. Value, terms, stacks and
processes are now generated by the following grammars. The names of the
corresponding sets are displayed on the right.
\begin{center}
\linesBefore(6)
\diagram(
let _ = array [`East ; `East ; `West; `West]
  ~horizontal_padding:(function 3 -> 20.0 | _ -> 1.0)
  [ [<$v,w$>; <$::=$>; <$ x \| {λx t} $>                   ; <$(Λ_{val})$>]
  ; [<$t,u$>; <$::=$>; <$ v \| {(t) u} \| {μα t} \| [π]t $>; <$(Λ)$>      ]
  ; [<$π,ρ$>; <$::=$>; <$ ε \| α \| {v ⋅ π} \| [t] π $>    ; <$(Π)$>      ]
  ; [<$p,s$>; <$::=$>; <$ t ∗ π $>                         ; <$(Λ × Π)$>  ] ]
)
\end{center}
\end{def}
Note that terms of the form $[π]t$ will only be available to the user if
$π$ is a stack variable. Allowing arbitrary stacks allow us to substitute
$μ$-variables by stacks during computation. Like with $λ$-variable, we will
need to be careful and avoid variable capture. However, we will not give
the full details this time.
\begin{def}
Given a value, term, stack or process $ψ$, we denote $FV_λ(ψ)$ (resp.
$BV_λ(ψ)$) the set of its free (resp. bound) $λ$-variables and $FV_μ(ψ)$
(resp. $BV_μ(ψ)$) the set of its free (resp. bound) $μ$-variables. These
sets are defined in a similar way to \defRef("freelvars").
\end{def}
\begin{def}
Let $t ∈ Λ$ be a term, $π ∈ Π$ be a stack and $α ∈ \cal{V}_μ$ be a
$μ$-variable. We denote $t[α := π]$ the (capture-avoiding) substitution of
$α$ by $π$ in $t$.
\end{def}
\begin{def}
The reduction relation $({\succ})$ is extended with two new reduction rules.
\begin{center}
\linesBefore(7)
\diagram(
let _ = array [`East ; `Main ; `West]
  ~horizontal_padding:(function _ -> 5.0)
  [ [ <${(t) u} ∗ π$>   ; <$\succ$>; <$u ∗ {[t]π}$>       ]
  ; [ <$v ∗ {[t] π}$>   ; <$\succ$>; <$t ∗ {v·π}$>        ]
  ; [ <$λx t ∗ {v · π}$>; <$\succ$>; <${t[x := v]} ∗ π$>  ]
  ; [ <$μα t ∗ π$>      ; <$\succ$>; <${t[α := π]} ∗ π$>  ]
  ; [ <$[ρ]t ∗ π$>      ; <$\succ$>; <$t ∗ ρ$>            ] ]
)
\end{center}
\end{def}
Now, when the abstract machine encounters a $μ$-abstraction $μα t$, the
current stack $π$ is substituted to the $μ$-variables $α$. As a consequence,
every subterm of the form $[α]u$ in $t$ becomes $[π]u$. When the machine
then reaches a state of the form $[π]u ∗ ρ$, the current stack $ρ$ is erased,
and computation resumes with the stored stack $π$. For example, the processus
$λx μα Ω_x [α]x ∗ v · ε$ where $Ω_x$ is an arbitrary term and $v$ is an
arbitrary value reduces as follows.
$$
  {{λx μα Ω_x [α]x} ∗ {v · ε}}
  \succ
  {{μα Ω_v [α]v} ∗ ε}
  \succ
  {{Ω_v [ε]v} ∗ ε}
  \succ
  {{[ε]v} ∗ {[Ω_v] ε}}
  \succ
  {v ∗ ε}
$$
Note that when a stack is erased, arbitrary terms might be erased. In
particular, we could have chosed $Ω_x = Ω = (λx x x) λx x x$ in the
previous example, although the reduction of this term does not terminate.
Indeed, we have
$$
  {Ω ∗ π}
  \succ
  {{λx x x} ∗ {[λx x x] π}}
  \succ
  {{λx x x} ∗ {{λx x x} · π}}
  \succ
  {Ω ∗ π}
$$
for every possible stack $π$.

=<

=> Records, variants and recursion

... (* TODO *)

=<

=> Full syntax and operational semantics

The programming language that is considered in this thesis is expressed in
terms of a //Krivine Abstract Machine// \cite("Krivine2007"). It is formed
using four syntactic entities: values, terms, stacks and processes. The
distinction between terms and values is specific to our call-by-value
presentation, they would be collapsed in call-by-name.
\begin{def}
We require three distinct, countable sets of variables:
$\cal{V}_λ = \{x, y, z...\}$ for $λ$-variables,
$\cal{V}_μ = \{α, β, γ...\}$ for $μ$-variables and
$\cal{V}_ι = \{a, b, c...\}$ for term variables.
We also require a contable set $\cal{L} = \{l, l₁, l₂...\}$ of labels to
name record fields and a countable set $\cal{C} = \{C, C₁, C₂...\}$ for
naming constructors.
\end{def}
\begin{def}
Value, terms, stacks and processes are mutually inductively defined as the
languages recognised by the following grammars. The names of the
corresponding sets are displayed on the right.
\begin{center}
\linesBefore(6)
\diagram(
let _ = array [`East ; `East ; `West; `West]
  ~horizontal_padding:(function 3 -> 10.0 | _ -> 1.0)
  [ [ <$v,w$> ; <$::=$>
    ; <$ x \| {λx t} \| \{(l_i = v_i)_{i∈I}\} \| {C[v]} $>
    ; <$(Λ_{val})$> ]
  ; [ <$t,u$> ; <$::=$>
    ; <$a \| v \| {t u} \| {μα t} \| {[π]t} \| {v{.}l} \|
       {[v \| (C_i → t_i)_{i∈I}]} \| {Y_{t,v}} \| {{unit}_v} $>
    ; <$(Λ)$> ]
  ; [ <$π,ρ$> ; <$::=$>
    ; <$ ε \| α \| {v⋅π} \| {[t]π} $>
    ; <$(Π)$> ]
  ; [ <$p,s$> ; <$::=$>
    ; <$ t ∗ π $>
    ; <$(Λ × Π)$> ] ]
)
\end{center}
\end{def}
Terms and values form a variation of the $λμ$-calculus \cite("Parigot1992"),
enriched with ML-like constructs (i.e. records and variants). One may note
that values are terms. A stack can be either the empty stack $ε$, a stack
variable, a value pushed on top of a stack, or a stack frame containing a
term on top of a stack. These two constructors are specific to the
call-by-value presentation, only one would be required in call-by-name.
(* TODO *)

\begin{def}
The relation $({\succ}) ⊆ (Λ×Π)²$ is defined as the smallest relation
satisfying the following reduction rules.
\begin{center}
\diagram(
let _ = array [`East ; `Main ; `West]
  ~horizontal_padding:(function _ -> 5.0)
  [ [ <${t u} ∗ π$>
    ; <$\succ$>; <$u ∗ {[t]π}$> ]
  ; [ <$v ∗ {[t] π}$>
    ; <$\succ$>; <$t ∗ {v·π}$> ]
  ; [ <$λx t ∗ {v · π}$>
    ; <$\succ$>; <${t[x := v]} ∗ π$> ]
  ; [ <$μα t ∗ π$>
    ; <$\succ$>; <${t[α := π]} ∗ π$> ]
  ; [ <$[ρ]t ∗ π$>
    ; <$\succ$>; <$t ∗ ρ$> ]
  ; [ <$\{(l_i = v_i)_{i∈I}\}.l_k ∗ π$>
    ; <$\succ$>; <$v_k ∗ π \hspace(2.6) when \hspace(0.4) k∈I$> ]
  ; [ <$[C_k[v] \| (C_i → t_i)_{i∈I}] ∗ π$>
    ; <$\succ$>; <$t_k v ∗ π \hspace(2.) when \hspace(0.4) k∈I$> ]
  ; [ <$Y_{t,v} ∗ π$>
    ; <$\succ$>; <$t ∗ {{λx Y_{t,x}}·v·π}$> ]
  ; [ <${unit}_{\{\id([])\}} ∗ π$>
    ; <$\succ$>; <$\{\id([])\} ∗ π$> ]
  ]
)
\end{center}
We will denote $({\succ}^{+})$ its transitive closure, $({\succ}^{∗})$ its
reflexive-transitive closure and $({\succ}^k)$ its $k$-fold application.
\end{def}

=<

=<

(*
\begin{remark}
We enforce values in constructors, record fields, projection and case
analysis. This makes the calculus simpler because only $\beta$-reduction
will manipulate the stack. We can define syntactic sugars such as the
following to hide the restriction from the programmer.
$$ t.l\;:=\;(\lambda x\,x.l)\,t \hspace{2cm} C[t]\;:=\;(\lambda x\,C[x])\,t $$
\end{remark}
\begin{definition}
Given a value, term, stack or process $\psi$ we denote $FV_\lambda(\psi)$
(resp. $FV_\mu(\psi)$, $TV(\psi)$) the set of free $\lambda$-variables (resp.
free $\mu$-variables, term variables) contained in $\psi$. We say that $\psi$
is closed if it does not contain any free variable of any kind. The set of
closed values and the set of closed terms are denoted $\Lambda_v^\ast$
and $\Lambda^\ast$ respectively.
\end{definition}
\begin{remark}
A stack, and hence a process, can never be closed as they always at least
contain a stack variable.
\end{remark}

\subsection{Call-by-value reduction relation}

Processes form the internal state of our abstract machine. They are to be
thought of as a term put in some evaluation context represented using a stack.
Intuitively, the stack $\pi$ in the process $t \ast \pi$ contains the
arguments to be fed to $t$. Since we are in call-by-value the
stack also handles the storing of functions while their arguments are being
evaluated. This is why we need stack frames (i.e. stacks of the form
$[t] \pi$). The operational semantics of our language is given by a relation
$(\succ)$ over processes.
\begin{definition}
The relation $(\succ) \subseteq (\Lambda\times\Pi)^2$ is defined as the
smallest relation satisfying the following reduction rules.
\begin{align*}
t\;u \ast \pi
  \;\;\;\;&\succ\;\;\;
  u \ast [t] \pi\\
v \ast [t] \pi
  \;\;\;\;&\succ\;\;\;
  t \ast v.\pi\\
\lambda x\;t \ast v.\pi
  \;\;\;\;&\succ\;\;\;
  t[x \!:=\! v] \ast \pi\\
\mu \alpha\;t \ast \pi
  \;\;\;\;&\succ\;\;\;
  t[\alpha \!:=\! \pi] \ast \pi\\
p \ast \pi
  \;\;\;\;&\succ\;\;\;
  p\\
  \{l_i = v_i\}_{i \in I}.l_k \ast \pi
  \;\;\;\;&\succ\;\;\;
  v_k \ast \pi & {k \in I}\\
\tcase{C_k[v]}{C_i[x_i] \to t_i}_{i \in I} \ast \pi
  \;\;\;\;&\succ\;\;\;
t_k[x_k \!:=\! v] \ast \pi & {k \in I}
\end{align*}
We will denote $(\succ^{+})$ its transitive closure, $(\succ^{*})$ its
reflexive-transitive closure and $(\succ^k)$ its $k$-fold application.
\end{definition}
The first three rules are those that handle $\beta$-reduction. When the
abstract machine encounters an application, the function is stored in a
stack-frame in order to evaluate its argument first. Once the argument has
been completely computed, a value faces the stack-frame containing the
function. At this point the function can be evaluated and the value is stored
in the stack ready to be consumed by the function as soon as it evaluates to a
$\lambda$-abstraction. A capture-avoiding substitution can then be performed
to effectively apply the argument to the function. The fourth and fifth rules
rules handle the classical part of computation. When a $\mu$-abstraction is
reached, the current stack (i.e. the current evaluation context) is captured
and substituted for the corresponding $\mu$-variable. Conversely, when a process
is reached, the current stack is thrown away and evaluation resumes with the
process. The last two rules perform projection and case analysis in the
expected way. Note that for now, states of the form $\delta_{v,w} \ast \pi$
are unaffected by the reduction relation.
\begin{remark}
For the abstract machine to be simpler, we use right-to-left call-by-value
evaluation, and not the more usual left-to-right call-by-value evaluation.
\end{remark}

\begin{lemma}\label{redcompatall}
The reduction relation $(\succ)$ is compatible with substitutions of variables
of any kind. More formally, if $p$ and $q$ are processes such that $p \succ q$
then:
\begin{itemize}
\item for all $x \in \mathcal{V}_\lambda$ and $v \in \Lambda_v$,
  $p[x := v] \succ q[x := v]$,
\item for all $\alpha \in \mathcal{V}_\mu$ and $\pi \in \Pi$,
  $p[\alpha := \pi] \succ q[\alpha := \pi]$,
\item for all $a \in \mathcal{V}_\iota$ and $t \in \Lambda$,
  $p[a := t] \succ q[a := t]$.
\end{itemize}
Consequently, if $\sigma$ is a substitution for variables of any kind and if
$p \succ q$ (resp. $p \succ^{*} q$, $p \succ^{+} q$, $p \succ^k q$) then 
$p\sigma \succ q\sigma$ (resp. $p\sigma \succ^{*} q\sigma$,
$p\sigma \succ^{+} q\sigma$, $p\sigma \succ^k q\sigma$).
\end{lemma}
\begin{proof}
  Immediate case analysis on the reduction rules.
\end{proof}
\medskip

We are now going to give the vocabulary that will be used to describe some
specific classes of processes. In particular we need to identify processes
that are to be considered as the evidence of a successful computation, and
those that are to be recognised as expressing failure.
\begin{definition}
A process $p \in \Lambda\times\Pi$ is said to be:
\begin{itemize}
\item \emph{final} if there is a value $v \in \Lambda_v$ and a stack variable
  $\alpha \in \mathcal{V}_\mu$ such that $p = v \ast \alpha$,
\item \emph{$\delta$-like} if there are values $v, w \in \Lambda_v$ and a
  stack $\pi \in \Pi$ such that $p = \delta_{v,w} \ast \pi$,
\item \emph{blocked} if there is no $q \in \Lambda\times\Pi$ such that
  $p \succ q$,
\item \emph{stuck} if it is not final nor $\delta$-like, and if for every
  substitution $\sigma$, $p\sigma$ is blocked,
\item \emph{non-terminating} if there is no blocked process
  $q \in \Lambda\times\Pi$ such that $p \succ^{*} q$.
\end{itemize}
\end{definition}

\begin{lemma}\label{redstable}
Let $p$ be a process and $\sigma$ be a substitution for variables of any kind.
If $p$ is $\delta$-like (resp. stuck, non-terminating) then $p\sigma$ is also
$\delta$-like (resp. stuck, non-terminating).
\end{lemma}
\begin{proof}
  Immediate by definition.
\end{proof}

\begin{lemma}\label{remark}
A stuck state is of one of the following forms, where $k \notin I$.
\begin{center}
\begin{tabular}{c}
$
C[v].l \ast \pi
\quad\quad\quad
(\lambda x\;t).l \ast \pi
\quad\quad\quad
C[v] \ast w.\pi
\quad\quad\quad
\{l_i = v_i\}_{i \in I} \ast v.\pi
$
\medskip
\\
$
\tcase{\lambda x\;t}{C_i[x_i] \to t_i}_{i \in I} \ast \pi
\quad\quad\quad
\tcase{\{l_i = v_i\}_{i \in I}}{C_j[x_j] \to t_j}_{j \in J} \ast \pi
$
\medskip
\\
$
\tcase{C_k[v]}{C_i[x_i] \to t_i}_{i \in I} \ast \pi
\quad\quad\quad
\{l_i = v_i\}_{i \in I}.l_k \ast \pi
$
\end{tabular}
\end{center}
\end{lemma}
\begin{proof}
  Simple case analysis.
\end{proof}

\begin{lemma}\label{possibilities}
A blocked process $p \in \Lambda\times\Pi$ is either stuck, final,
$\delta$-like, or of one of the following forms.
\begin{center}
\begin{tabular}{c}
$
x.l \ast \pi
\quad\quad\quad
x \ast v.\pi
\quad\quad\quad
\tcase{x}{C_i[x_i] \to t_i}_{i \in I} \ast \pi
\quad\quad\quad
a \ast \pi
$
\end{tabular}
\end{center}
\end{lemma}
\begin{proof}
Straight-forward case analysis using lemma \ref{remark}.
\end{proof}

\subsection{Reduction of $\delta_{v,w}$ and equivalence}

The idea now is to define a notion of observational equivalence over terms
using a relation $(\equiv)$. We then extend the reduction relation with a rule
reducing a state of the form $\delta_{v,w} \ast \pi$ to $v \ast \pi$ if
$v \not\equiv w$. If $v \equiv w$ then $\delta_{v,w}$ is stuck. With this rule
reduction and equivalence will become interdependent as equivalence will be
defined using reduction.
\begin{definition}
Given a reduction relation $R$, we say that a process $p \in \Lambda\times\Pi$
converges, and write $p \converge_R$, if there is a final state
$q \in \Lambda\times\Pi$ such that $p R^{*} q$ (where $R^{*}$ is the
reflexive-transitive closure of $R$). If $p$ does not converge we say that it
diverges and write $p \diverge_R$. We will use the notations $p \converge_i$
and $p \diverge_i$ when working with indexed notation symbols like $(\red_i)$.
\end{definition}

\begin{definition}
For every natural number $i$ we define a reduction relation $(\red_i)$ and an
equivalence relation $(\equiv_i)$ which negation will be denoted
$(\not\equiv_i)$.
$$ (\red_i) = (\succ) \cup \{(\delta_{v,w} \ast \pi, v \ast \pi) \;|\;
  \exists j < i, v \not\equiv_j w\} $$
$$ (\equiv_i) = \{(t,u) \;|\; \forall j \leq i, \forall \pi, \forall
  \sigma, {t\sigma \ast \pi \converge_j} \Leftrightarrow 
  {u\sigma \ast \pi \converge_j}\} $$
\end{definition}
It is easy to see that $(\red_0) = (\succ)$. For every natural number $i$,
the relation $(\equiv_i)$ is indeed an equivalence relation as it can be
seen as an intersection of equivalence relations. Its negation can be
expressed as follows.
$$ (\not\equiv_i) = \{(t,u), (u,t) \;|\; \exists j \leq i, \exists \pi,
  \exists \sigma, {t\sigma \ast \pi \converge_j} \land
  {u\sigma \ast \pi \diverge_j}\} $$

\begin{definition}
We define a reduction relation $(\red)$ and an equivalence relation $(\equiv)$
which negation will be denoted $(\not\equiv)$.
$$ (\red) = \bigcup_{i\in\bbN} {(\red_i)}
   \;\;\;\;\;\;
   \;\;\;\;\;\;
   (\equiv) = \bigcap_{i\in\bbN} {(\equiv_i)} $$
\end{definition}
These relations can be expressed directly (i.e. without the need of a union or
an intersection) in the following way.
\begin{align*}
(\equiv) &= \{(t,u) \;|\; \forall i, \forall \pi, \forall \sigma,
  {t\sigma \ast \pi \converge_i} \Leftrightarrow 
  {u\sigma \ast \pi \converge_i}\}\\
(\not\equiv) &= \{(t,u), (u,t) \;|\; \exists i, \exists \pi, \exists \sigma,
  {t\sigma \ast \pi \converge_i} \land {u\sigma \ast \pi \diverge_i}\}\\
(\red) &= (\succ) \cup \{(\delta_{v,w} \ast \pi, v \ast \pi) \;|\;
  v \not\equiv w\}
\end{align*}

\begin{remark}
Obviously $(\red_i) \subseteq (\red_{i+1})$ and
$(\equiv_{i+1}) \subseteq (\equiv_i)$.
As a consequence the construction of $(\red_i)_{i\in\bbN}$ and
$(\equiv_i)_{i\in\bbN}$ converges. In fact $(\red)$ and $(\equiv)$ form a
fixpoint at ordinal $\omega$. Surprisingly, this property is not explicitly
required.
\end{remark}

\begin{theorem}\label{equivpole}
Let $t$ and $u$ be terms. If $t \equiv u$ then for every stack $\pi \in \Pi$
and substitution $\sigma$ we have $t\sigma \ast \pi \converge_{\red}
\Leftrightarrow u\sigma \ast \pi \converge_{\red}$.
\end{theorem}
\begin{proof}
We suppose that $t \equiv u$ and we take $\pi_0 \in \Pi$ and a substitution
$\sigma_0$. By symmetry we can assume that
${t\sigma_0 \ast \pi_0} \converge_\red$ and show that
${u\sigma_0 \ast \pi_0} \converge_\red$. By definition there is $i_0 \in \bbN$
such that ${t\sigma_0 \ast \pi_0} \converge_{i_0}$. Since $t \equiv u$ we know
that for every $i \in \bbN$, $\pi \in \Pi$ and substitution $\sigma$ we have
${t\sigma \ast \pi} \converge_i \Leftrightarrow {u\sigma \ast \pi}
\converge_i$. This is true in particular for $i = i_0$, $\pi = \pi_0$ and
$\sigma = \sigma_0$. We hence obtain ${u\sigma_0 \ast \pi_0}
\converge_{i_0}$ which give us ${u\sigma_0 \ast \pi_0} \converge_\red$.
\end{proof}
\begin{remark}
The converse implication is not true in general: taking
$t = \delta_{\lambda x\,x,\{\}}$ and $u = \lambda x\,x$ gives a
counter-example. More generally
${p\converge_\red} \Leftrightarrow {q\converge_\red}$ does not necessarily
imply  ${p\converge_i} \Leftrightarrow {q\converge_i}$ for all
$i\in\mathbb{N}$. 
\end{remark}
\begin{corollary}\label{eqconvconv}
Let $t$ and $u$ be terms and $\pi$ be a stack. If $t \equiv u$ and
${t \ast \pi} \converge_\red$ then ${u \ast \pi} \converge_\red$.
\end{corollary}
\begin{proof}
Direct consequence of theorem \ref{equivpole} using $\pi$ and an empty
substitution.
\end{proof}

\subsection{Extensionality of the language}

In order to be able to work with the equivalence relation $(\equiv)$, we need
to check that it is extensional. In other words, we need to be able to replace
equals by equals at any place in terms without changing their observed
behaviour. This property is summarized in the following two theorems.

\begin{theorem}\label{extval}
Let $v$ and $w$ be values, $E$ be a term and $x$ be a $\lambda$-variable. If
$v \equiv w$ then $E[x := v] \equiv E[x := w]$.
\end{theorem}
\begin{proof}
We are going to prove the contrapositive so we suppose
$E[x := v] \not\equiv E[x := w]$ and show $v \not\equiv w$. By definition
there is $i \in \bbN$, $\pi \in \Pi$ and a substitution $\sigma$ such
that $(E[x := v])\sigma \ast \pi \converge_i$ and
$(E[x := w])\sigma \ast \pi \diverge_i$ (up to symmetry). Since we can
rename $x$ in such a way that it does not appear in $dom(\sigma)$, we can
suppose $E\sigma[x := v\sigma] \ast \pi \converge_i$ and
$E\sigma[x := w\sigma] \ast \pi \diverge_i$.
In order to show $v \not\equiv w$ we need to find $i_0 \in \bbN$,
$\pi_0 \in \Pi$ and a substitution $\sigma_0$ such that
$v\sigma_0 \ast \pi_0 \converge_{i_0}$ and
$w\sigma_0 \ast \pi_0 \diverge_{i_0}$ (up to symmetry). We take $i_0 = i$,
$\pi_0 = [\lambda x\;E\sigma]\pi$ and $\sigma_0 = \sigma$. These
values are suitable since by definition
${v\sigma_0 \ast \pi_0 } \red_{i_0} {E\sigma[x := v\sigma] \ast \pi}
\converge_{i_0}$ and
${w\sigma_0 \ast \pi_0} \red_{i_0} {E\sigma[x := w\sigma] \ast \pi}
\diverge_{i_0}$.
\end{proof}

\begin{lemma}\label{aposs}
Let $s$ be a process, $t$ be a term, $a$ be a term variable and $k$ be a
natural number. If $s[a := t] \converge_k$ then there is a blocked state
$p$ such that $s \succ^{*} p$ and either
\begin{itemize}
\item $p = v \ast \alpha$ for some value $v$ and a stack variable $\alpha$,
\item $p = a \ast \pi$ for some stack $\pi$,
\item $k > 0$ and $p = \delta(v,w) \ast \pi$ for some values $v$ and $w$ and
      stack $\pi$, and in this case $v[a := t] \not\equiv_j w[a := t]$ for
      some $j < k$.
\end{itemize}
\end{lemma}
\begin{proof}
Let $\sigma$ be the substitution $[a := t]$. If $s$ is non-terminating, lemma
\ref{redstable} tells us that $s\sigma$ is also non-terminating, which
contradicts $s\sigma \converge_k$. Consequently, there is a blocked process
$p$ such that $s \succ^{*} p$ since $(\succ) \subseteq (\red_k)$. Using lemma
\ref{redcompatall} we get
$s\sigma \succ^{*} p\sigma$ from which we obtain $p\sigma \converge_{k}$.
The process $p$ cannot be stuck, otherwise $p\sigma$ would also be stuck
by lemma \ref{redstable}, which would contradict $p\sigma \converge_{k}$.
Let us now suppose that $p = \delta_{v,w} \ast \pi$ for some values $v$ and
$w$ and some stack $\pi$. Since
$\delta_{v\sigma,w\sigma} \ast \pi \converge_k$ there must be $i < k$ such
that $v\sigma \not\equiv_j w\sigma$, otherwise this would contradict
$\delta_{v\sigma,w\sigma} \ast \pi \converge_k$. In this case we necessarily
have $k > 0$, otherwise there would be no possible candidate for $i$.
According to lemma \ref{possibilities} we need to rule out four more forms of
therms: $x.l \ast \pi$, $x \ast v.\pi$, $case_x\;B \ast \pi$ and $b \ast \pi$
in the case where $b \not= a$. If $p$ was of one of these forms the
substitution $\sigma$ would not be able to unblock the reduction of $p$, which
would contradict again $p\sigma \converge_{k}$.
\end{proof}

\begin{lemma}\label{aextlem}
Let $t_1$, $t_2$ and $E$ be terms and $a$ be a term variable. For every
$k \in \bbN$, if $t_1 \equiv_k t_2$ then $E[a \!:=\! t_1] \equiv_k E[a
\!:=\! t_2]$.
\end{lemma}
\begin{proof}
Let us take $k \in \bbN$, suppose that $t_1 \equiv_k t_2$ and show that
$E[a \!:=\! t_1] \equiv_k E[a \!:=\! t_1]$. By symmetry we can assume that
we have $i \leq k$, $\pi \in \Pi$ and a substitution $\sigma$ such that
$(E[a \!:=\! t_1])\sigma \ast \pi \converge_i$ and show that
$(E[a \!:=\! t_2])\sigma \ast \pi \converge_i$. As we are free to rename
$a$, we can suppose that it does not appear in $dom(\sigma)$, $TV(\pi)$,
$TV(t_1)$ or $TV(t_2)$. In order to lighten the notations we define
$E' = E\sigma$, $\sigma_1 = [a \!:=\! t_1\sigma]$ and
$\sigma_2 = [a \!:=\! t_2\sigma]$. We are hence assuming
$E'\sigma_1 \ast \pi \converge_i$ and trying to show
$E'\sigma_2 \ast \pi \converge_i$.

We will now build a sequence $(E_i,\pi_i,l_i)_{i \in I}$ in such a way that
$E'\sigma_1 \ast \pi \reds_k E_i\sigma_1 \ast \pi_i\sigma_1$ in $l_i$ steps
for every $i \in I$. Furthermore, we require that $(l_i)_{i \in I}$ is
increasing and that it has a strictly increasing subsequence. Under this
condition our sequence will necessarily be finite. If it was infinite the
number of reduction steps that could be taken from the state
$E'\sigma_1 \ast \pi$ would not be bounded, which would contradict
$E'\sigma_1 \ast \pi \converge_i$. We now denote our finite sequence
$(E_i,\pi_i,l_i)_{i \leq n}$ with $n \in \bbN$. In order to show that
$(l_i)_{i \leq n}$ has a strictly increasing subsequence, we will ensure that
it does not have three equal consecutive values. More formally, we will
require that if $0 < i < n$ and $l_{i-1} = l_i$ then $l_{i+1} > l_i$.

To define $(E_0,\pi_0,l_0)$ we consider the reduction of $E' \ast \pi$. Since
we know that $(E' \ast \pi)\sigma_1 = E'\sigma_1 \ast \pi \converge_i$ we
use lemma \ref{aposs} to obtain a blocked state $p$ such that
${E' \ast \pi} \succ^j p$.
We can now take $E_0 \ast \pi_0 = p$ and $l_0 = j$.
By lemma \ref{redcompatall} we have
$(E' \ast \pi)\sigma_1 \succ^j {E_0\sigma_1 \ast \pi_0\sigma_1}$ from which we
can deduce that
$(E' \ast \pi)\sigma_1 \reds_k {E_0\sigma_1 \ast \pi_0\sigma_1}$ in $l_0 = j$
steps.

To define $(E_{i+1},\pi_{i+1},l_{i+1})$ we consider the reduction of the
process $E_i\sigma_1 \ast \pi_i$. By construction we know that
${E'\sigma_1 \ast \pi}
\reds_k {E_i\sigma_1 \ast \pi_i\sigma_1 = (E_i\sigma_1 \ast \pi_i)\sigma_1}$
in $l_i$ steps. Using lemma \ref{aposs} we know that $E_i \ast \pi_i$ might be
of three shapes.
\begin{itemize}
\item If ${E_i \ast \pi_i} = {v \ast \alpha}$ for some value $v$ and stack
      variable $\alpha$ then the end of the sequence was reached with $n = i$.
\item If $E_i = a$ then we consider the reduction of $E_i\sigma_1 \ast \pi_i$.
      Since $(E_i\sigma_1 \ast \pi_i)\sigma_1 \converge_k$ we know from
      lemma \ref{aposs} that there is a blocked process $p$ such that
      ${E_i\sigma_1 \ast \pi_i} \succ^j p$. Using lemma \ref{redcompatall} we
      obtain that ${E_i\sigma_1 \ast \pi_i\sigma_1} \succ^j p\sigma_1$ from which we
      can deduce that ${E_i\sigma_1 \ast \pi_i\sigma_1} \red_k p\sigma_1$ in
      $j$ steps. We then take $E_{i+1} \ast \pi_{i+1} = p$ and
      $l_{i+1} = l_i + j$.

      Is it possible to have $j=0$? This can only happen when
      $E_i\sigma_1 \ast \pi_i$ is of one of the three forms of lemma
      \ref{aposs}. It cannot be of the form $a \ast \pi$ as we assumed that
      $a$ does not appear in $t_1$ or $\sigma$. If it is of the form $v \ast
      \alpha$, then we reached the end of the sequence with $i + 1 = n$ so
      there is no trouble. The process $E_i\sigma_1 \ast \pi_i$ may be of the
      form $\delta(v,w) \ast \pi$, but we will have $l_{i+2} > l_{i+1}$.
\item If $E_i = \delta(v,w)$ for some values $v$ and $w$ we have
      $m < k$ such that $v\sigma_1 \not\equiv_m w\sigma_1$. Hence
      ${E_i\sigma_1 \ast \pi_i = \delta(v\sigma_1,w\sigma_1) \ast \pi_i}
      \red_k {v\sigma_1 \ast \pi_i}$ by definition. Moreover
      ${E_i\sigma_1 \ast \pi_i\sigma_1} \red_k {v\sigma_1 \ast \pi_i\sigma_1}$
      by lemma \ref{redcompatall}. Since
      ${E'\sigma_1 \ast \pi} \reds_k {E_i\sigma_1 \ast \pi_i\sigma_1}$ in
      $l_i$ steps we obtain that
      ${E'\sigma_1 \ast \pi} \reds_k {v\sigma_1 \ast \pi_i\sigma_1}$ in
      $l_i + 1$ steps. This also gives us ${(v\sigma_1 \ast \pi_i)\sigma_1 = 
      v\sigma_1 \ast \pi_i\sigma_1} \converge_k$.
      
      We now consider the reduction of the process $v\sigma_1 \ast \pi_i$. By
      lemma \ref{aposs} there is a blocked process $p$ such that
      ${v\sigma_1 \ast \pi_i} \succ^j p$. Using lemma \ref{redcompatall} we
      obtain ${v\sigma_1 \ast \pi_i\sigma_1} \succ^j p\sigma_1$ from which we
      deduce that ${v\sigma_1 \ast \pi_i\sigma_1} \reds_k p\sigma_1$ in $j$
      steps. We then take $E_{i+1} \ast \pi_{i+1} = p$ and
      $l_{i+1} = l_i + j + 1$. Note that in this case we have $l_{i+1} > l_i$.
\end{itemize}
Intuitively $(E_i,\pi_i,l_i)_{i \leq n}$ mimics the reduction of
$E'\sigma_1 \ast \pi$ while making explicit every substitution of $a$ and
every reduction of a $\delta$-like state.

To end the proof we show that for every $i \leq n$ we have
${E_i\sigma_2 \ast \pi_i\sigma_2} \converge_k$. For $i = 0$ this will give
us ${E'\sigma_2 \ast \pi} \converge_k$ which is the expected result. Since
$E_n \ast \pi_n = v \ast \alpha$ we have $E_n\sigma_2 \ast \pi_n\sigma_2 =
v\sigma_2 \ast \alpha$ from which we trivially obtain
${E_n\sigma_2 \ast \pi_n\sigma_2} \converge_k$.
We now suppose that ${E_{i+1}\sigma_2 \ast \pi_i\sigma_2} \converge_k$ for
$0 \leq i < n$ and show that ${E_i\sigma_2 \ast \pi_i\sigma_2} \converge_k$.
By construction $E_i \ast \pi_i$ can be of two shapes\footnote{Only
$E_n \ast \pi_n$ can be of the form $v \ast \alpha$.}:
\begin{itemize}
\item If $E_i=a$ then ${t_1\sigma \ast \pi_i} \reds_k {E_{i+1} \ast \pi_{i+1}}$.
  Using lemma \ref{redcompatall} we obtain
  $t_1\sigma \ast \pi_i\sigma_2 \red_k E_{i+1}\sigma_2 \ast
  \pi_i\sigma_2$ from which we deduce $t_1\sigma \ast \pi_i\sigma_2
  \converge_k$ by induction hypothesis. Since $t_1 \equiv_k t_2$ we obtain
  ${t_2\sigma \ast \pi_i\sigma_2 = (E_i \ast \pi_i)\sigma_2} \converge_k$.
\item If $E_i = \delta(v,w)$ then ${v \ast \pi_i} \red_k {E_{i+1} \ast
  \pi_{i+1}}$ and hence $v\sigma_2 \ast \pi_i\sigma_2 \red_k
  E_{i+1}\sigma_2 \ast \pi_{i+1}\sigma_2$ by lemma \ref{redcompatall}. Using the
  induction hypothesis we obtain ${v\sigma_2 \ast \pi_i\sigma_2} \converge_k$.
  It remains to show that ${\delta(v\sigma_2,w\sigma_2) \ast \pi_i\sigma_2}
  \reds_k {v\sigma_2 \ast \pi_i\sigma_2}$.
  We need to find $j < k$ such that $v\sigma_2 \not\equiv_j w\sigma_2$.
  By construction there is $m < k$ such that
  $v\sigma_1 \not\equiv_m w\sigma_1$. We are going to show that
  $v\sigma_2 \not\equiv_m w\sigma_2$. By using the global induction
  hypothesis twice we obtain $v\sigma_1 \equiv_m v\sigma_2$ and
  $w\sigma_1 \equiv_m v\sigma_2$. Now if $v\sigma_2 \equiv_m w\sigma_2$ then
  $v\sigma_1 \equiv_m v\sigma_2 \equiv_m w\sigma_2 \equiv_m w\sigma_1$
  contradicts $v\sigma_1 \not\equiv w\sigma_1$. Hence we must have
  $v\sigma_2 \not\equiv_m w\sigma_2$.
\end{itemize}
\end{proof}

\begin{theorem}\label{extterm}
Let $t_1$, $t_2$ and $E$ be three terms and $a$ be a term variable. If
$t_1 \equiv t_2$ then $E[a \!:=\! t_1] \equiv E[a \!:=\! t_2]$.
\end{theorem}
\begin{proof}
We suppose that $t_1 \equiv t_2$ which means that $t_1 \equiv_i t_2$ for
every $i \in \bbN$. We need to show that
$E[a \!:=\! t_1] \equiv E[a \!:=\! t_2]$ so we take $i_0 \in \bbN$ and show
$E[a \!:=\! t_1] \equiv_{i_0} E[a \!:=\! t_2]$. By hypothesis we have
$t_1 \equiv_{i_0} t_2$ and hence we can conclude using lemma \ref{aextlem}.
\end{proof}
*)
