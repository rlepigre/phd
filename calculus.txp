\Caml(open Diagrams open ProofTree)
\Include{Macros}

=> Untyped calculus and abstract machine \label("calculus")

In this chapter, we introduce the programming language that will be
considered throughout this thesis. Its operational semantics is expressed
in terms of an abstract machine, which will allow us to account for
computational effects.

=> The pure $λ$-calculus

In this thesis, we consider a programming language of the ML family, similar
to OCaml or SML. Like every functional language, its syntax is based on the
$λ$-calculus. Introduced by Alonzo Church in the Thirties, the $λ$-calculus
\cite("Church1941") is a formalism for representing computable functions, and
in particular reccursive functions. As shown by Alan Turing, the $λ$-calculus
is a //universal model of computation// \cite("Turing1937").

(* The language of λ-terms. *)
\begin{def}\label("deflambda")
The terms of the $λ$-calculus (or $λ$-terms) are built from a countable
alphabet of variables (or $λ$-variables) denoted $\cal{V}_λ=\{x,y,z...\}$.
The set of all the $λ$-terms is denoted $Λ$ and is defined as the language
recognised by the following ||bnf|| grammar.
\Caml(
let _ = sidenote << $t,u ::= {x} \| {λx t} \| {t u}$ >> << $x ∈ \cal{V}_λ$ >>
)
A term of the form $λx t$ is called an abstraction (or $λ$-abstractions)
and a term of the form $t u$ is called an application.
\end{def}
Intuitively, a $λ$-abstraction $λx t$ forms a fonction by binding the
variable $x$ in the term $t$. This would be denoted $x ↦ t$ in common
mathematics. Similarly, a term of the form $t u$ denotes the application
of (the function) $t$ to (the argument) $u$. This would be denoted $t(u)$
in common mathematics. 
\begin{rem}
As $λ$-terms have a tree-like structure, parenthesis are sometimes required
for disambiguation. For example, the term $λx t u$ can be read both as
$(λx t) u$ and as $λx (t u)$. To lighten the notations we will consider
application to be left-associative and abstraction to bind stronger that
application. As a consequence, we will always read the term $λx t x u$ as
$λx ((t x) u)$.
\end{rem}
\begin{rem}
The syntax of the $λ$-calculus only allows for one-place functions. To form
a function of two arguments (or more) one must rely on Curryfication.
Indeed, a function of two arguments can be seen as a function of one
argument returning a function. Following this scheme, the multiple arguments
of the function are given in turn, and not simultaneously. As an example,
the function $(x,y) ↦ x$ can be encoded as $λx λy x$.
\end{rem}

(* Free variables and bound variables. *)
Although this is not reflected explicitely in the syntax of $λ$-terms,
a $λ$-variable may play two very different roles. It can be used either as
a constant, like $y$ in the constant function $λx y$, or as a reference to
a binder, like $x$ in the identity function $λx x$. Variable binding and
the associated notions of free and bound variable are hence essential.
\begin{def}\label("freelvars")
Given a term $t$, we denote by $FV_λ(t)$ the set of its free $λ$-variables
and $BV_λ(t)$ the set of its bound $λ$-variables. These sets are defined
inductively on the structure of the term $t$.
\begin{center}
\linesBefore(4) (* FIXME hack *)
\diagram(
let contents = two_cols
 [ [<$ FV_λ(x) $>   ; <$ = $>; <$ \{x\} $>                   ]
 ; [<$ FV_λ(λx t) $>; <$ = $>; <$ FV_λ(t) \setminus \{x\} $> ]
 ; [<$ FV_λ(t u) $> ; <$ = $>; <$ FV_λ(t) ∪ FV_λ(u) $>       ]
 ; [<$ BV_λ(x) $>   ; <$ = $>; <$ ∅ $>                       ]
 ; [<$ BV_λ(λx t) $>; <$ = $>; <$ BV_λ(t) ∪ \{x\} $>         ]
 ; [<$ BV_λ(t u) $> ; <$ = $>; <$ BV_λ(t) ∪ BV_λ(u) $>       ] ]
let _ = array [`East; `Main; `West; `East; `Main; `West] contents
          ~horizontal_padding:(fun n -> if n = 3 then 10.0 else 1.0)
)
\end{center}
\end{def}
\begin{rem}
Nothing prevents a $λ$-variable to have both free and bound occurrences in
a term. For example, in $t = λx y λy x y$ the first occurrence of $y$ is
free while its second occurrence is bound. We have $y ∈ FV_λ(t) = \{y\}$
and $y ∈ BV_λ(t) = \{x, y\}$.
\end{rem}

(* Substitution, α-equivalence and β-reduction. *)
When a $λ$-abstraction (i.e. a function) is applied to an argument, we
obtain a term of the form $(λx t) u$, called a $β$-redex. The reduction
of such $β$-redexes plays an essential role in computation. Intuitively,
the reduction of the $β$-redex $(λx t) u$ will be performed by replacing
every occurence of the bound variable $x$ by $u$ in the term $t$. This
operation, called substitution, is formally defined as follows.
\begin{def}
Let $t ∈ Λ$ and $u ∈ Λ$ be two $λ$-terms, and $x ∈ \cal{V}_λ$ be a
$λ$-variable. We denote $t[x ← u]$ the term $t$ in which every free
occurrence of $x$ has been replaced by $u$. This operation is defined
inductively on the structure of $t$.
\begin{center}
\linesBefore(5) (* FIXME hack *)
\diagram(
let contents = two_cols
 [ [<$ x[x ← u] $>      ; <$ = $>; <$ u $>                   ]
 ; [<$ y[x ← u] $>      ; <$ = $>; <$ y $>                   ]
 ; [<$ (λx t)[x ← u] $> ; <$ = $>; <$ λx t $>                ]
 ; [<$ (λy t)[x ← u] $> ; <$ = $>; <$ λy t[x ← u] $>         ]
 ; [<$ (t₁ t₂)[x ← u] $>; <$ = $>; <$ t₁[x ← u] t₂[x ← u] $> ]
 ]
let _ =
  array [`East; `Main; `West; `East; `Main; `West] contents
    ~horizontal_padding:(fun n -> if n = 3 then 10.0 else 1.0)
)
\end{center}
\end{def}
Substitution is a subtle notion, and care should be taken to avoir capture
of variables. For example, let us consider the function $λx λy x$ which
takes an argument $x$ and returns a constant function with value $x$. If we
apply this function to $y$, the expected result is a constant function with
value $y$. However, if we blindly substitute $x$ with $y$ in $λy x$ we obtain
the identity function $λy y$. Indeed, the free variable $y$ has been captured
and now references a binder that had (coincidentally) the same name.

To solve this problem, we need to make sure that whenever a substitution
$t[x ← u]$ is performed, no free variable of $u$ is bound in $t$ (i.e.
$FV_λ(u) ∩ BV_λ(t) = ∅$). Although we cannot rename the free variables of
$u$, it is possible to rename the bound variables of $t$. Indeed, changing
the name of a bound variable has no effect on computational behaviour of a
term. Two terms that are equivalent up to the names of their bound variables
are said to be $α$-equivalent.
\begin{def}
The $α$-equivalence relation $({≡}_α) ⊆ Λ×Λ$ is defined, like in
\cite("Krivine1990"), as the smallest relation such that:
\begin{itemize}
\item if $x ∈ \cal{V}_λ$ then $x ≡_α x$,
\item if $t₁ ≡_α t₂$ and $u₁ ≡_α u₂$ then ${t₁ u₁} ≡_α {t₂ u₂}$,
\item if ${t₁[x₁ ← y]} \nequiv_α {t₂[x₂ ← y]}$ for only finitely many
      $y ∈ \cal{V}_λ$ then ${λx₁ t₁} ≡_α {λx₂ t₂}$.
\end{itemize}
\end{def}
\begin{lem}\label("alphalem")
Given a term $t ∈ Λ$ and a finite set of variables $V ⊆ \cal{V}_λ$, it is
always possible to find a term $t' ∈ Λ$ such that $t' ≡_α t$ and
$BV_λ(t') ∩ V = ∅$.
\begin{proof}
A full proof is available in \cite("Krivine1990"), Lemma 1.11.
\end{proof}
\end{lem}
\begin{def}
Let $t ∈ Λ$ and $u ∈ Λ$ be two $λ$-terms, and $x ∈ \cal{V}_λ$ be a
$λ$-variable. We denote $t[x := u]$ the capture-avoiding substitution of
$x$ by $u$ in $t$. It is defined as $t'[x ← u]$ where $t' ∈ Λ$ is
a term such that $t' ≡_α t$ and $BV_λ(t') ∩ FV(u) = ∅$. Such a term exists
according to \lemRef("alphalem").
\end{def}

=<

=> Evaluation contexts and reduction

In order to define the most general notion of reduction over $λ$-terms, we
need to be able to refer to any $β$-redex in a term. To this aim, we
introduce the notion of evaluation context. Intuitively, a context will
consist in a term with a hole (i.e. a place-holder for a subterm) and it
will allow us to focus on any particular subterm of a term.
\begin{def}
The set of evaluation contexts $[Λ]$ is defined as the language recognised
by the following ||bnf|| grammar.
$$ E ::= [{\wc}] \| {λx E} \| {E t} \| {t E} \hspace(4.0) x∈\cal{V}_λ, t∈Λ$$
\end{def}
\begin{def}
Given a term $u ∈ Λ$ and an evaluation context $E ∈ [Λ]$, we $E[u]$ the term
formed by puting $u$ into the hole of the evaluation context $E$. It is
defined by induction on the structure of $E$ as follows.
\begin{center}
\diagram(
let contents = two_cols
 [ [<$ [{\wc}][u] $>; <$ = $>; <$ u       $> ]
 ; [<$ (λx E)[u]  $>; <$ = $>; <$ λx E[u] $> ]
 ; [<$ (E t)[u]   $>; <$ = $>; <$ E[u] t  $> ]
 ; [<$ (t E)[u]   $>; <$ = $>; <$ t E[u]  $> ]
 ]
let _ =
  array [`East; `Main; `West; `East; `Main; `West] contents
    ~horizontal_padding:(fun n -> if n = 3 then 10.0 else 1.0)
)
\end{center}
\end{def}
\begin{def}\label("generalbeta")
The general $β$-reduction relation is defined as the smallest relation
$({→}_β)$ such that for every evaluation context $E ∈ [Λ]$, for every terms
$t ∈ Λ$ and $u ∈ Λ$, and for every variable $x ∈ \cal{V}_λ$ we have the
following.
$$ {E[(λx t) u]} →_β {E[t[x := u]]}$$
We say that the term $t ∈ Λ$ is in $β$-normal-form if there is no $u ∈ Λ$
such that $t →_β u$. We denote $({→}_β^{∗})$ the reflexive, transitive
closure of $({→}_β)$.
\end{def}

The general $β$-reduction relation $({→}_β)$ is non-deterministic. Indeed,
given a term $t$, there might be two (different) terms $u₁$ and $u₂$ such
that $t →_β u₁$ and $t →_β u₂$. For example, the term
$((λx₁ x₁) λx₂ x₂) ((λx₃ x₃) λx₄ x₄)$ can either reduce to
$(λx₂ x₂) ((λx₃ x₃) λx₄ x₄)$ or to $((λx₁ x₁) λx₂ x₂) (λx₄ x₄)$. Indeed,
we can focus on the $β$-redex $(λx₁ x₁) λx₂ x₂$ using the evaluation context
$[{\wc}] ((λx₃ x₃) λx₄ x₄)$, or on the $β$-redex $(λx₃ x₃) λx₄ x₄$ using the
evaluation context $((λx₁ x₁) λx₂ x₂) [{\wc}]$.

Although it is non-deterministic, the general $β$-reduction relation
$({→}_β)$ has the Church-Rosser property \cite("Church1936").
\begin{thm}
Let $t ∈ Λ$ be a term. If there are $u₁ ∈ Λ$ and $u₂ ∈ Λ$ such that
$t →_β u₁$ and $t →_β u₂$, then there must be $u ∈ Λ$ such that
${u₁ →_β u}^{∗}$ and ${u₂ →_β u}^{∗}$.
\begin{proof}
A full proof is available in \cite("Church1936").
\end{proof}
\end{thm}
Intuitivly, the Church-Rosser property enforces a weak form of determinism.
Indeed, it implies that a program can only compute one particular result,
even if it can be attained in several different ways.

In this thesis, we consider a programming language that is effectful, hence
it does not have the Church-Rosser property. As a consequence, we need to
(* restrict the notion of reduction... *)
choose an evaluation order for the language to remain deterministic enough
(in our case completely deterministic). The two evaluation orders that are
the most widely used in practice are call-by-name and call-by-value. They
both reduce outermost $β$-redexes first, which means that they can be
formalised without the use of evaluation contexts. In call-by-name, the terms
that are in function position are reduced first. The computation of the terms
that are in argument position is postponed to the time of their effective
use. In call-by-value, the arguments are evaluated first, before performing
the application.
\begin{def}
The call-by-name reduction relation $({→}_N) ⊆ Λ×Λ$ is the smallest relation
satisfying the following two rules.
$$
  \axiomR{{(λx t) u} →_N {t[x := u]}}
  \hspace(6.0)
  \unaryR{t →_N t'}{{t u} →_N {t' u}}
$$
\end{def}
\begin{rem}
In this thesis, we will often define relations using deduction rule
systems. A deduction rule is formed using premisses $\{P_i\}_{1≤i≤n}$ and
a conclusion $C$ separated by an horizontal bar.
$$ \ternaryR{P₁}{...}{P_n}{C} $$
The meaning of such a rules is that the conclusion $C$ can be deduced when
all the premisses $P_i$ are true. Note that there might be no premisse, and
in this case the conclusion can be deduced immediatly.
\end{rem}
In call-by-value, both the function and its argument need to be fully
evaluated before the application can be performed. Consequently, two
different strategies can be defined: left-to-right and right-to-left
call-by-value evaluation. The evaluation order that is the most widely
used in practice is left-to-right call-by-value. However, some practical
languages like OCaml use right-to-left evaluation.
\begin{def}
A term $t$ is said to be a value if it is either a $λ$-variable or a
$λ$-abstraction. We will use the letters $v$ and $w$ to denote values.
\end{def}
\begin{def}
The left-to-right call-by-value reduction relation $({→}_S) ⊆ Λ×Λ$ is
the smallest relation satisfying the following rules.
$$
  \unaryR{v value}{{(λx t) v} →_S {t[x := v]}}
  \hspace(4.0)
  \unaryR{t →_S t'}{{t u} →_S {t' u}}
  \hspace(4.0)
  \unaryR{u →_S u'}{{(λx t) u} →_S {(λx t) u'}}
$$
\end{def}
\begin{def}
The right-to-left call-by-value reduction relation $({→}_V) ⊆ Λ×Λ$ is the
smallest relation satisfying the following rules.
$$
  \unaryR{v value}{{(λx t) v} →_V {t[x := v]}}
  \hspace(4.0)
  \unaryR{u →_V u'}{{t u} →_V {t u'}}
  \hspace(4.0)
  \binaryR{t →_V t'}{v value}{{t v} →_V {t' v}}
$$
\end{def}

In this thesis, $λ$-terms and programs in general will be evaluated in an
abstract machine called a Krivine machine \cite("Krivine2007"). This
machine will emulate the right-to-left evaluation relation $({→}_V)$.

=<

=> Call-by-value Krivine machine

In the previous section, we explicited the syntax of the $λ$-calculus and
the evaluation of $λ$-terms. We will now reformulate these definition in
terms of a call-by-value Krivine abstract machine \cite("Krivine2007").
Our presentation will differ from the original machine, which is
call-by-name. Although call-by-value Krivine machines have rarely been
published, they are well-known among classical realizability experts.

\begin{def}
Values, terms, stacks and processes are generated by the following four
||bnf|| grammars. The names of the corresponding sets are displayed on
the right.
\begin{center}
\linesBefore(6)
\diagram(
let _ = array [`East ; `East ; `West; `West]
  ~horizontal_padding:(function 3 -> 20.0 | _ -> 1.0)
  [ [ <$v,w$>; <$::=$>; <$ x \| {λx t} $>          ; <$(Λ_{val})$> ]
  ; [ <$t,u$>; <$::=$>; <$ v \| {t u} $>           ; <$(Λ)$>       ]
  ; [ <$π,ρ$>; <$::=$>; <$ ε \| {v ⋅ π} \| [t] π $>; <$(Π)$>       ]
  ; [ <$p,q$>; <$::=$>; <$ t ∗ π $>                ; <$(Λ × Π)$>   ] ]
)
\end{center}
\end{def}
The syntactic distinction between terms and values is specific to the
call-by-value presentation, they would be collapsed in call-by-name.
Intuitively, a stack can be thought of as an evaluation context represented
as a list of terms and values. The values are to be considered as arguments
to be fed to the term in the context, and the terms are to be considered as
functions to which the term in the context will be applied. The symbol $ε$
is used to denote an empty stack. A process $t ∗ π$ is to be considered as
the state of our abstract machine. Its reduction will consist in the
interaction between the term $t$ and its evaluation context encoded into
a stack $π$.

As we are considering a call-by-value calculus, only values are (and should
be) substituted to $λ$-variables during evaluation. From now on, we will
hence work with the following definition of substitution. In particular,
a substitution of the form $t[x := u]$ will be forbidden if $u$ is not a
syntactic value.
\begin{def}
Let $t ∈ Λ$ be a term, $x ∈ \cal{V}_λ$ be a $λ$-variable and $v ∈ Λ_{val}$
be a value. We denote $t[x := v]$ the capture-avoiding substitution of $x$
by $v$ in $t$.
\end{def}
\begin{def}
The reduction relation $({≻}) ⊆ (Λ×Π) × (Λ×Π)$ is defined as the smallest
relation satisfying the following rules. We will denote $({≻}^{∗})$ its
reflexive and transitive closure.
\begin{center}
\linesBefore(4)
\diagram(
let _ = array [`East ; `Main ; `West]
  ~horizontal_padding:(function _ -> 5.0)
  [ [ <${t u} ∗ π$>     ; <$≻$>; <$u ∗ {[t]π}$>       ]
  ; [ <$v ∗ {[t] π}$>   ; <$≻$>; <$t ∗ {v·π}$>        ]
  ; [ <$λx t ∗ {v · π}$>; <$≻$>; <${t[x := v]} ∗ π$>  ] ]
)
\end{center}
\end{def}
Three reduction rules are used to handle call-by-value evaluation. When an
application is encountered, the function is stored in a stack-frame in order
to evaluate its argument first. Once the argument has been completely
computed, a value faces the stack-frame containing the function. At this
point the function can be evaluated and the value is stored in the stack
ready to be consumed by the function as soon as it evaluates to a
$λ$-abstraction. A capture-avoiding substitution can then be performed to
effectively apply the argument to the function. For example, the process
${(λx x y) λz z} ∗ ε$ reduces in the following way.
$$
  {{(λx x y) λz z} ∗ ε}
  ≻
  {{λz z} ∗ {[λx x y]ε}}
  ≻
  {{λx x y} ∗ {{λz z} · ε}}
  ≻
  {{(λz z) y} ∗ ε}
  ≻
  {y ∗ ε}
$$
\begin{rem}
The choice of right-to-left call-by-value allows for a simpler abstract
machine. Indeed, an additional stack constructor and reduction rule would
be required to implement the more usual left-to-right evaluation.
\end{rem}

=<

=> Computational effects and $λμ$-calculus

We are now going to extend the calculus and our abstract machine with
operations allowing the manipulation of the stack. More precisely, we
will provide a way to save the stack (i.e. the evaluation context or the
continuation), so that it can be restored at a later stage. A natural
way to extend our language is to use the syntax of Michel Parigot's
$λμ$-calculus \cite("Parigot1992"). We hence introduce a new binder $μα t$
capturing the current stack in the $μ$-variable $α$. It can then be
restored in $t$ using the syntax $[α]u$.
\begin{def}
Let $\cal{V}_μ = \{α, β, γ...\}$ be a countable set of $μ$-variables (or
stack variables) disjoint from $\cal{V}_λ$. Value, terms, stacks and
processes are now generated by the following grammars. The names of the
corresponding sets are displayed on the right.
\begin{center}
\linesBefore(6)
\diagram(
let _ = array [`East ; `East ; `West; `West]
  ~horizontal_padding:(function 3 -> 20.0 | _ -> 1.0)
  [ [<$v,w$>; <$::=$>; <$ x \| {λx t} $>                   ; <$(Λ_{val})$>]
  ; [<$t,u$>; <$::=$>; <$ v \| {(t) u} \| {μα t} \| [π]t $>; <$(Λ)$>      ]
  ; [<$π,ρ$>; <$::=$>; <$ ε \| α \| {v ⋅ π} \| [t] π $>    ; <$(Π)$>      ]
  ; [<$p,s$>; <$::=$>; <$ t ∗ π $>                         ; <$(Λ × Π)$>  ] ]
)
\end{center}
\end{def}
Note that terms of the form $[π]t$ will only be available to the user if
$π$ is a stack variable. Allowing arbitrary stacks allow us to substitute
$μ$-variables by stacks during computation. Like with $λ$-variable, we will
need to be careful and avoid variable capture. However, we will not give
the full details this time.
\begin{def}
Given a value, term, stack or process $ψ$, we denote $FV_λ(ψ)$ (resp.
$BV_λ(ψ)$) the set of its free (resp. bound) $λ$-variables and $FV_μ(ψ)$
(resp. $BV_μ(ψ)$) the set of its free (resp. bound) $μ$-variables. These
sets are defined in a similar way to \defRef("freelvars").
\end{def}
\begin{def}
Let $t ∈ Λ$ be a term, $π ∈ Π$ be a stack and $α ∈ \cal{V}_μ$ be a
$μ$-variable. We denote $t[α := π]$ the (capture-avoiding) substitution of
$α$ by $π$ in $t$.
\end{def}
\begin{def}
The reduction relation $({≻})$ is extended with two new reduction rules.
\begin{center}
\linesBefore(7)
\diagram(
let _ = array [`East ; `Main ; `West]
  ~horizontal_padding:(function _ -> 5.0)
  [ [ <${(t) u} ∗ π$>   ; <$≻$>; <$u ∗ {[t]π}$>       ]
  ; [ <$v ∗ {[t] π}$>   ; <$≻$>; <$t ∗ {v·π}$>        ]
  ; [ <$λx t ∗ {v · π}$>; <$≻$>; <${t[x := v]} ∗ π$>  ]
  ; [ <$μα t ∗ π$>      ; <$≻$>; <${t[α := π]} ∗ π$>  ]
  ; [ <$[ρ]t ∗ π$>      ; <$≻$>; <$t ∗ ρ$>            ] ]
)
\end{center}
\end{def}
Now, when the abstract machine encounters a $μ$-abstraction $μα t$, the
current stack $π$ is substituted to the $μ$-variables $α$. As a consequence,
every subterm of the form $[α]u$ in $t$ becomes $[π]u$. When the machine
then reaches a state of the form $[π]u ∗ ρ$, the current stack $ρ$ is erased,
and computation resumes with the stored stack $π$. For example, the processus
$λx μα Ω_x [α]x ∗ v · ε$ where $Ω_x$ is an arbitrary term and $v$ is an
arbitrary value reduces as follows.
$$
  {{λx μα Ω_x [α]x} ∗ {v · ε}}
  ≻
  {{μα Ω_v [α]v} ∗ ε}
  ≻
  {{Ω_v [ε]v} ∗ ε}
  ≻
  {{[ε]v} ∗ {[Ω_v] ε}}
  ≻
  {v ∗ ε}
$$
Note that when a stack is erased, arbitrary terms might be erased. In
particular, we could have chosed $Ω_x = Ω = (λx x x) λx x x$ in the
previous example, although the reduction of this term does not terminate.
Indeed, we have
$$
  {Ω ∗ π}
  ≻
  {{λx x x} ∗ {[λx x x] π}}
  ≻
  {{λx x x} ∗ {{λx x x} · π}}
  ≻
  {Ω ∗ π}
$$
for every possible stack $π$.

=<

=> Records, variants and recursion

... (* TODO *)

=<

=> Full syntax and operational semantics

The programming language that is considered in this thesis is expressed in
terms of a //Krivine Abstract Machine// \cite("Krivine2007"). It is formed
using four syntactic entities: values, terms, stacks and processes. The
distinction between terms and values is specific to our call-by-value
presentation, they would be collapsed in call-by-name.
\begin{def}
We require three distinct, countable sets of variables:
$\cal{V}_λ = \{x, y, z...\}$ for $λ$-variables,
$\cal{V}_μ = \{α, β, γ...\}$ for $μ$-variables and
$\cal{V}_ι = \{a, b, c...\}$ for term variables.
We also require a contable set $\cal{L} = \{l, l₁, l₂...\}$ of labels to
name record fields and a countable set $\cal{C} = \{C, C₁, C₂...\}$ for
naming constructors.
\end{def}
\begin{def}
Value, terms, stacks and processes are mutually inductively defined as the
languages recognised by the following grammars. The names of the
corresponding sets are displayed on the right.
\begin{center}
\linesBefore(6)
\diagram(
let _ = array [`East ; `East ; `West; `West]
  ~horizontal_padding:(function 3 -> 10.0 | _ -> 1.0)
  [ [ <$v,w$> ; <$::=$>
    ; <$ x \| {λx t} \| \{(l_i = v_i)_{i∈I}\} \| {C[v]} $>
    ; <$(Λ_{val})$> ]
  ; [ <$t,u$> ; <$::=$>
    ; <$a \| v \| {t u} \| {μα t} \| {[π]t} \| {v{.}l} \|
       {[v \| (C_i → t_i)_{i∈I}]} \| {Y_{t,v}} \| {{unit}_v} $>
    ; <$(Λ)$> ]
  ; [ <$π,ρ$> ; <$::=$>
    ; <$ ε \| α \| {v⋅π} \| {[t]π} $>
    ; <$(Π)$> ]
  ; [ <$p,s$> ; <$::=$>
    ; <$ t ∗ π $>
    ; <$(Λ × Π)$> ] ]
)
\end{center}
\end{def}
Terms and values form a variation of the $λμ$-calculus \cite("Parigot1992"),
enriched with ML-like constructs (i.e. records and variants). One may note
that values are terms. A stack can be either the empty stack $ε$, a stack
variable, a value pushed on top of a stack, or a stack frame containing a
term on top of a stack. These two constructors are specific to the
call-by-value presentation, only one would be required in call-by-name.
\begin{rem}
We enforce values in constructors, record fields, projection and case
analysis. This makes the calculus simpler because only $β$-reduction will
manipulate the stack. We can define syntactic sugars such as the following
to hide the restriction from the programmer.
$$ t.l := (λx x.l) t \hspace(2.0) C[t] := (λx C[x]) t $$
\end{rem}
\begin{def}
Given a value, term, stack or process $ψ$ we denote $FV_λ(ψ)$ (resp.
$FV_μ(ψ)$, $TV(ψ)$) the set of free $λ$-variables (resp. free $μ$-variables,
term variables) contained in $ψ$. We say that $ψ$ is closed if it does not
contain any free variable of any kind. The set of closed values and the set
of closed terms are denoted $Λ_{val}^{∗}$ and $Λ^{∗}$ respectively.
\end{def}

Processes form the internal state of our abstract machine. They are to be
thought of as a term put in some evaluation context represented using a
stack. Intuitively, the stack $π$ in the process $t ∗ π$ contains the
arguments to be fed to $t$. Since we are in call-by-value the stack also
handles the storing of functions while their arguments are being evaluated.
This is why we need stack frames (i.e. stacks of the form $[t]π$). The
operational semantics of our language is given by a relation $({≻})$ over
processes.
\begin{def}
The relation $({≻}) ⊆ (Λ×Π)²$ is defined as the smallest relation
satisfying the following reduction rules.
\begin{center}
\diagram(
let _ = array [`East ; `Main ; `West]
  ~horizontal_padding:(function _ -> 5.0)
  [ [ <${t u} ∗ π$>
    ; <$≻$>; <$u ∗ {[t]π}$> ]
  ; [ <$v ∗ {[t] π}$>
    ; <$≻$>; <$t ∗ {v·π}$> ]
  ; [ <$λx t ∗ {v · π}$>
    ; <$≻$>; <${t[x := v]} ∗ π$> ]
  ; [ <$μα t ∗ π$>
    ; <$≻$>; <${t[α := π]} ∗ π$> ]
  ; [ <$[ρ]t ∗ π$>
    ; <$≻$>; <$t ∗ ρ$> ]
  ; [ <$\{(l_i = v_i)_{i∈I}\}.l_k ∗ π$>
    ; <$≻$>; <$v_k ∗ π \hspace(2.6) when \hspace(0.4) k∈I$> ]
  ; [ <$[C_k[v] \| (C_i → t_i)_{i∈I}] ∗ π$>
    ; <$≻$>; <$t_k v ∗ π \hspace(2.) when \hspace(0.4) k∈I$> ]
  ; [ <$Y_{t,v} ∗ π$>
    ; <$≻$>; <$t ∗ {{λx Y_{t,x}}·v·π}$> ]
  ; [ <${unit}_{\{\id([])\}} ∗ π$>
    ; <$≻$>; <$\{\id([])\} ∗ π$> ]
  ]
)
\end{center}
We will denote $({≻}^{+})$ its transitive closure, $({≻}^{∗})$ its
reflexive-transitive closure and $({≻}^k)$ its $k$-fold application.
\end{def}
The first three rules are those that handle $β$-reduction. When the abstract
machine encounters an application, the function is stored in a stack-frame in
order to evaluate its argument first. Once the argument has been completely
computed, a value faces the stack-frame containing the function. At this
point the function can be evaluated and the value is stored in the stack
ready to be consumed by the function as soon as it evaluates to a
$λ$-abstraction. A capture-avoiding substitution can then be performed to
effectively apply the argument to the function. The fourth and fifth rules
rules handle the classical part of computation. When a $μ$-abstraction is
reached, the current stack (i.e. the current evaluation context) is captured
and substituted for the corresponding $μ$-variable. Conversely, when a
process is reached, the current stack is thrown away and evaluation resumes
with the process. The last two rules perform projection and case analysis in
the expected way.

\begin{lem}\label{redcompatall}
The reduction relation $({≻})$ is compatible with substitutions of variables
of any kind. More formally, if $p$ and $q$ are processes such that $p ≻ q$
then:
\begin{itemize}
\item for all $x ∈ \cal{V}_λ$ and $v ∈ Λ_{val}$, $p[x := v] ≻ q[x := v]$,
\item for all $α ∈ \cal{V}_μ$ and $π ∈ Π$, $p[α := π] ≻ q[α := π]$,
\item for all $a ∈ \cal{V}_ι$ and $t ∈ Λ$, $p[a := t] ≻ q[a := t]$.
\end{itemize}
Consequently, if $σ$ is a substitution for variables of any kind and if
$p ≻ q$ (resp. $p ≻^{∗} q$, $p ≻^{+} q$, $p ≻^k q$) then $pσ ≻ qσ$ (resp.
$pσ ≻^{∗} qσ$, $pσ ≻^{+} qσ$, $pσ ≻^k qσ$).
\begin{proof}
Immediate case analysis on the reduction rules.
\end{proof}
\end{lem}

We are now going to give the vocabulary that will be used to describe some
specific classes of processes. In particular we need to identify processes
that are to be considered as the evidence of a successful computation, and
those that are to be recognised as expressing failure.
\begin{def}
A process $p ∈ Λ×Π$ is said to be:
\begin{itemize}
\item //final// if there is a value $v ∈ Λ_{val}$ such that $p = v ∗ α$,
\item //blocked// if there is no $q ∈ Λ×Π$ such that $p ≻ q$,
\item //stuck// if it is not final and if for every substitution $σ$, $pσ$
      is blocked,
\item //non-terminating// if there is no blocked process $q ∈ Λ×Π$ such that
      $p ≻^{∗} q$.
\end{itemize}
\end{def}

\begin{lem}\label("redstable")
Let $p$ be a process and $σ$ be a substitution for variables of any kind.
If $p$ is stuck (resp. non-terminating) then $pσ$ is also stuck (resp.
non-terminating).
\begin{proof}
Immediate by definition.
\end{proof}
\end{lem}

\begin{lem}\label("remark")
A stuck state is of one of the following forms, where $k ∉ I$.
$$
{C[v].l ∗ π}
\hspace(2.0)
{(λx t).l ∗ π}
\hspace(2.0)
{C[v] ∗ w · π}
\hspace(2.0)
{\{(l_i = v_i)_{i∈I}\} ∗ v · π}
$$

$$
{[λx t | (C_i → t_i)_{i∈I}] ∗ π}
\hspace(4.0)
{[\{(l_i = v_i)_{i∈I}\} | (C_j → t_j)_{i∈J}] ∗ π}
$$

$$
[C_k[v] | (C_j → t_j)_{i∈J}] ∗ π
\hspace(4.0)
\{(l_i = v_i)_{i∈I}\}.l_k ∗ π
$$
\begin{proof}
  Simple case analysis.
\end{proof}
\end{lem}

\begin{lem}\label("possibilities")
A blocked process $p ∈ Λ×Π$ is either stuck, final or of one of the following
forms.
$$
{x.l ∗ π}
\hspace(2.0)
{x ∗ v · π}
\hspace(2.0)
{[x | (C_i → t_i)_{i ∈ I}] ∗ π}
\hspace(2.0)
{a ∗ π}
$$
\begin{proof}
Straight-forward case analysis using lemma \lemRef("remark").
\end{proof}
\end{lem}

=<

=<
