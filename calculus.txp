\Caml(
open Diagrams
open ProofTree
open LCKAM.Lang
open Lang
)
\Include{Macros}

=> Untyped calculus and abstract machine \label("calculus")

In this chapter, we introduce the programming language that will be
considered throughout this thesis. Its operational semantics is expressed
in terms of an abstract machine, which will allow us to account for
computational effects.

=> The pure $λ$-calculus

In this thesis, we consider a programming language of the ML family, similar
to OCaml or SML. Like every functional language, its syntax is based on the
$λ$-calculus. Introduced by Alonzo Church in the Thirties, the $λ$-calculus
\cite("Church1941") is a formalism for representing computable functions, and
in particular recursive functions. As shown by Alan Turing, the $λ$-calculus
is a //universal model of computation// \cite("Turing1937").

(* The language of λ-terms. *)
\begin{def}\label("deflambda")
The terms of the $λ$-calculus (or $λ$-terms) are built from a countable
alphabet of variables (or $λ$-variables) denoted $\cal{V}_λ=\{x,y,z...\}$.
The set of all the $λ$-terms is denoted $Λ$ and is defined as the language
recognised by the following ||bnf|| grammar.
\Caml(
let _ = sidenote
  << $t,u ::= x \| \t("λx t") \| \t("t u")$ >>
  << $x ∈ \cal{V}_λ$ >>
)
A term of the form $\t("λx t")$ is called an abstraction (or
$λ$-abstractions) and a term of the form $\t("t u")$ is called an
application.
\end{def}
Intuitively, a $λ$-abstraction $\t("λx t")$ forms a function by binding
the variable $x$ in the term $t$. This would be denoted $x ↦ t$ in common
mathematics. Similarly, a term of the form $\t("t u")$ denotes the
application of (the function) $t$ to (the argument) $u$. This would be
denoted $t(u)$ in common mathematics. 
\begin{rem}
As $λ$-terms have a tree-like structure, parenthesis are sometimes required
for disambiguation. For example, the term $\t("λx t u")$ can be read both
as $\t("(λx t) u")$ and as $\t("λx (t u)")$. To lighten the notations
we will consider application to be left-associative and abstraction to bind
stronger that application. As a consequence, we will always read the term
$\t("λx t x u")$ as $\t("λx ((t x) u)")$.
\end{rem}
\begin{rem}
The syntax of the $λ$-calculus only allows for one-place functions. To form
a function of two arguments (or more) one must rely on Curryfication.
Indeed, a function of two arguments can be seen as a function of one
argument returning a function. Following this scheme, the multiple arguments
of the function are given in turn, and not simultaneously. As an example,
the function $(x,y) ↦ x$ can be encoded as $\t("λx λy x")$.
\end{rem}

Although this is not reflected explicitly in the syntax of $λ$-terms,
a $λ$-variable may play two very different roles. It can be used either as
a constant, like $y$ in the constant function $\t("λx y")$, or as a
reference to a binder, like $x$ in the identity function $\t("λx x")$.
Variable binding and the associated notions of free and bound variable are
hence essential.
\begin{def}\label("freelvars")
Given a term $t$, we denote by $FV_λ(t)$ the set of its free $λ$-variables
and $BV_λ(t)$ the set of its bound $λ$-variables. These sets are defined
inductively on the structure of the term $t$.
\begin{center}
\linesBefore(4)
\diagram(
let contents = two_cols
 [ [ <$ FV_λ(x) $>
   ; <$ = $>; <$ \{x\} $> ]
 ; [ <$ FV_λ(\t("λx t")) $>
   ; <$ = $>; <$ FV_λ(t) \setminus \{x\} $> ]
 ; [ <$ FV_λ(\t("t u")) $>
   ; <$ = $>; <$ FV_λ(t) ∪ FV_λ(u) $> ]
 ; [ <$ BV_λ(x) $>
   ; <$ = $>; <$ ∅ $> ]
 ; [ <$ BV_λ(\t("λx t")) $>
   ; <$ = $>; <$ BV_λ(t) ∪ \{x\} $> ]
 ; [ <$ BV_λ(\t("t u")) $>
   ; <$ = $>; <$ BV_λ(t) ∪ BV_λ(u) $> ] ]
let _ = array [`East; `Main; `West; `East; `Main; `West] contents
          ~horizontal_padding:(fun n -> if n = 3 then 10.0 else 1.0)
)
\end{center}
\end{def}
\begin{rem}
Nothing prevents a $λ$-variable to have both free and bound occurrences in
a term. For example, in $t = \t("λx y λy x y")$ the first occurrence of
$y$ is free while its second occurrence is bound.
We have $y ∈ FV_λ(t) = \{y\}$ and $y ∈ BV_λ(t) = \{x, y\}$.
\end{rem}

(* Substitution, α-equivalence and β-reduction. *)
When a $λ$-abstraction (i.e. a function) is applied to an argument, we
obtain a term of the form $\t("(λx t) u")$, called a $β$-redex. The
reduction of such $β$-redexes plays an essential role in computation.
Intuitively, the reduction of the $β$-redex $\t("(λx t) u")$ will be
performed by replacing every occurrence of the bound variable $x$ by $u$ in
the term $t$. This operation, called substitution, is formally defined as
follows.
\begin{def}
Let $t ∈ Λ$ and $u ∈ Λ$ be two $λ$-terms, and $x ∈ \cal{V}_λ$ be a
$λ$-variable. We denote $\lterm("t[x ← u]")$ the term $t$ in which every
free occurrence of $x$ has been replaced by $u$. This operation is defined
inductively on the structure of $t$.
\begin{center}
\linesBefore(5)
\diagram(
let contents =
  let line l r = [ <$ \lterm(l) $> ; <$ = $> ; <$ \lterm(r) $> ] in
  let data =
    [ line "x[x ← u]"       "u"
    ; line "y[x ← u]"       "y"
    ; line "(λx t)[x ← u]"  "λx t"
    ; line "(λy t)[x ← u]"  "λy t[x ← u]"
    ; line "(t₁ t₂)[x ← u]" "t₁[x ← u] t₂[x ← u]" ]
  in two_cols data

let _ =
  array [`East; `Main; `West; `East; `Main; `West] contents
    ~horizontal_padding:(fun n -> if n = 3 then 10.0 else 1.0)
)
\end{center}
\end{def}
Substitution is a subtle notion, and care should be taken to avoid capture
of variables. For example, let us consider the function $\lterm("λx λy x")$
which takes an argument $x$ and returns a constant function with value $x$.
If we apply this function to $y$, the expected result is a constant function
with value $y$. However, if we blindly substitute $x$ with $y$ in
$\lterm("λy x")$ we obtain the identity function $\lterm("λy y")$. Indeed,
the free variable $y$ has been captured and now references a binder that had
(coincidentally) the same name.

To solve this problem, we need to make sure that whenever a substitution
$\lterm("t[x ← u]")$ is performed, no free variable of $u$ is bound in $t$
(i.e. $FV_λ(u) ∩ BV_λ(t) = ∅$). Although we cannot rename the free variables
of $u$, it is possible to rename the bound variables of $t$. Indeed, changing
the name of a bound variable has no effect on the computational behaviour of
a term. Two terms that are equivalent up to the names of their bound
variables are said to be $α$-equivalent.
\begin{def}
The $α$-equivalence relation $({≡}_α) ⊆ Λ×Λ$ is defined, like in
\cite("Krivine1990"), as the smallest relation such that:
\begin{itemize}
\item if $x ∈ \cal{V}_λ$ then $x ≡_α x$,
\item if $t₁ ≡_α t₂$ and $u₁ ≡_α u₂$ then
      $\lterm("t₁ u₁") ≡_α \lterm("t₂ u₂")$,
\item if $\lterm("t₁[x₁ ← y]") \nequiv_α \lterm("t₂[x₂ ← y]")$ for only
      finitely many $y ∈ \cal{V}_λ$ then
      $\lterm("λx₁ t₁") ≡_α \lterm("λx₂ t₂")$.
\end{itemize}
\end{def}
\begin{lem}\label("alphalem")
Given a term $t ∈ Λ$ and a finite set of variables $V ⊆ \cal{V}_λ$, it is
always possible to find a term $t₀ ∈ Λ$ such that $t₀ ≡_α t$ and
$BV_λ(t₀) ∩ V = ∅$.
\begin{proof}
A full proof is available in \id(dcite "Krivine1990" "Lemma 1.11").
\end{proof}
\end{lem}
\begin{def}
Let $t ∈ Λ$ and $u ∈ Λ$ be two $λ$-terms, and $x ∈ \cal{V}_λ$ be a
$λ$-variable. We denote $\lterm("t[x ≔ u]")$ the capture-avoiding
substitution of $x$ by $u$ in $t$. It is defined as $\lterm("t₀[x ← u]")$
where $t₀ ∈ Λ$ is a term such that $t₀ ≡_α t$ and $BV_λ(t₀) ∩ FV(u) = ∅$.
Such a term exists according to \lemma("alphalem").
\end{def}

=<

=> Evaluation contexts and reduction

To define the most general notion of reduction over $λ$-terms, we
need to be able to refer to any $β$-redex. To this aim, we
introduce the notion of evaluation context. Intuitively, a context will
consist in a term with a hole (i.e. a place-holder for a subterm) and it
will allow us to focus on any particular subterm of a term.
\begin{def}
The set of evaluation contexts $[Λ]$ is defined as the language recognised
by the following ||bnf|| grammar.
\Caml(
let _ = sidenote
  << $E, F ::= \lctxt("[]") \| \lctxt("λx E") \| \lctxt("E t") \|
     \lctxt("t E")$ >>
  << $x∈\cal{V}_λ$, $t∈Λ$ >>
)
\end{def}
\begin{def}
Given a term $u ∈ Λ$ and an evaluation context $E ∈ [Λ]$, we denote
$\lterm("E[u]")$ the term formed by putting $u$ into the hole of the
evaluation context $E$. It is defined by induction on the structure of
$E$ as follows.
\begin{center}
\diagram(
let contents =
  let line l r = [ <$ \lterm(l) $> ; <$ = $> ; <$ \lterm(r) $> ] in
  let data =
    [ line "[][u]"     "u"
    ; line "(λx E)[u]" "λx E[u]"
    ; line "(E t)[u]"  "E[u] t"
    ; line "(t E)[u]"  "t E[u]" ]
  in two_cols data
let _ =
  array [`East; `Main; `West; `East; `Main; `West] contents
    ~horizontal_padding:(fun n -> if n = 3 then 10.0 else 1.0)
)
\end{center}
\end{def}
\begin{def}
Given a set of evaluation context $C ⊆ [Λ]$, we denote $\cal{R}(C) ⊆ Λ×Λ$
the $β$-reduction relation induced by $C$. It is defined as the smallest
relation such that for every evaluation context $E ∈ C$, for every terms
$t ∈ Λ$ and $u ∈ Λ$, and for every variable $x ∈ \cal{V}_λ$ we have the
following.
$$ (\lterm("E[(λx t) u]"), \lterm("E[t[x ≔ u]]")) ∈ \cal{R}(C) $$
\end{def}
\begin{def}\label("generalbeta")
The general $β$-reduction $({→}_β) ⊆ Λ×Λ$ is defined as $\cal{R}([Λ])$. We
say that the term $t ∈ Λ$ is in $β$-normal-form if there is no $u ∈ Λ$ such
that $t →_β u$. We denote $({→}_β^{∗})$ the reflexive, transitive closure of
$({→}_β)$.
\end{def}

The general $β$-reduction relation $({→}_β)$ is non-deterministic. Indeed,
given a term $t$, there might be two (different) terms $u₁$ and $u₂$ such
that $t →_β u₁$ and $t →_β u₂$. For example,
$\lterm("((λx₁ x₁) λx₂ x₂) ((λx₃ x₃) λx₄ x₄)")$ can either reduce to
$\lterm("(λx₂ x₂) ((λx₃ x₃) λx₄ x₄)")$ or to
$\lterm("((λx₁ x₁) λx₂ x₂) (λx₄ x₄)")$. Indeed, we can focus on the $β$-redex
$\lterm("(λx₁ x₁) λx₂ x₂")$ using the evaluation context
$\lctxt("[] ((λx₃ x₃) λx₄ x₄)")$, or on the $β$-redex
$\lterm("(λx₃ x₃) λx₄ x₄")$ using the evaluation context
$\lctxt("((λx₁ x₁) λx₂ x₂) []")$.
Although it is non-deterministic, the general $β$-reduction relation
$({→}_β)$ has the Church-Rosser property \cite("Church1936").
\begin{thm}
Let $t ∈ Λ$ be a term. If there are $u₁ ∈ Λ$ and $u₂ ∈ Λ$ such that
$t →_β u₁$ and $t →_β u₂$, then there must be $u ∈ Λ$ such that
${u₁ →_β u}^{∗}$ and ${u₂ →_β u}^{∗}$.
\begin{proof}
A full proof is available in \cite("Church1936").
\end{proof}
\end{thm}
Intuitively, the Church-Rosser property enforces a weak form of determinism.
Indeed, it implies that a program can only compute one particular result,
even if it can be attained in several different ways.

In the following, we are going to consider an effectful language, that does
not have the Church-Rosser property. As a consequence, we will need to
restrict ourselves to a deterministic subset of the general $β$-reduction
relation. If we were to work with a completely non-deterministic reduction
relation, it would be extremely difficult to reason about our language.
Programs would not only compute different possible results, but also
terminate in a non-deterministic way. Moreover, it would be silly to
implement a non-deterministic evaluation procedure for a programming
language.

The choice of the order in which $β$-redexes are reduced is called an
//evaluation strategy//. The two evaluation strategies that are the most
widely used in practice are called //call-by-name// and //call-by-value//.
They both reduce outermost $β$-redexes first, and do not reduce $β$-redexes
that are contained in the body of a $λ$-abstraction. This means that the
term $\lterm("λx (λy y) x")$ is considered to be in normal form and cannot
be evaluated further. In call-by-name, terms that are in function position
are reduced first, and the computation of their arguments is delayed to the
time of their effective use. In call-by-value, both arguments and functions
are evaluated before performing the $β$-reduction. One way to formalize
these evaluation strategies is to restrict the notion of evaluation context,
to only allow focusing on the $β$-redex that is going to be reduced next.
\begin{def}
The set of call-by-name evaluation contexts $[N] ⊆ [Λ]$ is defined as the
language recognised by the following ||bnf|| grammar.
\Caml(
let _ = sidenote
  << $E, F ::= \lctxt("[]") \| \lctxt("E t") $ >>
  << $t∈Λ$ >>
)
The call-by-name reduction relation $({→}_N) ⊆ Λ×Λ$ is defined as
$\cal{R}([N])$.
\end{def}

In call-by-value, both the function and its argument need to be fully
evaluated before the application can be performed. Consequently, two
different call-by-value strategies can be defined: left-to-right and
right-to-left call-by-value evaluation. The former fully evaluates the
terms that are in function position first and the latter evaluates the
terms that are in argument position first. Although left-to-right
call-by-value evaluation is most widely used, some practical languages
like OCaml use right-to-left evaluation. In this thesis, we make the
same choice and only consider right-to-left call-by-value evaluation.
\begin{def}
A term $t$ is said to be a value if it is either a $λ$-variable or a
$λ$-abstraction. The set $Λ_{val} ⊆ Λ$ of all the values is generated by the
following ||bnf|| grammar.
\Caml(
let _ = sidenote
  << $v,w ::= x \| \lterm("λx t") $ >>
  << $x ∈ \cal{V}_λ$ >>
)
\end{def}
\begin{def}
The set of right-to-left call-by-value evaluation contexts $[V] ⊆ [Λ]$ is
defined as the language recognised by the following ||bnf|| grammar.
\Caml(
let _ = sidenote
  << $E, F ::= \lctxt("[]") \| \lctxt("E v") \| \lctxt("t E") $ >>
  << $v∈Λ_{val}$, $t∈Λ$ >>
)
The right-to-left call-by-value reduction relation $({→}_V) ⊆ Λ×Λ$ is
defined as $\cal{R}([V])$.
\end{def}
\begin{rem}
Left-to-right call-by-value evaluation can be defined using evaluation
contexts generated by the following ||bnf|| grammar.
\Caml(
let _ = sidenote
  << $E, F ::= \lctxt("[]") \| \lctxt("E t") \| \lctxt("v E") $ >>
  << $t∈Λ$, $v∈Λ_{val}$ >>
)
\end{rem}

A given term of the $λ$-calculus may reduce in a very different way with
different evaluation strategies. For example, the evaluation of the term
$\lterm("(λy z) ((λx x x) (λx x x))")$ stops in one step in call-by-name
$$ \lterm("(λy z) ((λx x x) (λx x x))") →_N \lterm("z") $$
and it goes into a loop in call-by-value.
$$ \lterm("(λy z) ((λx x x) (λx x x))") →_V
   \lterm("(λy z) ((λx x x) (λx x x))") $$

\begin{rem}
Our reduction relations can be alternatively defined using a deduction rule
system. A deduction rule is formed using premisses $\{P_i\}_{1≤i≤n}$ and
a conclusion $C$ separated by an horizontal bar.
$$ \ternaryR{P₁}{...}{P_n}{C} $$
The meaning of such a rules is that the conclusion $C$ can be deduced when
all the premisses $P_i$ are true. In particular, if there is no premise then
the conclusion can be deduced immediately. Using this formalism, the
call-by-value reduction corresponds to the smallest relation satisfying the
following two rules.
$$
  \axiomR{\lterm("(λx t) u") →_N \lterm("t[x ≔ u]")}
  \hspace(6.0)
  \unaryR{\lterm("t₁") →_N \lterm("t₂")}{\lterm("t₁ u") →_N \lterm("t₂ u")}
$$
Similarly, the right-to-left call-by-value reduction relation $({→}_V)$
corresponds to the smallest relation satisfying the following three rules.
$$
  \axiomR{\lterm("(λx t) v") →_V \lterm("t[x ≔ v]")}
  \hspace(4.0)
  \unaryR{\lterm("u₁") →_V \lterm("u₂")}{\lterm("t u₁") →_V \lterm("t u₂")}
  \hspace(4.0)
  \unaryR{\lterm("t₁") →_V \lterm("t₂")}{\lterm("t₁ v") →_V \lterm("t₂ v")}
$$
\end{rem}

In this thesis, $λ$-terms and programs in general will be evaluated in an
abstract machine called a Krivine machine \cite("Krivine2007"). This
machine will emulate the right-to-left evaluation relation $({→}_V)$. It
will provide us with a computational framework in which programs and their
evaluation contexts can be manipulated easily.

=<

=> Call-by-value Krivine machine

In the previous section, we introduced the syntax of the $λ$-calculus and
the evaluation of $λ$-terms. We will now reformulate these definition in
terms of a call-by-value Krivine abstract machine \cite("Krivine2007").
Our presentation will differ from the original machine, which is
call-by-name. Although call-by-value Krivine machines have rarely been
published, they are well-known in the classical realizability community.

The main idea behind the Krivine abstract machine is to think of a term
$\t("t₀") ∈ Λ$ as a couple $(\t("t"),\c("E")) ∈ Λ × [V]$ such that
$\t("t₀") = \t("E[t]")$ (the term $\t("t")$ is said to be in head position).
Using this reprensentation, $β$-reduction proceeds in two steps. First, the
machine state $(\t("t"), \c("E"))$ is transformed into a state of the form
$(\t("(λx u) v"), \c("F"))$, in such a way that
$\t("E[t]") = \t("F[(λx u) v]")$. The $β$-redex can then be reduced to
obtain the state $(\t("u[x ≔ v]"), \c("F"))$. This behaviour can be attained
using the following reduction rules, which are obtained naturally from the
definition of right-to-left call-by-value evaluation.
\begin{center}
\diagram(
let _ =
  let line (t1,c1) (t2,c2) n =
    [ <$(\t(t1), \c(c1))$> ; <$→$>; <$(\t(t2), \c(c2))$>; n ]
  in
  array [`East ; `Main ; `West; `West] ~horizontal_padding:(function _ -> 5.0)
    [ line ("t u"     , "E"      ) ("u"       , "E[t []]")
      <$rhen \hspace(0.4) u∉Λ_{val}$>
    ; line ("v"       , "E[t []]") ("t v"     , "E"      )
      []
    ; line ("t v"     , "E"      ) ("t"       , "E[[] v]")
      <$when \hspace(0.4) t∉Λ_{val}$>
    ; line ("v"       , "E[[] w]") ("v w"     , "E"      )
      []
    ; line ("(λx t) v", "E"      ) ("t[x ≔ v]", "E"      )
      [] ]
)
\linesAfter(2)
\end{center}
The four first rules are responsible for bringing the next $β$-redex
(according to our reduction strategy) in head position, and the last
rule performs the $β$-reduction. Note that the first four rules do not
change the represented term, and only move arguments or functions between
the term and the evaluation context. Our set of reduction rule can be
simplified to the following, by composing the last two pairs of rules.
\begin{center}
\linesBefore(5)
\diagram(
let _ =
  let line (t1,c1) (t2,c2) =
    [ <$(\t(t1), \c(c1))$> ; <$→$>; <$(\t(t2), \c(c2))$> ]
  in
  array [`East ; `Main ; `West] ~horizontal_padding:(function _ -> 5.0)
    [ line ("t u" , "E"      ) ("u"       , "E[t []]")
    ; line ("v"   , "E[t []]") ("t"       , "E[[] v]")
    ; line ("λx t", "E[[] v]") ("t[x ≔ v]", "E"      ) ]
)
\linesAfter(2)
\end{center}
The first rule is used to focus on the argument of an application, to compute
it first. When the argument has been evaluated to a value, the second rule
can be used to swap the argument with the unevaluated function. The
computation can then continue with the evaluation of the function, which
should (hopefully) evaluate to a $λ$-abstraction. If it is the case, the
third rule can be applied to actually perform the $β$-reduction.

The state of the abstract machine can be seen as a zipper \cite("Huet1997")
on the tree structure of a term. Indeed, the term that is in head position
is the subterm on which the machine is focusing. It is also worth noting
that the machine manipulates evaluation contexts from the inside out, which
results in a heavy syntax. However, it is possible to represent right-to-left
call-by-value evaluation contexts using a stack of functions (i.e. terms)
and argument (i.e. values). We will take this approach in the following.
\begin{def}
Values, terms, stacks and processes are generated by the following four
||bnf|| grammars. The names of the corresponding sets are displayed on
the left.
\Caml(
let _ = bnfs
  [ ( << $(Λ_{val})$ >>
    , << $v, w$ >>
    , << $\v("x") \| \v("λx t")$ >>
    , << $x∈\cal{V}_λ$ >> )
  ; ( << $(Λ)$ >>
    , << $t, u$ >>
    , << $\t("v") \| \t("t u")$ >>
    , [] )
  ; ( << $(Π)$ >>
    , << $π, ρ$ >>
    , << $\s("ε") \| \s("v·π") \| \s("[t]π")$ >>
    , [] )
  ; ( << $(Λ × Π)$ >>
    , << $p, q$ >>
    , << $\p("t ∗ π")$ >>
    , [] ) ]
)
\end{def}
The syntactic distinction between terms and values is specific to the
call-by-value presentation, they would be collapsed in call-by-name.
Intuitively, a stack can be thought of as an evaluation context represented
as a list of terms and values. The values are to be considered as arguments
to be fed to the term in the context, and the terms are to be considered as
functions to which the term in the context will be applied. The symbol $ε$
is used to denote an empty stack. A process $\p("t ∗ π")$ is to be considered
as the state of our abstract machine. Its reduction will consist in the
interaction between the term $t$ and its evaluation context encoded into a
stack $π$.

As we are considering a call-by-value calculus, only values are (and should
be) substituted to $λ$-variables during evaluation. From now on, we will
hence work with the following definition of substitution. In particular, a
substitution of the form $\lterm("t[x ≔ u]")$ will be forbidden if $u$ is not
a syntactic value.
\begin{def}
Let $t ∈ Λ$ be a term, $x ∈ \cal{V}_λ$ be a $λ$-variable and $v ∈ Λ_{val}$
be a value. We denote $\t("t[x ≔ v]")$ the capture-avoiding substitution
of $x$ by $v$ in $t$.
\end{def}
\begin{def}
The reduction relation $({≻}) ⊆ (Λ×Π) × (Λ×Π)$ is defined as the smallest
relation satisfying the following rules. We will denote $({≻}^{∗})$ its
reflexive and transitive closure.
\begin{center}
\linesBefore(4)
\diagram(
let _ =
  let line l r = [ <$\p(l)$> ; <$≻$>; <$\p(r)$> ] in
  array [`East ; `Main ; `West] ~horizontal_padding:(function _ -> 5.0)
    [ line "t u ∗ π"    "u ∗ [t]π"
    ; line "v ∗ [t] π"  "t ∗ v·π"
    ; line "λx t ∗ v·π" "t[x ≔ v] ∗ π" ]
)
\end{center}
\end{def}
Three reduction rules are used to handle call-by-value evaluation. When an
application is encountered, the function is stored in a stack-frame in order
to evaluate its argument first. Once the argument has been completely
computed, a value faces the stack-frame containing the function. At this
point the function can be evaluated and the value is stored in the stack,
ready to be consumed by the function as soon as it evaluates to a
$λ$-abstraction. A capture-avoiding substitution can then be performed to
effectively apply the argument to the function. As an example, the process
$\p("(λx x y) λz z ∗ ε")$ reduces to $\p("y ∗ ε")$ in the following way, and
cannot evaluate further.
\begin{center}
\linesBefore(7)
\diagram(
let _ =
  let line s = [ []; <$≻$>; <$\p(s)$> ] in
  array [`East ; `East ; `West]
  [ [ <$\p("(λx x y) λz z ∗ ε")$>; <$≻$>; <$\p("λz z ∗ [λx x y]ε")$> ]
  ; line "λx x y ∗ λz z · ε"
  ; line "(λz z) y ∗ ε"
  ; line "y ∗ [λz z] ε"
  ; line "λz z ∗ y · ε"
  ; line "y ∗ ε" ]
)
\end{center}

\begin{rem}
A left-to-right call-by-value machine could be defined in a similar way, but
the roles of terms and values would be swapped in stacks. Stack frames would
contain values, and terms would be pushed on the stack. The resulting
reduction relation would be the following.
\begin{center}
\diagram(
let _ =
  let line l r = [ <$\proc(l)$> ; <$≻_{RL}$>; <$\proc(r)$> ] in
  array [`East ; `Main ; `West] ~horizontal_padding:(function _ -> 5.0)
    [ line "t u ∗ π"     "t ∗ u·π"
    ; line "v ∗ u·π"     "u ∗ [v]π"
    ; line "v ∗ [λx t]π" "t[x ≔ v] ∗ π" ]
)
\end{center}
\end{rem}

The state of our abstract machine contains two parts: a term being evaluated
(i.e. the term in head position) and its evaluation context (i.e. the stack).
As a consequence, it is possible to define reduction rules that manipulate
the stack (i.e. the evaluation context) as a first class object. Such
reduction rules produce computational effects.

=<

=> Computational effects and \lmcalc

We are now going to extend the calculus and our abstract machine with
operations allowing the manipulation of the stack. More precisely, we
will provide a way to save the stack (i.e. the evaluation context or the
continuation), so that it can be restored at a later stage. A natural
way to extend our language is to use the syntax of Michel Parigot's
\lmcalc \cite("Parigot1992"). We hence introduce a new binder $\t("μα t")$
capturing the current stack in the $μ$-variable $α$. The stack can then be
restored in $t$ using the syntax $\t("[α]u")$.
\begin{def}
Let $\cal{V}_μ = \{α, β, γ...\}$ be a countable set of $μ$-variables (or
stack variables) disjoint from $\cal{V}_λ$. Value, terms, stacks and
processes are now generated by the following grammars. The names of the
corresponding sets are displayed on the left.
\pagesBefore(1) (* hack *)
\Caml(
let _ = bnfs
  [ ( << $(Λ_{val})$ >>
    , << $v, w$ >>
    , << $\v("x") \| \v("λx t")$ >>
    , << $x∈\cal{V}_λ$ >> )
  ; ( << $(Λ)$ >>
    , << $t, u$ >>
    , << $\t("v") \| \t("t u") \| \t("μα t") \| \t("[α]t")$ >>
    , [] )
  ; ( << $(Π)$ >>
    , << $π, ρ$ >>
    , << $\s("ε") \| \s("α") \| \s("v·π") \| \s("[t]π")$ >>
    , << $α∈\cal{V}_μ$ >> )
  ; ( << $(Λ × Π)$ >>
    , << $p, q$ >>
    , << $\p("t ∗ π")$ >>
    , [] ) ]
)
\end{def}
Note that terms of the form $\t("[π]t")$ will only be available to the user
if $π$ is a stack variable. Allowing arbitrary stacks allow us to substitute
$μ$-variables by stacks during computation. Like with $λ$-variable, we will
need to be careful and avoid variable capture. However, we will not give
the full details this time.
\begin{def}
Given a value, term, stack or process $ψ$, we denote $FV_λ(ψ)$ (resp.
$BV_λ(ψ)$) the set of its free (resp. bound) $λ$-variables and $FV_μ(ψ)$
(resp. $BV_μ(ψ)$) the set of its free (resp. bound) $μ$-variables. These
sets are defined in a similar way to \definition("freelvars").
\end{def}
\begin{def}
Let $t ∈ Λ$ be a term, $π ∈ Π$ be a stack and $α ∈ \cal{V}_μ$ be a
$μ$-variable. We denote $\t("t[α ≔ π]")$ the (capture-avoiding) substitution
of $α$ by $π$ in $t$.
\end{def}
\begin{def}
The reduction relation $({≻})$ is extended with two new reduction rules.
\begin{center}
\linesBefore(7)
\diagram(
let _ =
  let line l r = [ <$\p(l)$> ; <$≻$>; <$\p(r)$> ] in
  array [`East ; `Main ; `West] ~horizontal_padding:(function _ -> 5.0)
    [ line "t u ∗ π"      "u ∗ [t]π"
    ; line "v ∗ [t] π"    "t ∗ v · π"
    ; line "λx t ∗ v · π" "t[x ≔ v] ∗ π"
    ; line "μα t ∗ π"     "t[α ≔ π] ∗ π"
    ; line "[ρ]t ∗ π"     "t ∗ ρ" ]
)
\end{center}
\end{def}
Now, when the abstract machine encounters a $μ$-abstraction $\t("μα t")$, the
current stack $π$ is substituted to the $μ$-variables $α$. As a consequence,
every subterm of the form $\t("[α]u")$ in $t$ becomes $\t("[π]u")$. When the
machine then reaches a state of the form $\p("[π]u ∗ ρ")$, the current stack
$ρ$ is erased, and computation resumes with the stored stack $π$. For
example, the process $\p("λx μα t [α]x ∗ v·ε")$ where $t$ is an arbitrary
term and $v$ is an arbitrary value reduces as follows.
\begin{center}
\linesBefore(5)
\diagram(
let _ =
  let line s = [ []; <$≻$>; <$\p(s)$> ] in
  array [`East ; `East ; `West]
  [ [ <$\p("λx μα t [α]x ∗ v·ε")$>; <$≻$>; <$\p("μα t[x ≔ v] [α]v ∗ ε")$> ]
  ; line "t[x ≔ v] [ε]v ∗ ε"
  ; line "[ε]v ∗ [t[x ≔ v]]ε"
  ; line "v ∗ ε" ]
)
\end{center}
Note that when a stack is erased, arbitrary terms might be erased. In
particular, we could have chose $t = Ω = \t("(λx x x) λx x x")$ in the
previous example, although the reduction of this term does not terminate.
Indeed, we have
\begin{center}
\linesBefore(4)
\diagram(
let _ =
  let line s = [ []; <$≻$>; <$\p(s)$> ] in
  array [`East ; `East ; `West]
  [ [ <$\p("Ω ∗ π")$>; <$≻$>; <$\p("λx x x ∗ [λx x x] π")$> ]
  ; line "λx x x ∗ λx x x · π"
  ; line "Ω ∗ π" ]
)
\end{center}
for every possible stack $π$.

The abstract machine defined in this section can be used for evaluating
terms of the \lmcalc. Although this language is very elegant and concise,
it is not suitable for practical programming. In the following section,
our language will be extend with records (i.e. tuples with named fields)
and variants (i.e. constructors and pattern-matching). Consequently, we
will obtain a simple language with a concise formal definition, but that
will be closer to beging a practical programming language.

=<

=> Full syntax and operational semantics

In this section, we present the syntax and the reduction relation of the
abstract machine that will be considered throughout this thesis. Although
the following extends definitions given in the previous secitons, we
choose not to avoid repetitions so that this section remains completely
self-contained.

In this thesis, we consider a language that is expressed in terms of a
//Krivine Abstract Machine// \cite("Krivine2007"). As our machine has the
specificity of being call-by-value, which requires a syntax formed with
four entities: values, terms, stacks and processes. Note that the
distinction between terms and values is specific to our call-by-value
presentation, they would be collapsed in call-by-name.
\begin{def}
We require three distinct, countable sets of variables:
$\cal{V}_λ = \{x, y, z...\}$ for $λ$-variables,
$\cal{V}_μ = \{α, β, γ...\}$ for $μ$-variables and
$\cal{V}_ι = \{a, b, c...\}$ for term variables.
We also require a countable set $\cal{L} = \{l, k...\}$ of labels to
name record fields and a countable set $\cal{C} = \{C, D...\}$ for
naming constructors.
\end{def}
As usual, our $λ$-variables and $μ$-variables will be bound in terms to
respectively form functions and capture continuations. Term variables
will always appear free in terms, but they will be bound in types (see
\chapter("typesystem")).

\begin{def}
Value, terms, stacks and processes are mutually inductively defined as the
languages recognised by the following grammars. The names of the
corresponding sets are displayed on the left.
\Caml(
let _ = bnfs
  [ ( << $(Λ_{val})$ >>
    , << $v, w$ >>
    , << $\v("x") \| \v("λx t") \| \v("C[v]") \| \v("{(li = vi) i∈I}")$ >>
    , << $x∈\cal{V}_λ$ >> )
  ; ( << $(Λ)$ >>
    , << $t, u$ >>
    , << $\t("a") \| \t("v") \| \t("t u") \| \t("μα t") \| \t("[π]t")
         \| \t("v.l") \| \t("[v | (Ci[xi] → ti) i∈I]") \| \t("Y(t, v)")
         \| \t("U(v)") \| \t("δ(v,w)") $ >>
    , << $a∈\cal{V}_ι$ >> )
  ; ( << $(Π)$ >>
    , << $π, ρ$ >>
    , << $\s("ε") \| \s("α") \| \s("v·π") \| \s("[t]π")$ >>
    , << $α∈\cal{V}_μ$ >> )
  ; ( << $(Λ × Π)$ >>
    , << $p, q$ >>
    , << $\p("t ∗ π")$ >>
    , [] ) ]
)
\end{def}
Terms and values form a variation of the \lmcalc \cite("Parigot1992"),
enriched with ML-like constructs (i.e. records and variants). A value of
the form $\v("C[v]")$ corresponds to a constructor, which always has
exactly one argument in our language. Pattern-matching on constructors is
done using a syntax of the form $\t("[v | (Ci[xi] → ti) i∈I]")$, which
includes a finite list of patterns associating $\v("Ci[xi]")$ to the term
$t_i$ for all $i$ in $I$.
Similarly, values of the form $\v("{(li = vi) i∈I}")$ correspond to records,
which are tuples with named fields. The empty tuple (i.e. the tuple with
no field) will be denoted $\v("{}")$. A term of the form $\t("Y(t,v)")$
denotes a fixpoint combinator, which roughly corresponds to OCaml's
“##let rec##” construct. The terms of the form $\t("U(v)")$ of
$\t("δ(v,w)")$ are only included for technical purposes that are going to
be explained later. They are not intended to be used for programming, and
are not even included in the implementation.

A stack can be either the empty stack $ε$, a stack variable, a value pushed
on top of a stack, or a stack frame containing a term on top of a stack.
These two constructors are specific to the call-by-value presentation, only
one is required in call-by-name.
\begin{rem}
The syntax $\t("[v | (Ci[xi] → ti) i∈I]")$ for pattern-matching and the
syntax $\v("{(l = v) i∈I}")$ for records are part of our meta-language. We
only use them as shortcuts to designate pattern-matching on arbitrary
patterns and records with arbitrary fields. In the actual syntax, the full
list of pattern or the full list of fields always need to be specified. For
example, we would write $\t("{l₁ = v₁; l₂ = v₂;}")$ or
$\t("[v | C₁[x₁] → t₁ | C₂[x₂] → t₂]")$ if $I = \{1, 2\}$.
\end{rem}
\begin{rem}
We enforce values in constructors, record fields, projection and case
analysis. This makes the calculus simpler because only $β$-reduction will
manipulate the stack. We can define syntactic sugars such as the following
to hide the restriction from the programmer.
\Caml(
let sugarproj =
  let open Lang_maths in
  let open Lang_ast in
  t2m (TProj (VMeta ("t", None), ("l", None)))
let sugarcons =
  let open Lang_maths in
  let open Lang_ast in
  t2m (TValu (VCons (("C", None), VMeta ("t", None))))
)
$$
  \sugarproj ≔ \t("(λx x.l) t")
  \hspace(4.0)
  \sugarcons ≔ \t("(λx C[x]) t")
$$
\end{rem}
\begin{def}
Given a value, term, stack or process $ψ$ we denote $FV_λ(ψ)$ (resp.
$FV_μ(ψ)$, $FV_ι(ψ)$) the set of free $λ$-variables (resp. free
$μ$-variables, term variables) contained in $ψ$. We also denote
$BV_λ(ψ)$ (resp. $BV_μ(ψ)$) the set of bound $λ$-variables (resp.
$μ$-variables) contained in $ψ$. We say that $ψ$ is closed if
$FV_λ(ψ) ∪ FV_μ(ψ) ∪ FV_ι(ψ) = ∅$.
\end{def}
\begin{def}
Let $ψ$ be a value, term, stack or process. Given a $λ$-variable
$x ∈ \cal{V}_λ$ and a value $w ∈ Λ_{val}$, we denote $\p("ψ[x ≔ v]")$
the capture-avoiding substitution of $x$ by $v$ in $ψ$. Given a
$μ$-variable $α ∈ \cal{V}_μ$ and a stack $π ∈ Π$, we denote
$\p("ψ[α ≔ π]")$ the capture-avoiding substitution of $α$ by $π$ in $ψ$.
Given a term variable $a ∈ \cal{V}_ι$ and a term $t ∈ Λ$, we denote
$\p("ψ[a ≔ t]")$ the substitution of $a$ by $t$ in $ψ$.
\end{def}

Processes form the internal state of our abstract machine. They are to be
thought of as a term put in some evaluation context represented using a
stack. Intuitively, the stack $π$ in the process $\p("t ∗ π")$ contains the
arguments to be fed to $t$. Since we are in call-by-value the stack also
handles the storing of functions while their arguments are being evaluated.
This is why we need stack frames (i.e. stacks of the form $\s("[t]π")$). The
operational semantics of our language is given by a relation $({≻})$ over
processes.
\begin{def}
The relation $({≻}) ⊆ (Λ×Π) × (Λ×Π)$ is defined as the smallest relation
satisfying the following reduction rules.
\begin{center}
\diagram(
let _ =
  let line p1 p2 = [ <$\p(p1)$>; <$≻$>; <$\p(p2)$> ] in
  array [`East ; `Main ; `West]
  ~horizontal_padding:(function 1 -> 5.0 | _ -> 4.0) (* Fix *)
  [ line "t u ∗ π"                   "u ∗ [t]π"
  ; line "v ∗ [t]π"                  "t ∗ v·π"
  ; line "λx t ∗ v·π"                "t[x ≔ v] ∗ π"
  ; line "μα t ∗ π"                  "t[α ≔ π] ∗ π"
  ; line "[ρ]t ∗ π"                  "t ∗ ρ"
  ; line "{⋯ l = v ⋯}.l ∗ π"         "v ∗ π"
  ; line "[C[v] | ⋯ C[x] → t ⋯] ∗ π" "t[x≔v] ∗ π"
  ; line "Y(t,v) ∗ π"                "t ∗ λx Y(t,x)·v·π"
  ; line "U({}) ∗ π"                 "{} ∗ π" ]
)
\linesAfter(2)
\end{center}
We will denote $({≻}^{+})$ its transitive closure, $({≻}^{∗})$ its
reflexive-transitive closure and $({≻}^k)$ its $k$-fold application.
\end{def}
The first three rules are those that handle $β$-reduction. When the abstract
machine encounters an application, the function is stored in a stack-frame in
order to evaluate its argument first. Once the argument has been completely
computed, a value faces the stack-frame containing the function. At this
point the function can be evaluated and the value is stored in the stack
ready to be consumed by the function as soon as it evaluates to a
$λ$-abstraction. A capture-avoiding substitution can then be performed to
effectively apply the argument to the function. The fourth and fifth rules
rules handle the classical part of computation. When a $μ$-abstraction is
reached, the current stack (i.e. the current evaluation context) is captured
and substituted for the corresponding $μ$-variable. Conversely, when a
term of the form $\t("[ρ]t")$, the current stack is thrown away and
evaluation resumes with the process $\p("t ∗ ρ")$.
(* *)
In addition to the reduction rules for $β$-reduction and stack manipulation,
we provide reduction rules for handling record projection, case analysis and
recursion using a fixpoint operator.

\begin{lem}\label{redcompatall}
The reduction relation $({≻})$ is compatible with substitutions of variables
of any kind. More formally, if $p$ and $q$ are processes such that $p ≻ q$
then $\p("p[x ≔ v]") ≻ \p("q[x ≔ v]")$ for all $x∈\cal{V}_λ$ and $v∈Λ_{val}$.
Similarly, $\p("p[α ≔ π]") ≻ \p("q[α ≔ π]")$ for all $α ∈ \cal{V}_μ$ and
$π ∈ Π$, and $\p("p[a ≔ t]") ≻ \p("q[a ≔ t]")$ for all $a ∈ \cal{V}_ι$ and
$t ∈ Λ$.
\begin{proof}
Immediate case analysis on the reduction rules.
\end{proof}
\end{lem}
\begin{lem}
If $σ$ is a substitution for variables of any kind and if $p≻q$ (resp.
$p ≻^{∗} q$, $p ≻^{+} q$, $p ≻^k q$) then $\p("pσ") ≻ \p("qσ")$ (resp.
$\p("pσ") ≻^{∗} \p("qσ")$, $\p("pσ") ≻^{+} \p("qσ")$,
$\p("pσ") ≻^k \p("qσ")$).
\begin{proof}
Immediate consequence of \lemma("redcompatall") and the definition of the
relations.
\end{proof}
\end{lem}

We are now going to give the vocabulary that will be used to describe some
specific classes of processes. In particular we need to identify processes
that are to be considered as the evidence of a successful computation, and
those that are to be recognised as the expression of a failure of the
machine (i.e. a crash).
\begin{def}
A process $p ∈ Λ×Π$ is said to be:
\begin{itemize}
\item //final// if there is a value $v ∈ Λ_{val}$ such that $p = \p("v ∗ ε")$,
\item //$δ$-like// if there are values $v,w ∈ Λ_{val}$ and a stack $π ∈ Π$
      such that $\p("p") = \p("δ(v,w) ∗ π")$,
\item //blocked// if there is no $q ∈ Λ×Π$ such that $p ≻ q$,
\item //stuck// if it is not final nor $δ$-like and if for every substitution
      $σ$, $\p("pσ")$ is blocked,
\item //non-terminating// if there is no blocked process $q ∈ Λ×Π$ such that
      $p ≻^{∗} q$.
\end{itemize}
\end{def}
\begin{lem}\label("redstable")
Let $p$ be a process and $σ$ be a substitution for variables of any kind.
If $p$ is final (resp. stuck, resp. non-terminating, resp. $δ$-like) then
$pσ$ is also final (resp. stuck, resp. non-terminating, resp. $δ$-like).
\begin{proof}
Immediate by definition.
\end{proof}
\end{lem}

\begin{lem}\label("remark")
A stuck state can only be of one of the following forms.
$$
\p("C[v].l ∗ π")
\hspace(2.0)
\p("(λx t).l ∗ π")
\hspace(2.0)
\p("C[v] ∗ w·π")
\hspace(2.0)
\p("{(li = vi) i∈I} ∗ v·π")
$$
$$
\p("[λx t | (Ci[xi] → ti) i∈I] ∗ π")
\hspace(4.0)
\p("[{(li = vi) i∈I} | (Cj[xj] → tj) j∈J] ∗ π")
$$
$$
\p("[D[v] | (Ci[xi] → ti) i∈I] ∗ π") \hspace(2.0) with {D ≠ C_i} for all i∈I
$$
$$
\p("{(li = vi) i∈I}.k ∗ π")      \hspace(2.0) with {k ≠ l_i} for all i∈I
$$
$$
\p("U(λx t) ∗ π")
\hspace(2.0)
\p("U(C[v]) ∗ π")
\hspace(2.0)
\p("U({(li = vi) i∈I≠∅}) ∗ π")
\hspace(2.0)
\p("[x |] ∗ π")
$$
\begin{proof}
  Simple case analysis.
\end{proof}
\end{lem}

\begin{lem}\label("possibilities")
A blocked process $p ∈ Λ×Π$ is either stuck, final, $δ$-like, or of one of
the following forms.
$$
\p("x.l ∗ π")
\hspace(2.0)
\p("x ∗ v·π")
\hspace(2.0)
\p("[x | (Ci[xi] → ti) i∈I≠∅] ∗ π")
$$
$$
\p("a ∗ π")
\hspace(2.0)
\p("U(x) ∗ π")
\hspace(2.0)
\p("v ∗ α")
\hspace(2.0)
\p("v ∗ ε")
$$
\begin{proof}
Straight-forward case analysis using \lemma("remark").
\end{proof}
\end{lem}

=<

=> Observational equivalence

In this section, we define a notion of equivalence over terms. The idea is
that two terms should be considered equivalent if they have the same
observable computational behaviour in every evaluation context. In this
thesis, the considered observable behavious is successful termination
(versus non-termination or runtime error).
\begin{def}
Given a reduction relation $R$, we say that a process $p ∈ Λ×Π$ converges,
and we write $p {⇓}_R$, if there is a final state $q ∈ Λ×Π$ such that
$(p,q) ∈ {R^{∗}}$ (where $R^{∗}$ is the reflexive-transitive closure of $R$).
If $p$ does not converge we say that it diverges and write $p {⇑}_R$. Note
that we will use the notations $p {⇓}_i$ and $p {⇑}_i$ when working with
indexed relation symbols such as $({\epi}_i)$.
\end{def}
\begin{def}
The relation $({≡}_{≻}) ⊆ {Λ×Λ}$ is define as follows.
$$ ({≡}_{≻}) = \{(t, u) \| ∀{π∈Π}, ∀σ,
     {{\p("tσ∗π")} {⇓}_{≻}} ⇔ {{\p("uσ∗π")} {⇓}_{≻}}\} $$
\end{def}
\begin{lem}
$({≡}_{≻})$ is an equivalence relation.
\begin{proof}
Immediate.
\end{proof}
\end{lem}

To be able to work with the equivalence relation $({≡})_{≻}$, we need to
check that it is extensional. In other words, we need to be able to replace
equals by equals at any place in terms without changing their observed
behaviour. This property is summarized in the following two theorems.
\begin{thm}\label("sextval")
Let $v₁ ∈ Λ_{val}$ and $v₂ ∈ Λ_{val}$ be values, $t ∈ Λ$ be a term and
$x ∈ \cal{V}_ι$ be a $λ$-variable. If $v₁ ≡_{≻} v₂$ then
$\t("t[x≔v₁]") ≡_{≻} \t("t[x≔v₂]")$.
\begin{proof}
We are going to prove the contrapositive so we suppose
$\t("t[x≔v₁]") \nequiv_{≻} \t("t[x≔v₂]")$ and we show $v₁ \nequiv_{≻} v₂$.
By definition, there is a stack $π$ and a substitution $σ$ such that we
have ${\p("(t[x≔v₁])σ ∗ π")} {⇓}_{≻}$ and ${\p("(t[x≔v₂])σ ∗ π")} {⇑}_{≻}$
(up to symmetry). As $x$ is bound we can rename it so that
$\t("(t[x≔v₁])σ") = \t("tσ[x≔v₁σ]")$ and
$\t("(t[x≔v₂])σ") = \t("tσ[x≔v₂σ]")$. To finish the proof, we need to find
a stack $π₀$ and a substitution $σ₀$ such that ${\p("v₁σ₀ ∗ π₀")} {⇓}_{≻}$
and ${\p("v₂σ₀ ∗ π₀")} {⇑}_{≻}$ (up to symmetry). We can take
$π₀ = \s("[λx tσ]π")$ and $σ₀ = σ$ since by definition we know that
${\p("v₁σ ∗ [λx tσ]π")} ≻^2 {\p("tσ[x≔v₁σ] ∗ π")} {⇓}_{≻}$ and
${\p("v₂σ ∗ [λx tσ]π")} ≻^2 {\p("tσ[x≔v₂σ] ∗ π")} {⇑}_{≻}$.
\end{proof}
\end{thm}
\begin{lem}\label("sexttermaux")
Let $p ∈ Λ×Π$ be a process, $a ∈ \cal{V}_τ$ be a term variable and $t$ be
a term such that ${\p("p[a≔t]")} {⇓}_{≻}$. Either there is a value
$v ∈ Λ_{val}$ such that $p ≻^{∗} \p("v ∗ ε")$ or there is a stack $π ∈ Π$
such that $p ≻^{∗} \p("a ∗ π")$.
\begin{proof}
There must be a blocked process $q$ such that $p ≻^{∗} q$. If it were not
the case $p$ would be non-termination, and hence $\p("p[a≔t]")$ would also
be non-termination according to \lemRef("redstable"). This would hence
contradict the hypothesis that ${\p("p[a≔t]")} {⇓}_{≻}$. Since
$\p("p") ≻^{∗} \p("q")$ we obtain $\p("p[a≔t]") ≻^{∗} \p("q[a≔t]")$ by
\lemRef("redcompatall") and hence ${\p("q[a≔t]")} {⇓}_{≻}$. We then
proceed by case analysis according to \lemRef("possibilities"). If $q$ is
not of the form $\p("v ∗ ε")$ or $\p("a ∗ π")$ then $\p("q[a≔t]")$ cannot
be reduced, which contradicts ${\p("q[a≔t]")} {⇓}_{≻}$.
\end{proof}
\end{lem}
\begin{thm}\label("sextterm")
Let $u₁ ∈ Λ$, $u₂ ∈ Λ$ and $t ∈ Λ$ be three terms and $a ∈ \cal{V}_τ$ be a
term variable. If $u₁ ≡_{≻} u₂$ then $\t("t[a≔u₁]") ≡_{≻} \t("t[a≔u₂]")$.

\begin{proof}
Let us suppose $u₁ ≡_{≻} u₂$ and show $\t("t[a≔u₁]") ≡_{≻} \t("t[a≔u₂]")$.
We take a stack $π$, a substitution $σ$ and we show
${{\p("(t[a≔u₁])σ ∗ π")} {⇓}_{≻}} ⇔ {{\p("(t[a≔u₂])σ ∗ π")} {⇓}_{≻}}$. As we
are free to rename $a$ we may assume $\t("(t[a≔u₁])σ") = \t("tσ[a≔u₁σ]")$,
$\t("(t[a≔u₂])σ") = \t("tσ[a≔u₂σ]")$ and $a ∉ FV(π)$. Consequently our goal
is now ${{\p("tσ[a≔u₁σ] ∗ π")} {⇓}_{≻}} ⇔ {{\p("tσ[a≔u₂σ] ∗ π")} {⇓}_{≻}}$.
By symmetry we can suppose that ${\p("tσ[a≔u₁σ] ∗ π")} {⇓}_{≻}$ and show
${\p("tσ[a≔u₂σ] ∗ π")} {⇓}_{≻}$. Let us now consider the reduction of the
process $\p("tσ ∗ π")$. According to \lemRef("sexttermaux") there are two
possibilities.
\begin{itemize}
\item If $\p("tσ ∗ π") ≻^{∗} \p("v ∗ ε")$ for some value $v ∈ Λ_{val}$ then
  $\p("(tσ ∗ π)[a≔u₂]") ≻^{∗} \p("(v ∗ ε)[a≔u₂]")$ by \lemRef("redcompatall").
  This rewrites to $\p("tσ[a≔u₂] ∗ π") ≻^{∗} \p("v[a≔u₂] ∗ ε")$ as
  $a ∉ FV(π)$, and hence we obtain ${\p("tσ[a≔u₂] ∗ π")} {⇓}_{≻}$.
\item If $\p("tσ ∗ π") ≻^{∗} \p("a ∗ π₀")$ for some stack $π₀ ∈ Π$ then
  $\p("(tσ ∗ π)[a≔u₁]") ≻^{∗} \p("(a ∗ π₀)[a≔u₁]")$ and
  $\p("(tσ ∗ π)[a≔u₂]") ≻^{∗} \p("(a ∗ π₀)[a≔u₂]")$ by
  \lemRef("redcompatall"). As a consequence, it is enough to show
  ${{\p("(a ∗ π₀)[a≔u₁]")} {⇓}_{≻}} ⇒ {{\p("(a ∗ π₀)[a≔u₂]")} {⇓}_{≻}}$.
  We assume ${\p("(a ∗ π₀)[a≔u₁]")} {⇓}_{≻}$ and show
  ${\p("(a ∗ π₀)[a≔u₂]")} {⇓}_{≻}$. We are going to build a sequence of
  stacks $(π_i)_{i≤n}$, starting with $π₀$, such that
  $\p("(a ∗ πi)[a≔u₁]") ≻^{+} \p("(a ∗ πi+1)[a≔u₁]")$ for all $i<n$. Note
  that the sequence has to be finite, otherwise the reduction of
  $\p("(a ∗ π₀)[a≔u₁]")$ would be infinite and this would contradict
  ${\p("(a ∗ π₀)[a≔u₁]")} {⇓}_{≻}$. To define $π_{i+1}$ we consider
  $\p("u₁ ∗ πi")$. By transitivity
  $\p("(a ∗ π₀)[a≔u₁]") ≻^{∗} \p("(a ∗ πi)[a≔u₁]") = \p("(u₁ ∗ πi)[a≔u₁]")$
  and hence ${\p("(u₁ ∗ πi)[a≔u₁]")} {⇓}_{≻}$. According to
  \lemRef("sexttermaux") there are two possibilities for the reduction of
  $\p("u₁ ∗ πi")$. Either $\p("u₁ ∗ πi") ≻^{∗} \p("v ∗ ε")$ for some value
  $v$ and the sequence ends with $n = i$, or
  $\p("u₁ ∗ πi") ≻^{∗} \p("a ∗ ρ")$ for some stack $ρ$ and we define
  $π_{i+1} = ρ$.
  
  To end the proof, we will now show ${\p("(a ∗ πi)[a≔u₂]")} {⇓}_{≻}$ for all
  $i ≤ n$. For $i = 0$ this will give us ${\p("(a ∗ π₀)[a≔u₂]")} {⇓}_{≻}$
  which is the expected result. For $i = n$ we know that
  $\p("u₁ ∗ πn") ≻^{∗} {\p("v ∗ ε")} {⇓}_{≻}$, and hence
  ${\p("u₂ ∗ πn")} {⇓}_{≻}$ since $u₁ ≡_{≻} u₂$. As a consequence, we obtain
  that ${\p("(a ∗ πn)[a≔u₂]")} = {\p("(u₂ ∗ πn)[a≔u₂]")} {⇓}_{≻}$ by
  \lemRef("redstable"). Let us now suppose that we have
  ${\p("(a ∗ πi+1)[a≔u₂]")} = {\p("u₂ ∗ πi+1[a≔u₂]")} {⇓}_{≻}$ for some
  $i<n$ and show that
  ${\p("(a ∗ πi)[a≔u₂]")} = {\p("u₂ ∗ πi[a≔u₂]")} {⇓}_{≻}$. Since we know
  $u₁ ≡_{≻} u₂$ we can deduce that ${\p("u₁ ∗ πi+1[a≔u₂]")} {⇓}_{≻}$.
  Moreover, since $\p("u₁ ∗ πi") ≻^{∗} \p("a ∗ πi+1")$ we may use
  \lemRef("redcompatall") to obtain
  $\p("u₁ ∗ πi[a≔u₂]") = \p("(u₁ ∗ πi)[a≔u₂]") ≻^{∗} \p("(a ∗ πi+1)[a≔u₂]")
  = \p("u₂ ∗ πi+1[a≔u₂]")$. As a consequence, we have
  ${\p("u₁ ∗ πi[a≔u₂]")} {⇓}_{≻}$ from which we obtain
  ${\p("u₂ ∗ πi[a≔u₂]")} {⇓}_{≻}$ since $u₁ ≡_{≻} u₂$.
\end{itemize}
\end{proof}
\end{thm}

It is not enough for our equivalence relation to be extentional. As we want
to identify programs with the same computational behaviour, we expect
equivalence to be preserved by evaluation.

\begin{lem}\label("redequiv")
Let $t$ and $u$ be two terms. If for all stack $π ∈Π$ there is a process
$p ∈ {Λ × Π}$ such that $\p("t ∗ π") ≻^{∗} p$ and $\p("u ∗ π") ≻^{∗} p$ then
$t ≡_{≻} u$.
\begin{proof}
Let us take a stack $π₀$ and a substitution $σ₀$. By hypothesis, there is a
process $p₀$ such that $\p("t ∗ π₀") ≻^{∗} p₀$ and $\p("u ∗ π₀") ≻^{∗} p₀$.
Using \lemRef("redcompatall") we obtain $\p("tσ₀ ∗ π₀") ≻^{∗} p₀σ₀$ and
$\p("uσ₀ ∗ π₀") ≻^{∗} p₀$. Consequently,
${{\p("tσ₀ ∗ π₀")} {⇓}_{≻}} ⇔ {{\p("pσ₀")} {⇓}_{≻}}$ and
${{\p("uσ₀ ∗ π₀")} {⇓}_{≻}} ⇔ {{\p("pσ₀")} {⇓}_{≻}}$. We hence have
${{\p("tσ₀ ∗ π₀")} {⇓}_{≻}} ⇔ {{\p("uσ₀ ∗ π₀")} {⇓}_{≻}}$, which gives us
$t ≡_{≻} u$.
\end{proof}
\end{lem}
\begin{cor}\label("redtosame")
Let $t$ and $u$ be terms. If for all $π$ we have
$\p("t ∗ π") ≻ \p("u ∗ π")$ then $t ≡_{≻} u$.
\begin{proof}
Direct conseqence of \lemRef("redequiv") using $p = \p("u∗π")$.
\end{proof}
\end{cor}

\begin{thm}\label("cbvbeta")
For every $t ∈ Λ$, $v ∈ Λ_{val}$ and $x ∈ \cal{V}_ι$ we have
$\t("(λx t) v") ≡_{≻} \t("t[x≔v]")$.
\begin{proof}
Since $\p("(λxt) v∗π") ≻ \p("v∗[λxt]π") ≻ \p("λxt∗v·π") ≻ \p("t[x≔v]∗π")$
for all $π$ we can apply \corRef("redtosame").
\end{proof}
\end{thm}

\begin{thm}
Let $I$ be a finite set of index with $k ∈ I$. If $v_i ∈ Λ_{val}$ and
$l_i ∈ \cal{L}$ for all $i ∈ I$ then
$\t("{(li = vi) i∈I}.lk") ≡_{≻} \t("vk")$.
\begin{proof}
Since $\p("{(li = vi) i∈I}.lk ∗ π") ≻ \p("vk ∗ π")$ for all $π$ we
can use \corRef("redtosame").
\end{proof}
\end{thm}
(* TODO case *)

\begin{thm}\label("consnequiv")
Let $C ∈ \cal{C}$ and $D ∈ \cal{C}$ be two constructors and $v ∈ Λ_{val}$
and $w ∈ Λ_{val}$ be two values. If $C ≠ D$ then
$\v("C[v]") \nequiv_{≻} \v("D[w]")$.
\begin{proof}
If we take $π = \s("[λx [x | C[y] → y]]ε")$ and $σ = ∅$ then we have
${\p("(C[v])σ ∗ π")} {⇓}_{≻}$ since
$\p("(C[v])σ ∗ π") ≻ \p("λx [x | C[y] → y] ∗ C[v] · ε") ≻
\p("[C[v] | C[y] → y] ∗ ε") ≻ \p("v ∗ ε")$. Moreover, we have
${\p("(D[w])σ ∗ π")} {⇑}_{≻}$ since
$\p("(D[v])σ ∗ π") ≻ \p("λx [x | C[y] → y] ∗ D[w] · ε") ≻
\p("[D[w] | C[y] → y] ∗ ε")$ and $\p("[D[w] | C[y] → y] ∗ ε")$ is stuck.
\end{proof}
\end{thm}
(* TODO other easy inequivalences *)
\begin{thm}\label("nequiv")
...
\begin{proof}
...
\end{proof}
\end{thm}

=<

=<
